{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:28:20.346843Z",
     "start_time": "2025-05-24T16:28:16.679897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from util import get_dataframe_from_json\n",
    "from sklearn.metrics import classification_report\n",
    "from bert_util import bert_tokenize_data, tensor_train_test_split, train_bert_model, model_predict, get_data_loader, \\\n",
    "    train_pipeline, randomized_cv_rasearch\n",
    "\n",
    "os.environ[\"USE_TF\"] = \"0\""
   ],
   "id": "e79ad018e19671f2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emiliano/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-24 18:28:18.801448: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-24 18:28:18.808293: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748104098.816863   12337 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748104098.819440   12337 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748104098.826259   12337 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748104098.826267   12337 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748104098.826268   12337 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748104098.826268   12337 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-24 18:28:18.828600: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-24 18:28:20,116\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-05-24 18:28:20,310\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:28:20.414871Z",
     "start_time": "2025-05-24T16:28:20.403558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = pd.read_json('../train_dataset.json', lines=False)\n",
    "test_df = pd.read_json('../test_dataset.json', lines=False)\n",
    "train_df"
   ],
   "id": "4d6cb2e52215c782",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              utterance  hat\n",
       "0     Wow, fantastic. I like the font and color for ...    1\n",
       "1     OK. Let's take a walk and look for a shop that...    4\n",
       "2     This kind of school is good at helping people ...    4\n",
       "3     Thank you, Rick! As you can see, ladies and ge...    3\n",
       "4                 Who knows! That might come to later.     0\n",
       "...                                                 ...  ...\n",
       "3995        Do you think I believe the story like that?    0\n",
       "3996  You ’ Ve had over a month to get this finalize...    0\n",
       "3997                             Hurry up, little boy.     1\n",
       "3998  That's good to know. Oh, listen! The band's pl...    3\n",
       "3999                   Definitely. I ’ m really sorry!     0\n",
       "\n",
       "[4000 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance</th>\n",
       "      <th>hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow, fantastic. I like the font and color for ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK. Let's take a walk and look for a shop that...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This kind of school is good at helping people ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thank you, Rick! As you can see, ladies and ge...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who knows! That might come to later.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>Do you think I believe the story like that?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>You ’ Ve had over a month to get this finalize...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>Hurry up, little boy.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>That's good to know. Oh, listen! The band's pl...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>Definitely. I ’ m really sorry!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T14:15:26.496384Z",
     "start_time": "2025-05-24T14:15:26.069518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "token_ids, attention_masks = bert_tokenize_data(tokenizer, train_df['utterance'].values)\n",
    "train_dataloader, val_dataloader = tensor_train_test_split(torch.tensor(train_df['hat'].values), token_ids, attention_masks, test_size=0.1)"
   ],
   "id": "e70d7ca38d0438c6",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T14:15:32.163094Z",
     "start_time": "2025-05-24T14:15:31.687496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "epochs = 10\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ],
   "id": "a1d52aff034ff532",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T14:16:10.338485Z",
     "start_time": "2025-05-24T14:15:32.185001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "epochs = 10\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "num_training_steps = epochs * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)\n",
    "\n",
    "model = train_bert_model(model, optimizer, scheduler, train_dataloader, val_dataloader, epochs)"
   ],
   "id": "8eac212dbbcdf59a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Epoch 1 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 16:15:32.720315\n",
      "Average Training Loss: 1.233990160667378\n",
      "Time Taken:            0:00:07.294805\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 16:15:40.015512\n",
      "Average Validation Loss:     1.2737085927616467\n",
      "Average Validation Accuracy: 0.5340909090909091\n",
      "Time Taken:                  0:00:00.238261\n",
      "\n",
      "-------------------- Epoch 2 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 16:15:40.255083\n",
      "Average Training Loss: 0.99287924753583\n",
      "Time Taken:            0:00:07.275139\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 16:15:47.530589\n",
      "Average Validation Loss:     1.220288959416476\n",
      "Average Validation Accuracy: 0.48863636363636365\n",
      "Time Taken:                  0:00:00.247728\n",
      "\n",
      "-------------------- Epoch 3 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 16:15:47.779455\n",
      "Average Training Loss: 0.6303189602559027\n",
      "Time Taken:            0:00:07.270178\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 16:15:55.050005\n",
      "Average Validation Loss:     1.1791074059226296\n",
      "Average Validation Accuracy: 0.5795454545454546\n",
      "Time Taken:                  0:00:00.237041\n",
      "\n",
      "-------------------- Epoch 4 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 16:15:55.288062\n",
      "Average Training Loss: 0.3639292970866613\n",
      "Time Taken:            0:00:07.283269\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 16:16:02.571702\n",
      "Average Validation Loss:     1.4996078393676064\n",
      "Average Validation Accuracy: 0.48863636363636365\n",
      "Time Taken:                  0:00:00.243934\n",
      "No improvement for 1 epoch(s).\n",
      "\n",
      "-------------------- Epoch 5 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 16:16:02.816035\n",
      "Average Training Loss: 0.22284097854128998\n",
      "Time Taken:            0:00:07.275227\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 16:16:10.091625\n",
      "Average Validation Loss:     1.494208575649695\n",
      "Average Validation Accuracy: 0.5340909090909091\n",
      "Time Taken:                  0:00:00.243687\n",
      "No improvement for 2 epoch(s).\n",
      "\n",
      "Early stopping triggered after 5 epochs.\n",
      "\n",
      "Total training time: 0:00:37.617252\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T14:16:10.677261Z",
     "start_time": "2025-05-24T14:16:10.349171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_texts = test_df['utterance'].values\n",
    "serie = pd.Series(test_texts)\n",
    "tids, amids = bert_tokenize_data(tokenizer, serie, max_length=64)\n",
    "dl = get_data_loader(tids, amids, batch_size=5, shuffle=False)\n",
    "preds, confidences = model_predict(model, dl)\n",
    "labels_flat = test_df['hat'].values.flatten()\n",
    "accuracy = np.sum(preds == labels_flat) / len(labels_flat)\n",
    "accuracy"
   ],
   "id": "d4f71d611d87e5b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5784313725490197)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T14:16:10.693858Z",
     "start_time": "2025-05-24T14:16:10.689050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hat_map = {\n",
    "    0: \"red\",\n",
    "    1: \"white\",\n",
    "    2: \"black\",\n",
    "    3: \"yellow\",\n",
    "    4: \"green\",\n",
    "}\n",
    "preds_array = np.array(preds)\n",
    "print(classification_report(labels_flat, preds_array, target_names=list(hat_map.values())))"
   ],
   "id": "9b2a2e4b9deb1a83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.50      0.55      0.52        40\n",
      "       white       0.72      0.71      0.72       110\n",
      "       black       0.30      0.35      0.32        23\n",
      "      yellow       0.50      0.32      0.39        19\n",
      "       green       0.31      0.33      0.32        12\n",
      "\n",
      "    accuracy                           0.58       204\n",
      "   macro avg       0.47      0.45      0.45       204\n",
      "weighted avg       0.59      0.58      0.58       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:49:32.980818Z",
     "start_time": "2025-05-21T16:49:32.958916Z"
    }
   },
   "cell_type": "code",
   "source": "augmented_train_df = pd.read_json('../eda_train_dataset.json', lines=False)",
   "id": "12094416d5c6987e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:58:22.024666Z",
     "start_time": "2025-05-21T16:49:44.049212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "token_ids, attention_masks = bert_tokenize_data(tokenizer, augmented_train_df['utterance'].values)\n",
    "train_dataloader, val_dataloader = tensor_train_test_split(torch.tensor(augmented_train_df['hat'].values), token_ids, attention_masks, test_size=0.1)\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "epochs = 10\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "num_training_steps = epochs * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)\n",
    "\n",
    "model = train_bert_model(model, optimizer, scheduler, train_dataloader, val_dataloader, epochs)\n",
    "test_texts = test_df['utterance'].values\n",
    "serie = pd.Series(test_texts)\n",
    "tids, amids = bert_tokenize_data(tokenizer, serie, max_length=64)\n",
    "dl = get_data_loader(tids, amids, batch_size=5, shuffle=False)\n",
    "preds, confidences = model_predict(model, dl)\n",
    "labels_flat = test_df['hat'].values.flatten()\n",
    "\n",
    "preds_array = np.array(preds)\n",
    "print(classification_report(labels_flat, preds_array, target_names=list(hat_map.values())))"
   ],
   "id": "7a72c4f6b580b1d9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Epoch 1 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:49:45.599930\n",
      "Average Loss:     0.8352723453822158\n",
      "Time Taken:       0:00:49.121369\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:50:34.721732\n",
      "Average Loss:     1.6427994484597064\n",
      "Average Accuracy: 0.4376899696048632\n",
      "Time Taken:       0:00:01.687571\n",
      "\n",
      "-------------------- Epoch 2 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:50:36.409734\n",
      "Average Loss:     0.19497602208099393\n",
      "Time Taken:       0:00:49.416321\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:51:25.826465\n",
      "Average Loss:     1.0135578555054963\n",
      "Average Accuracy: 0.7815349544072948\n",
      "Time Taken:       0:00:01.698944\n",
      "\n",
      "-------------------- Epoch 3 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:51:27.525819\n",
      "Average Loss:     0.07621283072581728\n",
      "Time Taken:       0:00:49.632151\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:52:17.158519\n",
      "Average Loss:     1.231666887971632\n",
      "Average Accuracy: 0.7541793313069909\n",
      "Time Taken:       0:00:01.710382\n",
      "\n",
      "-------------------- Epoch 4 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:52:18.869434\n",
      "Average Loss:     0.02680864908466737\n",
      "Time Taken:       0:00:50.003383\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:53:08.873264\n",
      "Average Loss:     0.6529110647339196\n",
      "Average Accuracy: 0.8765197568389057\n",
      "Time Taken:       0:00:01.723106\n",
      "\n",
      "-------------------- Epoch 5 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:53:10.596770\n",
      "Average Loss:     0.015489446931209464\n",
      "Time Taken:       0:00:50.291185\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:54:00.888419\n",
      "Average Loss:     0.46924261628164654\n",
      "Average Accuracy: 0.9088145896656535\n",
      "Time Taken:       0:00:01.729271\n",
      "\n",
      "-------------------- Epoch 6 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:54:02.618108\n",
      "Average Loss:     0.01538447400683182\n",
      "Time Taken:       0:00:50.013795\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:54:52.632321\n",
      "Average Loss:     0.5539516341275575\n",
      "Average Accuracy: 0.9012158054711246\n",
      "Time Taken:       0:00:01.734657\n",
      "\n",
      "-------------------- Epoch 7 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:54:54.367345\n",
      "Average Loss:     0.00910563577684649\n",
      "Time Taken:       0:00:50.156465\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:55:44.524198\n",
      "Average Loss:     0.8235992171524211\n",
      "Average Accuracy: 0.875\n",
      "Time Taken:       0:00:01.747269\n",
      "\n",
      "-------------------- Epoch 8 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:55:46.271840\n",
      "Average Loss:     0.008155516401214487\n",
      "Time Taken:       0:00:50.259353\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:56:36.532004\n",
      "Average Loss:     0.6683721565242059\n",
      "Average Accuracy: 0.8852583586626139\n",
      "Time Taken:       0:00:01.721954\n",
      "\n",
      "-------------------- Epoch 9 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:56:38.254347\n",
      "Average Loss:     0.004453104325992414\n",
      "Time Taken:       0:00:49.945758\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:57:28.200643\n",
      "Average Loss:     0.7898830939539426\n",
      "Average Accuracy: 0.8723404255319149\n",
      "Time Taken:       0:00:01.742859\n",
      "\n",
      "-------------------- Epoch 10 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:57:29.944091\n",
      "Average Loss:     0.0038203647106290284\n",
      "Time Taken:       0:00:49.904555\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:58:19.849185\n",
      "Average Loss:     0.6627375366803422\n",
      "Average Accuracy: 0.8852583586626139\n",
      "Time Taken:       0:00:01.735604\n",
      "\n",
      "Total training time: 0:08:35.985286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.56      0.50      0.53        40\n",
      "       white       0.68      0.88      0.77       110\n",
      "       black       0.30      0.13      0.18        23\n",
      "      yellow       0.40      0.21      0.28        19\n",
      "       green       0.50      0.25      0.33        12\n",
      "\n",
      "    accuracy                           0.62       204\n",
      "   macro avg       0.49      0.39      0.42       204\n",
      "weighted avg       0.58      0.62      0.58       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T14:23:18.781733Z",
     "start_time": "2025-05-24T14:23:18.455458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_texts = test_df['utterance'].values\n",
    "serie = pd.Series(test_texts)\n",
    "tids, amids = bert_tokenize_data(tokenizer, serie, max_length=64)\n",
    "dl = get_data_loader(tids, amids, batch_size=5, shuffle=False)\n",
    "preds, confidences = model_predict(model, dl)\n",
    "labels_flat = test_df['hat'].values.flatten()\n",
    "\n",
    "preds_array = np.array(preds)\n",
    "print(classification_report(labels_flat, preds_array, target_names=list(hat_map.values())))"
   ],
   "id": "c77ace27fdba0741",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.42      0.53      0.47        40\n",
      "       white       0.76      0.66      0.71       110\n",
      "       black       0.36      0.43      0.39        23\n",
      "      yellow       0.45      0.26      0.33        19\n",
      "       green       0.26      0.42      0.32        12\n",
      "\n",
      "    accuracy                           0.56       204\n",
      "   macro avg       0.45      0.46      0.44       204\n",
      "weighted avg       0.59      0.56      0.57       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Randomized Search for Hyperparameter Tuning usage example",
   "id": "2c6e51b7f17b3c20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:31:21.898877Z",
     "start_time": "2025-05-24T16:28:28.805437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_model():\n",
    "    return BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "best_model, best_config, best_score = randomized_cv_rasearch(build_model, tokenizer, train_df['utterance'].values, train_df['hat'].values, test_df['utterance'].values, test_df['hat'].values, num_samples=20, use_lora=True)\n"
   ],
   "id": "6512d1475d97e995",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying configuration 1/20: {'epochs': 10, 'model_dropout': 0.1, 'optimizer_lr': 3e-05, 'scheduler_warmup_steps': 0, 'lora_r': 4, 'lora_alpha': 16, 'lora_dropout': 0.2, 'tokenizer_max_length': 16, 'dataloader_batch_size': 32, 'clip_grad_norm': 5.0, 'early_stopping_patience': 10, 'early_stopping_delta': 0.01, 'scheduler_type': 'constant', 'optimizer_type': 'Adafactor', 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Epoch 1 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:28:30.271862\n",
      "Average Training Loss: 1.6411139643192292\n",
      "Time Taken:            0:00:05.738222\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:28:36.010729\n",
      "Average Validation Loss:     1.627811713218689\n",
      "Average Validation Accuracy: 0.2175\n",
      "Time Taken:                  0:00:00.425744\n",
      "\n",
      "-------------------- Epoch 2 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:28:36.438049\n",
      "Average Training Loss: 1.6428919222950935\n",
      "Time Taken:            0:00:05.477142\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:28:41.915900\n",
      "Average Validation Loss:     1.6229253697395325\n",
      "Average Validation Accuracy: 0.2175\n",
      "Time Taken:                  0:00:00.422760\n",
      "No improvement for 1 epoch(s).\n",
      "\n",
      "-------------------- Epoch 3 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:28:42.339401\n",
      "Average Training Loss: 1.6344801139831544\n",
      "Time Taken:            0:00:05.736158\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:28:48.076226\n",
      "Average Validation Loss:     1.6188087213039397\n",
      "Average Validation Accuracy: 0.21875\n",
      "Time Taken:                  0:00:00.427666\n",
      "No improvement for 2 epoch(s).\n",
      "\n",
      "-------------------- Epoch 4 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:28:48.504582\n",
      "Average Training Loss: 1.6318513622879982\n",
      "Time Taken:            0:00:05.570225\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:28:54.075443\n",
      "Average Validation Loss:     1.6154430735111236\n",
      "Average Validation Accuracy: 0.22125\n",
      "Time Taken:                  0:00:00.413563\n",
      "\n",
      "-------------------- Epoch 5 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:28:54.490755\n",
      "Average Training Loss: 1.6295141875743866\n",
      "Time Taken:            0:00:05.905414\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:29:00.396818\n",
      "Average Validation Loss:     1.6127112531661987\n",
      "Average Validation Accuracy: 0.22625\n",
      "Time Taken:                  0:00:00.434303\n",
      "No improvement for 1 epoch(s).\n",
      "\n",
      "-------------------- Epoch 6 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:29:00.831938\n",
      "Average Training Loss: 1.623795950114727\n",
      "Time Taken:            0:00:05.588259\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:29:06.420830\n",
      "Average Validation Loss:     1.610522254705429\n",
      "Average Validation Accuracy: 0.22875\n",
      "Time Taken:                  0:00:00.430874\n",
      "No improvement for 2 epoch(s).\n",
      "\n",
      "-------------------- Epoch 7 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:29:06.852457\n",
      "Average Training Loss: 1.6206033980846406\n",
      "Time Taken:            0:00:05.711881\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:29:12.564974\n",
      "Average Validation Loss:     1.6086216235160828\n",
      "Average Validation Accuracy: 0.23375\n",
      "Time Taken:                  0:00:00.432683\n",
      "No improvement for 3 epoch(s).\n",
      "\n",
      "-------------------- Epoch 8 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:29:12.998402\n",
      "Average Training Loss: 1.6205324482917787\n",
      "Time Taken:            0:00:05.728043\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:29:18.727078\n",
      "Average Validation Loss:     1.6072022557258605\n",
      "Average Validation Accuracy: 0.2325\n",
      "Time Taken:                  0:00:00.435017\n",
      "No improvement for 4 epoch(s).\n",
      "\n",
      "-------------------- Epoch 9 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:29:19.162850\n",
      "Average Training Loss: 1.6195994490385055\n",
      "Time Taken:            0:00:05.814914\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:29:24.978418\n",
      "Average Validation Loss:     1.605845913887024\n",
      "Average Validation Accuracy: 0.23625\n",
      "Time Taken:                  0:00:00.435286\n",
      "No improvement for 5 epoch(s).\n",
      "\n",
      "-------------------- Epoch 10 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:29:25.414435\n",
      "Average Training Loss: 1.6176157793402672\n",
      "Time Taken:            0:00:05.802364\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:29:31.217472\n",
      "Average Validation Loss:     1.6047972774505614\n",
      "Average Validation Accuracy: 0.2375\n",
      "Time Taken:                  0:00:00.430552\n",
      "\n",
      "Total training time: 0:01:01.378034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.36      0.20      0.25       200\n",
      "       white       0.18      0.05      0.08       200\n",
      "       black       0.00      0.00      0.00       200\n",
      "      yellow       0.22      0.89      0.35       200\n",
      "       green       0.18      0.02      0.04       200\n",
      "\n",
      "    accuracy                           0.23      1000\n",
      "   macro avg       0.19      0.23      0.14      1000\n",
      "weighted avg       0.19      0.23      0.14      1000\n",
      "\n",
      "Score: 0.2300\n",
      "Trying configuration 2/20: {'epochs': 10, 'model_dropout': 0.1, 'optimizer_lr': 2e-05, 'scheduler_warmup_steps': 0, 'lora_r': 16, 'lora_alpha': 64, 'lora_dropout': 0.05, 'tokenizer_max_length': 32, 'dataloader_batch_size': 32, 'clip_grad_norm': 5.0, 'early_stopping_patience': 10, 'early_stopping_delta': 0.1, 'scheduler_type': 'cosine', 'optimizer_type': 'AdamW', 'weight_decay': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Epoch 1 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:29:33.474711\n",
      "Average Training Loss: 1.6186267480254173\n",
      "Time Taken:            0:00:05.490895\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:29:38.966287\n",
      "Average Validation Loss:     1.5751414167881013\n",
      "Average Validation Accuracy: 0.28625\n",
      "Time Taken:                  0:00:00.645148\n",
      "\n",
      "-------------------- Epoch 2 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:29:39.613272\n",
      "Average Training Loss: 1.5287788954377175\n",
      "Time Taken:            0:00:05.504528\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:29:45.118469\n",
      "Average Validation Loss:     1.490739973783493\n",
      "Average Validation Accuracy: 0.3425\n",
      "Time Taken:                  0:00:00.649427\n",
      "No improvement for 1 epoch(s).\n",
      "\n",
      "-------------------- Epoch 3 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:29:45.768668\n",
      "Average Training Loss: 1.4070635312795639\n",
      "Time Taken:            0:00:05.496974\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:29:51.266311\n",
      "Average Validation Loss:     1.3689591896533966\n",
      "Average Validation Accuracy: 0.4225\n",
      "Time Taken:                  0:00:00.649897\n",
      "\n",
      "-------------------- Epoch 4 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:29:51.917916\n",
      "Average Training Loss: 1.2950500059127807\n",
      "Time Taken:            0:00:05.506798\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:29:57.425403\n",
      "Average Validation Loss:     1.3034708434343338\n",
      "Average Validation Accuracy: 0.44125\n",
      "Time Taken:                  0:00:00.646473\n",
      "No improvement for 1 epoch(s).\n",
      "\n",
      "-------------------- Epoch 5 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:29:58.072614\n",
      "Average Training Loss: 1.2409836035966872\n",
      "Time Taken:            0:00:05.516849\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:30:03.590164\n",
      "Average Validation Loss:     1.2719741022586823\n",
      "Average Validation Accuracy: 0.4525\n",
      "Time Taken:                  0:00:00.649277\n",
      "No improvement for 2 epoch(s).\n",
      "\n",
      "-------------------- Epoch 6 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:30:04.240239\n",
      "Average Training Loss: 1.2178757616877556\n",
      "Time Taken:            0:00:05.574451\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:30:09.815470\n",
      "Average Validation Loss:     1.2581002122163774\n",
      "Average Validation Accuracy: 0.455\n",
      "Time Taken:                  0:00:00.654145\n",
      "\n",
      "-------------------- Epoch 7 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:30:10.471525\n",
      "Average Training Loss: 1.200031590759754\n",
      "Time Taken:            0:00:05.564843\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:30:16.037198\n",
      "Average Validation Loss:     1.2559095454216003\n",
      "Average Validation Accuracy: 0.455\n",
      "Time Taken:                  0:00:00.666969\n",
      "No improvement for 1 epoch(s).\n",
      "\n",
      "-------------------- Epoch 8 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:30:16.705021\n",
      "Average Training Loss: 1.2025027103722095\n",
      "Time Taken:            0:00:05.538524\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:30:22.244274\n",
      "Average Validation Loss:     1.2464425784349442\n",
      "Average Validation Accuracy: 0.46375\n",
      "Time Taken:                  0:00:00.656420\n",
      "No improvement for 2 epoch(s).\n",
      "\n",
      "-------------------- Epoch 9 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:30:22.901521\n",
      "Average Training Loss: 1.1893907700479032\n",
      "Time Taken:            0:00:05.563718\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:30:28.466020\n",
      "Average Validation Loss:     1.2463651710748673\n",
      "Average Validation Accuracy: 0.46125\n",
      "Time Taken:                  0:00:00.658397\n",
      "No improvement for 3 epoch(s).\n",
      "\n",
      "-------------------- Epoch 10 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:30:29.125258\n",
      "Average Training Loss: 1.1838626116514206\n",
      "Time Taken:            0:00:05.555330\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:30:34.681365\n",
      "Average Validation Loss:     1.2462141185998916\n",
      "Average Validation Accuracy: 0.4625\n",
      "Time Taken:                  0:00:00.657864\n",
      "No improvement for 4 epoch(s).\n",
      "\n",
      "Total training time: 0:01:01.865503\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.66      0.77      0.71       200\n",
      "       white       0.40      0.14      0.21       200\n",
      "       black       0.39      0.52      0.45       200\n",
      "      yellow       0.47      0.52      0.50       200\n",
      "       green       0.41      0.45      0.43       200\n",
      "\n",
      "    accuracy                           0.48      1000\n",
      "   macro avg       0.47      0.48      0.46      1000\n",
      "weighted avg       0.47      0.48      0.46      1000\n",
      "\n",
      "Score: 0.4780\n",
      "Trying configuration 3/20: {'epochs': 20, 'model_dropout': 0.1, 'optimizer_lr': 2e-05, 'scheduler_warmup_steps': 200, 'lora_r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1, 'tokenizer_max_length': 32, 'dataloader_batch_size': 8, 'clip_grad_norm': 2.0, 'early_stopping_patience': 3, 'early_stopping_delta': 0.01, 'scheduler_type': 'cosine', 'optimizer_type': 'AdamW', 'weight_decay': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Epoch 1 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:30:37.460599\n",
      "Average Training Loss: 1.6368529230356217\n",
      "Time Taken:            0:00:05.565567\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:30:43.026998\n",
      "Average Validation Loss:     1.5847070550918578\n",
      "Average Validation Accuracy: 0.32375\n",
      "Time Taken:                  0:00:00.656340\n",
      "\n",
      "-------------------- Epoch 2 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:30:43.685196\n",
      "Average Training Loss: 1.5250296261906624\n",
      "Time Taken:            0:00:05.552702\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:30:49.238628\n",
      "Average Validation Loss:     1.4314633226394653\n",
      "Average Validation Accuracy: 0.35875\n",
      "Time Taken:                  0:00:00.654237\n",
      "\n",
      "-------------------- Epoch 3 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:30:49.895050\n",
      "Average Training Loss: 1.4116925109922887\n",
      "Time Taken:            0:00:05.556081\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:30:55.451895\n",
      "Average Validation Loss:     1.3731704872846604\n",
      "Average Validation Accuracy: 0.40125\n",
      "Time Taken:                  0:00:00.651816\n",
      "\n",
      "-------------------- Epoch 4 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:30:56.105604\n",
      "Average Training Loss: 1.3559447194635867\n",
      "Time Taken:            0:00:05.569289\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:31:01.675620\n",
      "Average Validation Loss:     1.3197192168235778\n",
      "Average Validation Accuracy: 0.42875\n",
      "Time Taken:                  0:00:00.654764\n",
      "\n",
      "-------------------- Epoch 5 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:31:02.332136\n",
      "Average Training Loss: 1.3149396568536758\n",
      "Time Taken:            0:00:05.570350\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:31:07.903279\n",
      "Average Validation Loss:     1.2832246017456055\n",
      "Average Validation Accuracy: 0.44875\n",
      "Time Taken:                  0:00:00.654196\n",
      "\n",
      "-------------------- Epoch 6 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:31:08.559720\n",
      "Average Training Loss: 1.2707691007852555\n",
      "Time Taken:            0:00:05.579264\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:31:14.139751\n",
      "Average Validation Loss:     1.257083886861801\n",
      "Average Validation Accuracy: 0.45\n",
      "Time Taken:                  0:00:00.656962\n",
      "\n",
      "-------------------- Epoch 7 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 18:31:14.798810\n",
      "Average Training Loss: 1.2465778644382954\n",
      "Time Taken:            0:00:05.581398\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 18:31:20.380996\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      2\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m BertForSequenceClassification.from_pretrained(\u001B[33m'\u001B[39m\u001B[33mbert-base-uncased\u001B[39m\u001B[33m'\u001B[39m, num_labels=\u001B[32m5\u001B[39m)\n\u001B[32m      4\u001B[39m tokenizer = BertTokenizer.from_pretrained(\u001B[33m\"\u001B[39m\u001B[33mbert-base-uncased\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m best_model, best_config, best_score = \u001B[43mrandomized_cv_rasearch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuild_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_df\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mutterance\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_df\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mhat\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_df\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mutterance\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_df\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mhat\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_samples\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_lora\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/bert_util.py:117\u001B[39m, in \u001B[36mrandomized_cv_rasearch\u001B[39m\u001B[34m(model_fn, tokenizer, train_data, train_labels, test_data, test_labels, num_samples, use_lora)\u001B[39m\n\u001B[32m    114\u001B[39m model.config.attention_probs_dropout_prob = config[\u001B[33m\"\u001B[39m\u001B[33mmodel_dropout\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    116\u001B[39m \u001B[38;5;66;03m# Train the model with current configuration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m117\u001B[39m trained_model = \u001B[43mtrain_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_labels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_lora\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_lora\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    119\u001B[39m \u001B[38;5;66;03m# Tokenize and prepare val_dataloader again to evaluate\u001B[39;00m\n\u001B[32m    120\u001B[39m tids, amids = bert_tokenize_data(tokenizer, pd.Series(test_data), max_length=config[\u001B[33m\"\u001B[39m\u001B[33mtokenizer_max_length\u001B[39m\u001B[33m\"\u001B[39m])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/bert_util.py:177\u001B[39m, in \u001B[36mtrain_pipeline\u001B[39m\u001B[34m(model, tokenizer, train_data, train_labels, config, use_lora)\u001B[39m\n\u001B[32m    174\u001B[39m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m get_constant_schedule_with_warmup\n\u001B[32m    175\u001B[39m     scheduler = get_constant_schedule_with_warmup(optimizer, config[\u001B[33m\"\u001B[39m\u001B[33mscheduler_warmup_steps\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m--> \u001B[39m\u001B[32m177\u001B[39m model = \u001B[43mtrain_bert_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    178\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/bert_util.py:249\u001B[39m, in \u001B[36mtrain_bert_model\u001B[39m\u001B[34m(model, optimizer, scheduler, train_dataloader, val_dataloader, config)\u001B[39m\n\u001B[32m    246\u001B[39m batch_labels = batch[\u001B[32m2\u001B[39m].to(device)\n\u001B[32m    248\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m--> \u001B[39m\u001B[32m249\u001B[39m     loss, logits = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    250\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbatch_token_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    251\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_labels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    253\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[32m    255\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    256\u001B[39m logits = logits.detach().cpu().numpy()\n\u001B[32m    257\u001B[39m label_ids = batch_labels.to(\u001B[33m'\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m'\u001B[39m).numpy()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/peft/peft_model.py:1559\u001B[39m, in \u001B[36mPeftModelForSequenceClassification.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001B[39m\n\u001B[32m   1557\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m peft_config.peft_type == PeftType.POLY:\n\u001B[32m   1558\u001B[39m             kwargs[\u001B[33m\"\u001B[39m\u001B[33mtask_ids\u001B[39m\u001B[33m\"\u001B[39m] = task_ids\n\u001B[32m-> \u001B[39m\u001B[32m1559\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbase_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1560\u001B[39m \u001B[43m            \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1561\u001B[39m \u001B[43m            \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1562\u001B[39m \u001B[43m            \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1563\u001B[39m \u001B[43m            \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1564\u001B[39m \u001B[43m            \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1565\u001B[39m \u001B[43m            \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1566\u001B[39m \u001B[43m            \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1567\u001B[39m \u001B[43m            \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1568\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1570\u001B[39m batch_size = _get_batch_size(input_ids, inputs_embeds)\n\u001B[32m   1571\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1572\u001B[39m     \u001B[38;5;66;03m# concat prompt attention mask\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:193\u001B[39m, in \u001B[36mBaseTuner.forward\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    192\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args: Any, **kwargs: Any):\n\u001B[32m--> \u001B[39m\u001B[32m193\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1675\u001B[39m, in \u001B[36mBertForSequenceClassification.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m   1667\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1668\u001B[39m \u001B[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[32m   1669\u001B[39m \u001B[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[32m   1670\u001B[39m \u001B[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[32m   1671\u001B[39m \u001B[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[32m   1672\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1673\u001B[39m return_dict = return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.use_return_dict\n\u001B[32m-> \u001B[39m\u001B[32m1675\u001B[39m outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1676\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1677\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1678\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1679\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1680\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1681\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1682\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1683\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1684\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1685\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1687\u001B[39m pooled_output = outputs[\u001B[32m1\u001B[39m]\n\u001B[32m   1689\u001B[39m pooled_output = \u001B[38;5;28mself\u001B[39m.dropout(pooled_output)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1144\u001B[39m, in \u001B[36mBertModel.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m   1137\u001B[39m \u001B[38;5;66;03m# Prepare head mask if needed\u001B[39;00m\n\u001B[32m   1138\u001B[39m \u001B[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001B[39;00m\n\u001B[32m   1139\u001B[39m \u001B[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001B[39;00m\n\u001B[32m   1140\u001B[39m \u001B[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001B[39;00m\n\u001B[32m   1141\u001B[39m \u001B[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001B[39;00m\n\u001B[32m   1142\u001B[39m head_mask = \u001B[38;5;28mself\u001B[39m.get_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m.config.num_hidden_layers)\n\u001B[32m-> \u001B[39m\u001B[32m1144\u001B[39m encoder_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1145\u001B[39m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1146\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1147\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1148\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1149\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1150\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1151\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1152\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1153\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1154\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1155\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1156\u001B[39m sequence_output = encoder_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m   1157\u001B[39m pooled_output = \u001B[38;5;28mself\u001B[39m.pooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.pooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:695\u001B[39m, in \u001B[36mBertEncoder.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m    684\u001B[39m     layer_outputs = \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(\n\u001B[32m    685\u001B[39m         layer_module.\u001B[34m__call__\u001B[39m,\n\u001B[32m    686\u001B[39m         hidden_states,\n\u001B[32m   (...)\u001B[39m\u001B[32m    692\u001B[39m         output_attentions,\n\u001B[32m    693\u001B[39m     )\n\u001B[32m    694\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m695\u001B[39m     layer_outputs = \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    696\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    697\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    698\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    699\u001B[39m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    700\u001B[39m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    701\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    702\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    703\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    705\u001B[39m hidden_states = layer_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    706\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:585\u001B[39m, in \u001B[36mBertLayer.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[39m\n\u001B[32m    573\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\n\u001B[32m    574\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    575\u001B[39m     hidden_states: torch.Tensor,\n\u001B[32m   (...)\u001B[39m\u001B[32m    582\u001B[39m ) -> Tuple[torch.Tensor]:\n\u001B[32m    583\u001B[39m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[32m    584\u001B[39m     self_attn_past_key_value = past_key_value[:\u001B[32m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m585\u001B[39m     self_attention_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    586\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    587\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    588\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    589\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    590\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m=\u001B[49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    591\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    592\u001B[39m     attention_output = self_attention_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    594\u001B[39m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:524\u001B[39m, in \u001B[36mBertAttention.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[39m\n\u001B[32m    505\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\n\u001B[32m    506\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    507\u001B[39m     hidden_states: torch.Tensor,\n\u001B[32m   (...)\u001B[39m\u001B[32m    513\u001B[39m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] = \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    514\u001B[39m ) -> Tuple[torch.Tensor]:\n\u001B[32m    515\u001B[39m     self_outputs = \u001B[38;5;28mself\u001B[39m.self(\n\u001B[32m    516\u001B[39m         hidden_states,\n\u001B[32m    517\u001B[39m         attention_mask,\n\u001B[32m   (...)\u001B[39m\u001B[32m    522\u001B[39m         output_attentions,\n\u001B[32m    523\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m524\u001B[39m     attention_output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m(\u001B[49m\u001B[43mself_outputs\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    525\u001B[39m     outputs = (attention_output,) + self_outputs[\u001B[32m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n\u001B[32m    526\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:467\u001B[39m, in \u001B[36mBertSelfOutput.forward\u001B[39m\u001B[34m(self, hidden_states, input_tensor)\u001B[39m\n\u001B[32m    465\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n\u001B[32m    466\u001B[39m     hidden_states = \u001B[38;5;28mself\u001B[39m.dense(hidden_states)\n\u001B[32m--> \u001B[39m\u001B[32m467\u001B[39m     hidden_states = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    468\u001B[39m     hidden_states = \u001B[38;5;28mself\u001B[39m.LayerNorm(hidden_states + input_tensor)\n\u001B[32m    469\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:70\u001B[39m, in \u001B[36mDropout.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m     69\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m---> \u001B[39m\u001B[32m70\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/torch/nn/functional.py:1425\u001B[39m, in \u001B[36mdropout\u001B[39m\u001B[34m(input, p, training, inplace)\u001B[39m\n\u001B[32m   1422\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m p < \u001B[32m0.0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m p > \u001B[32m1.0\u001B[39m:\n\u001B[32m   1423\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mdropout probability has to be between 0 and 1, but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mp\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m   1424\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[32m-> \u001B[39m\u001B[32m1425\u001B[39m     _VF.dropout_(\u001B[38;5;28minput\u001B[39m, p, training) \u001B[38;5;28;01mif\u001B[39;00m inplace \u001B[38;5;28;01melse\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1426\u001B[39m )\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T15:31:44.172822Z",
     "start_time": "2025-05-24T15:31:43.996347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bert_util import scoring_fn\n",
    "tids, amids = bert_tokenize_data(tokenizer, pd.Series(test_df['utterance'].values), max_length=best_config[\"tokenizer_max_length\"])\n",
    "val_dataloader = get_data_loader(tids, amids, batch_size=best_config[\"dataloader_batch_size\"], shuffle=False)\n",
    "\n",
    "score = scoring_fn(best_model, val_dataloader, test_df['hat'].values)"
   ],
   "id": "ae6861a3c6aacf9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.46      0.45      0.46        40\n",
      "       white       0.69      0.78      0.74       110\n",
      "       black       0.21      0.13      0.16        23\n",
      "      yellow       0.40      0.32      0.35        19\n",
      "       green       0.25      0.25      0.25        12\n",
      "\n",
      "    accuracy                           0.57       204\n",
      "   macro avg       0.40      0.39      0.39       204\n",
      "weighted avg       0.54      0.57      0.55       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset = pd.read_csv('../dailydialog/final_dataset.csv')",
   "id": "9440497c07a47c3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b908f74d231c34d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
