{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T15:29:23.636830Z",
     "start_time": "2025-05-24T15:29:20.028643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from util import get_dataframe_from_json\n",
    "from sklearn.metrics import classification_report\n",
    "from bert_util import bert_tokenize_data, tensor_train_test_split, train_bert_model, model_predict, get_data_loader, \\\n",
    "    train_pipeline, randomized_cv_rasearch\n",
    "\n",
    "os.environ[\"USE_TF\"] = \"0\""
   ],
   "id": "e79ad018e19671f2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emiliano/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-24 17:29:22.126647: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-24 17:29:22.133489: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748100562.141807    9985 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748100562.144399    9985 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748100562.150802    9985 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748100562.150808    9985 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748100562.150809    9985 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748100562.150810    9985 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-24 17:29:22.153042: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-24 17:29:23,402\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-05-24 17:29:23,599\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T15:29:23.715459Z",
     "start_time": "2025-05-24T15:29:23.705540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = pd.read_json('../train_dataset.json', lines=False)\n",
    "test_df = pd.read_json('../test_dataset.json', lines=False)\n",
    "train_df"
   ],
   "id": "4d6cb2e52215c782",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     turn                                          utterance     emotion  \\\n",
       "0       3                               I'll take one, too.    happiness   \n",
       "1       8  You know, we are superior to other clothes com...  no_emotion   \n",
       "2       5                          Her new boyfriend, right?  no_emotion   \n",
       "3       9  How about recommending him to use the storage ...  no_emotion   \n",
       "4       1   Oh, a bouquet of flowers. It's very kind of you.    surprise   \n",
       "..    ...                                                ...         ...   \n",
       "808     0                    I prefer potatoes to eggplants.  no_emotion   \n",
       "809     0  Mr. Smith, I would like to get right to the po...  no_emotion   \n",
       "810     4                                              Yeah?  no_emotion   \n",
       "811     0                             I am so bored all day.  no_emotion   \n",
       "812     2                           Do you play much tennis?  no_emotion   \n",
       "\n",
       "            act  hat  \n",
       "0        inform    0  \n",
       "1        inform    3  \n",
       "2    commissive    1  \n",
       "3     directive    4  \n",
       "4    commissive    1  \n",
       "..          ...  ...  \n",
       "808      inform    0  \n",
       "809    question    1  \n",
       "810    question    1  \n",
       "811      inform    0  \n",
       "812    question    1  \n",
       "\n",
       "[813 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turn</th>\n",
       "      <th>utterance</th>\n",
       "      <th>emotion</th>\n",
       "      <th>act</th>\n",
       "      <th>hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>I'll take one, too.</td>\n",
       "      <td>happiness</td>\n",
       "      <td>inform</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>You know, we are superior to other clothes com...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Her new boyfriend, right?</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>commissive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>How about recommending him to use the storage ...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>directive</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Oh, a bouquet of flowers. It's very kind of you.</td>\n",
       "      <td>surprise</td>\n",
       "      <td>commissive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>0</td>\n",
       "      <td>I prefer potatoes to eggplants.</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>0</td>\n",
       "      <td>Mr. Smith, I would like to get right to the po...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>4</td>\n",
       "      <td>Yeah?</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>0</td>\n",
       "      <td>I am so bored all day.</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>2</td>\n",
       "      <td>Do you play much tennis?</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>813 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T14:15:26.496384Z",
     "start_time": "2025-05-24T14:15:26.069518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "token_ids, attention_masks = bert_tokenize_data(tokenizer, train_df['utterance'].values)\n",
    "train_dataloader, val_dataloader = tensor_train_test_split(torch.tensor(train_df['hat'].values), token_ids, attention_masks, test_size=0.1)"
   ],
   "id": "e70d7ca38d0438c6",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T14:15:32.163094Z",
     "start_time": "2025-05-24T14:15:31.687496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "epochs = 10\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ],
   "id": "a1d52aff034ff532",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T14:16:10.338485Z",
     "start_time": "2025-05-24T14:15:32.185001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "epochs = 10\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "num_training_steps = epochs * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)\n",
    "\n",
    "model = train_bert_model(model, optimizer, scheduler, train_dataloader, val_dataloader, epochs)"
   ],
   "id": "8eac212dbbcdf59a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Epoch 1 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 16:15:32.720315\n",
      "Average Training Loss: 1.233990160667378\n",
      "Time Taken:            0:00:07.294805\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 16:15:40.015512\n",
      "Average Validation Loss:     1.2737085927616467\n",
      "Average Validation Accuracy: 0.5340909090909091\n",
      "Time Taken:                  0:00:00.238261\n",
      "\n",
      "-------------------- Epoch 2 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 16:15:40.255083\n",
      "Average Training Loss: 0.99287924753583\n",
      "Time Taken:            0:00:07.275139\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 16:15:47.530589\n",
      "Average Validation Loss:     1.220288959416476\n",
      "Average Validation Accuracy: 0.48863636363636365\n",
      "Time Taken:                  0:00:00.247728\n",
      "\n",
      "-------------------- Epoch 3 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 16:15:47.779455\n",
      "Average Training Loss: 0.6303189602559027\n",
      "Time Taken:            0:00:07.270178\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 16:15:55.050005\n",
      "Average Validation Loss:     1.1791074059226296\n",
      "Average Validation Accuracy: 0.5795454545454546\n",
      "Time Taken:                  0:00:00.237041\n",
      "\n",
      "-------------------- Epoch 4 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 16:15:55.288062\n",
      "Average Training Loss: 0.3639292970866613\n",
      "Time Taken:            0:00:07.283269\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 16:16:02.571702\n",
      "Average Validation Loss:     1.4996078393676064\n",
      "Average Validation Accuracy: 0.48863636363636365\n",
      "Time Taken:                  0:00:00.243934\n",
      "No improvement for 1 epoch(s).\n",
      "\n",
      "-------------------- Epoch 5 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 16:16:02.816035\n",
      "Average Training Loss: 0.22284097854128998\n",
      "Time Taken:            0:00:07.275227\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 16:16:10.091625\n",
      "Average Validation Loss:     1.494208575649695\n",
      "Average Validation Accuracy: 0.5340909090909091\n",
      "Time Taken:                  0:00:00.243687\n",
      "No improvement for 2 epoch(s).\n",
      "\n",
      "Early stopping triggered after 5 epochs.\n",
      "\n",
      "Total training time: 0:00:37.617252\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T14:16:10.677261Z",
     "start_time": "2025-05-24T14:16:10.349171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_texts = test_df['utterance'].values\n",
    "serie = pd.Series(test_texts)\n",
    "tids, amids = bert_tokenize_data(tokenizer, serie, max_length=64)\n",
    "dl = get_data_loader(tids, amids, batch_size=5, shuffle=False)\n",
    "preds, confidences = model_predict(model, dl)\n",
    "labels_flat = test_df['hat'].values.flatten()\n",
    "accuracy = np.sum(preds == labels_flat) / len(labels_flat)\n",
    "accuracy"
   ],
   "id": "d4f71d611d87e5b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5784313725490197)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T14:16:10.693858Z",
     "start_time": "2025-05-24T14:16:10.689050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hat_map = {\n",
    "    0: \"red\",\n",
    "    1: \"white\",\n",
    "    2: \"black\",\n",
    "    3: \"yellow\",\n",
    "    4: \"green\",\n",
    "}\n",
    "preds_array = np.array(preds)\n",
    "print(classification_report(labels_flat, preds_array, target_names=list(hat_map.values())))"
   ],
   "id": "9b2a2e4b9deb1a83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.50      0.55      0.52        40\n",
      "       white       0.72      0.71      0.72       110\n",
      "       black       0.30      0.35      0.32        23\n",
      "      yellow       0.50      0.32      0.39        19\n",
      "       green       0.31      0.33      0.32        12\n",
      "\n",
      "    accuracy                           0.58       204\n",
      "   macro avg       0.47      0.45      0.45       204\n",
      "weighted avg       0.59      0.58      0.58       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:49:32.980818Z",
     "start_time": "2025-05-21T16:49:32.958916Z"
    }
   },
   "cell_type": "code",
   "source": "augmented_train_df = pd.read_json('../eda_train_dataset.json', lines=False)",
   "id": "12094416d5c6987e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:58:22.024666Z",
     "start_time": "2025-05-21T16:49:44.049212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "token_ids, attention_masks = bert_tokenize_data(tokenizer, augmented_train_df['utterance'].values)\n",
    "train_dataloader, val_dataloader = tensor_train_test_split(torch.tensor(augmented_train_df['hat'].values), token_ids, attention_masks, test_size=0.1)\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "epochs = 10\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "num_training_steps = epochs * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)\n",
    "\n",
    "model = train_bert_model(model, optimizer, scheduler, train_dataloader, val_dataloader, epochs)\n",
    "test_texts = test_df['utterance'].values\n",
    "serie = pd.Series(test_texts)\n",
    "tids, amids = bert_tokenize_data(tokenizer, serie, max_length=64)\n",
    "dl = get_data_loader(tids, amids, batch_size=5, shuffle=False)\n",
    "preds, confidences = model_predict(model, dl)\n",
    "labels_flat = test_df['hat'].values.flatten()\n",
    "\n",
    "preds_array = np.array(preds)\n",
    "print(classification_report(labels_flat, preds_array, target_names=list(hat_map.values())))"
   ],
   "id": "7a72c4f6b580b1d9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Epoch 1 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:49:45.599930\n",
      "Average Loss:     0.8352723453822158\n",
      "Time Taken:       0:00:49.121369\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:50:34.721732\n",
      "Average Loss:     1.6427994484597064\n",
      "Average Accuracy: 0.4376899696048632\n",
      "Time Taken:       0:00:01.687571\n",
      "\n",
      "-------------------- Epoch 2 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:50:36.409734\n",
      "Average Loss:     0.19497602208099393\n",
      "Time Taken:       0:00:49.416321\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:51:25.826465\n",
      "Average Loss:     1.0135578555054963\n",
      "Average Accuracy: 0.7815349544072948\n",
      "Time Taken:       0:00:01.698944\n",
      "\n",
      "-------------------- Epoch 3 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:51:27.525819\n",
      "Average Loss:     0.07621283072581728\n",
      "Time Taken:       0:00:49.632151\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:52:17.158519\n",
      "Average Loss:     1.231666887971632\n",
      "Average Accuracy: 0.7541793313069909\n",
      "Time Taken:       0:00:01.710382\n",
      "\n",
      "-------------------- Epoch 4 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:52:18.869434\n",
      "Average Loss:     0.02680864908466737\n",
      "Time Taken:       0:00:50.003383\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:53:08.873264\n",
      "Average Loss:     0.6529110647339196\n",
      "Average Accuracy: 0.8765197568389057\n",
      "Time Taken:       0:00:01.723106\n",
      "\n",
      "-------------------- Epoch 5 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:53:10.596770\n",
      "Average Loss:     0.015489446931209464\n",
      "Time Taken:       0:00:50.291185\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:54:00.888419\n",
      "Average Loss:     0.46924261628164654\n",
      "Average Accuracy: 0.9088145896656535\n",
      "Time Taken:       0:00:01.729271\n",
      "\n",
      "-------------------- Epoch 6 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:54:02.618108\n",
      "Average Loss:     0.01538447400683182\n",
      "Time Taken:       0:00:50.013795\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:54:52.632321\n",
      "Average Loss:     0.5539516341275575\n",
      "Average Accuracy: 0.9012158054711246\n",
      "Time Taken:       0:00:01.734657\n",
      "\n",
      "-------------------- Epoch 7 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:54:54.367345\n",
      "Average Loss:     0.00910563577684649\n",
      "Time Taken:       0:00:50.156465\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:55:44.524198\n",
      "Average Loss:     0.8235992171524211\n",
      "Average Accuracy: 0.875\n",
      "Time Taken:       0:00:01.747269\n",
      "\n",
      "-------------------- Epoch 8 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:55:46.271840\n",
      "Average Loss:     0.008155516401214487\n",
      "Time Taken:       0:00:50.259353\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:56:36.532004\n",
      "Average Loss:     0.6683721565242059\n",
      "Average Accuracy: 0.8852583586626139\n",
      "Time Taken:       0:00:01.721954\n",
      "\n",
      "-------------------- Epoch 9 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:56:38.254347\n",
      "Average Loss:     0.004453104325992414\n",
      "Time Taken:       0:00:49.945758\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:57:28.200643\n",
      "Average Loss:     0.7898830939539426\n",
      "Average Accuracy: 0.8723404255319149\n",
      "Time Taken:       0:00:01.742859\n",
      "\n",
      "-------------------- Epoch 10 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:57:29.944091\n",
      "Average Loss:     0.0038203647106290284\n",
      "Time Taken:       0:00:49.904555\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:58:19.849185\n",
      "Average Loss:     0.6627375366803422\n",
      "Average Accuracy: 0.8852583586626139\n",
      "Time Taken:       0:00:01.735604\n",
      "\n",
      "Total training time: 0:08:35.985286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.56      0.50      0.53        40\n",
      "       white       0.68      0.88      0.77       110\n",
      "       black       0.30      0.13      0.18        23\n",
      "      yellow       0.40      0.21      0.28        19\n",
      "       green       0.50      0.25      0.33        12\n",
      "\n",
      "    accuracy                           0.62       204\n",
      "   macro avg       0.49      0.39      0.42       204\n",
      "weighted avg       0.58      0.62      0.58       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T14:23:18.781733Z",
     "start_time": "2025-05-24T14:23:18.455458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_texts = test_df['utterance'].values\n",
    "serie = pd.Series(test_texts)\n",
    "tids, amids = bert_tokenize_data(tokenizer, serie, max_length=64)\n",
    "dl = get_data_loader(tids, amids, batch_size=5, shuffle=False)\n",
    "preds, confidences = model_predict(model, dl)\n",
    "labels_flat = test_df['hat'].values.flatten()\n",
    "\n",
    "preds_array = np.array(preds)\n",
    "print(classification_report(labels_flat, preds_array, target_names=list(hat_map.values())))"
   ],
   "id": "c77ace27fdba0741",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.42      0.53      0.47        40\n",
      "       white       0.76      0.66      0.71       110\n",
      "       black       0.36      0.43      0.39        23\n",
      "      yellow       0.45      0.26      0.33        19\n",
      "       green       0.26      0.42      0.32        12\n",
      "\n",
      "    accuracy                           0.56       204\n",
      "   macro avg       0.45      0.46      0.44       204\n",
      "weighted avg       0.59      0.56      0.57       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Randomized Search for Hyperparameter Tuning usage example",
   "id": "2c6e51b7f17b3c20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T15:31:33.961228Z",
     "start_time": "2025-05-24T15:30:21.054849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "best_model, best_config, best_score = randomized_cv_rasearch(model, tokenizer, train_df['utterance'].values, train_df['hat'].values, test_df['utterance'].values, test_df['hat'].values, num_samples=2, use_lora=False)\n"
   ],
   "id": "6512d1475d97e995",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying configuration 1/2: {'epochs': 10, 'model_dropout': 0.1, 'optimizer_lr': 3e-05, 'scheduler_warmup_steps': 0, 'lora_r': 4, 'lora_alpha': 16, 'lora_dropout': 0.2, 'tokenizer_max_length': 16, 'dataloader_batch_size': 32, 'clip_grad_norm': 5.0, 'early_stopping_patience': 10, 'early_stopping_delta': 0.01, 'scheduler_type': 'constant', 'optimizer_type': 'Adafactor', 'weight_decay': 0.01}\n",
      "\n",
      "-------------------- Epoch 1 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 17:30:22.254312\n",
      "Average Training Loss: 1.774469180804927\n",
      "Time Taken:            0:00:03.686337\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 17:30:25.940996\n",
      "Average Validation Loss:     1.6343998568398612\n",
      "Average Validation Accuracy: 0.1765873015873016\n",
      "Time Taken:                  0:00:00.071323\n",
      "\n",
      "-------------------- Epoch 2 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 17:30:26.013083\n",
      "Average Training Loss: 1.5592435830976905\n",
      "Time Taken:            0:00:03.330103\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 17:30:29.343545\n",
      "Average Validation Loss:     1.4715739602134341\n",
      "Average Validation Accuracy: 0.43253968253968256\n",
      "Time Taken:                  0:00:00.070881\n",
      "\n",
      "-------------------- Epoch 3 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 17:30:29.415208\n",
      "Average Training Loss: 1.388458004811915\n",
      "Time Taken:            0:00:03.347054\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 17:30:32.762652\n",
      "Average Validation Loss:     1.4093756108056932\n",
      "Average Validation Accuracy: 0.46825396825396826\n",
      "Time Taken:                  0:00:00.072481\n",
      "\n",
      "-------------------- Epoch 4 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 17:30:32.836042\n",
      "Average Training Loss: 1.3129465674481742\n",
      "Time Taken:            0:00:03.407062\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 17:30:36.243474\n",
      "Average Validation Loss:     1.400752794174921\n",
      "Average Validation Accuracy: 0.4583333333333333\n",
      "Time Taken:                  0:00:00.071178\n",
      "No improvement for 1 epoch(s).\n",
      "\n",
      "-------------------- Epoch 5 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 17:30:36.315055\n",
      "Average Training Loss: 1.2699459689419443\n",
      "Time Taken:            0:00:03.410487\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 17:30:39.725920\n",
      "Average Validation Loss:     1.3710642797606332\n",
      "Average Validation Accuracy: 0.47817460317460314\n",
      "Time Taken:                  0:00:00.071300\n",
      "\n",
      "-------------------- Epoch 6 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 17:30:39.798022\n",
      "Average Training Loss: 1.2496181537465352\n",
      "Time Taken:            0:00:03.409123\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 17:30:43.207533\n",
      "Average Validation Loss:     1.3852826328504653\n",
      "Average Validation Accuracy: 0.46825396825396826\n",
      "Time Taken:                  0:00:00.068828\n",
      "No improvement for 1 epoch(s).\n",
      "\n",
      "-------------------- Epoch 7 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 17:30:43.276730\n",
      "Average Training Loss: 1.2242298177102717\n",
      "Time Taken:            0:00:03.337274\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 17:30:46.614403\n",
      "Average Validation Loss:     1.3775281054633004\n",
      "Average Validation Accuracy: 0.46825396825396826\n",
      "Time Taken:                  0:00:00.070570\n",
      "No improvement for 2 epoch(s).\n",
      "\n",
      "-------------------- Epoch 8 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 17:30:46.685337\n",
      "Average Training Loss: 1.2135174979523915\n",
      "Time Taken:            0:00:03.327413\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 17:30:50.013138\n",
      "Average Validation Loss:     1.3473247119358607\n",
      "Average Validation Accuracy: 0.4880952380952381\n",
      "Time Taken:                  0:00:00.067953\n",
      "\n",
      "-------------------- Epoch 9 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 17:30:50.081903\n",
      "Average Training Loss: 1.1874226222677928\n",
      "Time Taken:            0:00:03.314299\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 17:30:53.396583\n",
      "Average Validation Loss:     1.389089164279756\n",
      "Average Validation Accuracy: 0.4583333333333333\n",
      "Time Taken:                  0:00:00.067799\n",
      "No improvement for 1 epoch(s).\n",
      "\n",
      "-------------------- Epoch 10 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 17:30:53.464726\n",
      "Average Training Loss: 1.175111175310321\n",
      "Time Taken:            0:00:03.345209\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 17:30:56.810366\n",
      "Average Validation Loss:     1.3837223847707112\n",
      "Average Validation Accuracy: 0.4583333333333333\n",
      "Time Taken:                  0:00:00.070047\n",
      "No improvement for 2 epoch(s).\n",
      "\n",
      "Total training time: 0:00:34.626546\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.00      0.00      0.00        40\n",
      "       white       0.54      1.00      0.70       110\n",
      "       black       0.00      0.00      0.00        23\n",
      "      yellow       0.00      0.00      0.00        19\n",
      "       green       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           0.54       204\n",
      "   macro avg       0.11      0.20      0.14       204\n",
      "weighted avg       0.29      0.54      0.38       204\n",
      "\n",
      "Score: 0.5392\n",
      "Trying configuration 2/2: {'epochs': 10, 'model_dropout': 0.1, 'optimizer_lr': 2e-05, 'scheduler_warmup_steps': 0, 'lora_r': 16, 'lora_alpha': 64, 'lora_dropout': 0.05, 'tokenizer_max_length': 32, 'dataloader_batch_size': 32, 'clip_grad_norm': 5.0, 'early_stopping_patience': 10, 'early_stopping_delta': 0.1, 'scheduler_type': 'cosine', 'optimizer_type': 'AdamW', 'weight_decay': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emiliano/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/emiliano/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/emiliano/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/emiliano/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/emiliano/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/emiliano/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Epoch 1 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 17:30:57.092997\n",
      "Average Training Loss: 1.17313210339081\n",
      "Time Taken:            0:00:03.468951\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 17:31:00.562367\n",
      "Average Validation Loss:     1.3038132474535988\n",
      "Average Validation Accuracy: 0.5079365079365079\n",
      "Time Taken:                  0:00:00.141896\n",
      "\n",
      "-------------------- Epoch 2 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 17:31:00.705119\n",
      "Average Training Loss: 0.9270630849570762\n",
      "Time Taken:            0:00:03.451904\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 17:31:04.157410\n",
      "Average Validation Loss:     1.2575146499134244\n",
      "Average Validation Accuracy: 0.5218253968253969\n",
      "Time Taken:                  0:00:00.142290\n",
      "No improvement for 1 epoch(s).\n",
      "\n",
      "-------------------- Epoch 3 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 17:31:04.300063\n",
      "Average Training Loss: 0.6803209082019038\n",
      "Time Taken:            0:00:03.479247\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 17:31:07.779701\n",
      "Average Validation Loss:     1.3309496953373863\n",
      "Average Validation Accuracy: 0.5297619047619048\n",
      "Time Taken:                  0:00:00.128759\n",
      "No improvement for 2 epoch(s).\n",
      "\n",
      "-------------------- Epoch 4 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 17:31:07.908849\n",
      "Average Training Loss: 0.432634638849555\n",
      "Time Taken:            0:00:03.477299\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 17:31:11.386551\n",
      "Average Validation Loss:     1.4526864673410143\n",
      "Average Validation Accuracy: 0.5119047619047619\n",
      "Time Taken:                  0:00:00.144294\n",
      "No improvement for 3 epoch(s).\n",
      "\n",
      "-------------------- Epoch 5 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 17:31:11.531268\n",
      "Average Training Loss: 0.2603452858492369\n",
      "Time Taken:            0:00:03.448496\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 17:31:14.980142\n",
      "Average Validation Loss:     1.549245229789189\n",
      "Average Validation Accuracy: 0.5555555555555556\n",
      "Time Taken:                  0:00:00.137871\n",
      "No improvement for 4 epoch(s).\n",
      "\n",
      "-------------------- Epoch 6 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 17:31:15.118406\n",
      "Average Training Loss: 0.1499432995461109\n",
      "Time Taken:            0:00:03.485236\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 17:31:18.604072\n",
      "Average Validation Loss:     1.6451678247678847\n",
      "Average Validation Accuracy: 0.5436507936507936\n",
      "Time Taken:                  0:00:00.150881\n",
      "No improvement for 5 epoch(s).\n",
      "\n",
      "-------------------- Epoch 7 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 17:31:18.755376\n",
      "Average Training Loss: 0.09634686586242623\n",
      "Time Taken:            0:00:03.614400\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 17:31:22.370257\n",
      "Average Validation Loss:     1.652079707100278\n",
      "Average Validation Accuracy: 0.5297619047619048\n",
      "Time Taken:                  0:00:00.143654\n",
      "No improvement for 6 epoch(s).\n",
      "\n",
      "-------------------- Epoch 8 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 17:31:22.514340\n",
      "Average Training Loss: 0.07284055291334303\n",
      "Time Taken:            0:00:03.612463\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 17:31:26.127300\n",
      "Average Validation Loss:     1.731102472259885\n",
      "Average Validation Accuracy: 0.5218253968253969\n",
      "Time Taken:                  0:00:00.150223\n",
      "No improvement for 7 epoch(s).\n",
      "\n",
      "-------------------- Epoch 9 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 17:31:26.277991\n",
      "Average Training Loss: 0.07084266109982641\n",
      "Time Taken:            0:00:03.585753\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 17:31:29.864223\n",
      "Average Validation Loss:     1.7222713970002674\n",
      "Average Validation Accuracy: 0.5317460317460317\n",
      "Time Taken:                  0:00:00.150120\n",
      "No improvement for 8 epoch(s).\n",
      "\n",
      "-------------------- Epoch 10 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-24 17:31:30.014780\n",
      "Average Training Loss: 0.06378796360478169\n",
      "Time Taken:            0:00:03.621095\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-24 17:31:33.636379\n",
      "Average Validation Loss:     1.7038075097260021\n",
      "Average Validation Accuracy: 0.5357142857142857\n",
      "Time Taken:                  0:00:00.152256\n",
      "No improvement for 9 epoch(s).\n",
      "\n",
      "Total training time: 0:00:36.696244\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.46      0.45      0.46        40\n",
      "       white       0.69      0.78      0.74       110\n",
      "       black       0.21      0.13      0.16        23\n",
      "      yellow       0.40      0.32      0.35        19\n",
      "       green       0.25      0.25      0.25        12\n",
      "\n",
      "    accuracy                           0.57       204\n",
      "   macro avg       0.40      0.39      0.39       204\n",
      "weighted avg       0.54      0.57      0.55       204\n",
      "\n",
      "Score: 0.5686\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T15:31:44.172822Z",
     "start_time": "2025-05-24T15:31:43.996347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bert_util import scoring_fn\n",
    "tids, amids = bert_tokenize_data(tokenizer, pd.Series(test_df['utterance'].values), max_length=best_config[\"tokenizer_max_length\"])\n",
    "val_dataloader = get_data_loader(tids, amids, batch_size=best_config[\"dataloader_batch_size\"], shuffle=False)\n",
    "\n",
    "score = scoring_fn(best_model, val_dataloader, test_df['hat'].values)"
   ],
   "id": "ae6861a3c6aacf9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.46      0.45      0.46        40\n",
      "       white       0.69      0.78      0.74       110\n",
      "       black       0.21      0.13      0.16        23\n",
      "      yellow       0.40      0.32      0.35        19\n",
      "       green       0.25      0.25      0.25        12\n",
      "\n",
      "    accuracy                           0.57       204\n",
      "   macro avg       0.40      0.39      0.39       204\n",
      "weighted avg       0.54      0.57      0.55       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9440497c07a47c3c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
