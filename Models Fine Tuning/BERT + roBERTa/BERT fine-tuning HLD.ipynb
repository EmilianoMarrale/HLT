{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:42:15.169816Z",
     "start_time": "2025-05-21T16:42:11.679683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from bert_util import bert_tokenize_data, tensor_train_test_split, train_bert_model, model_predict, get_data_loader, \\\n",
    "    calculate_accuracy\n",
    "from util import get_dataframe_from_json\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "os.environ[\"USE_TF\"] = \"0\""
   ],
   "id": "e79ad018e19671f2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emiliano/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-21 18:42:14.005978: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-21 18:42:14.013432: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747845734.021803   15590 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747845734.024371   15590 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747845734.031396   15590 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747845734.031404   15590 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747845734.031405   15590 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747845734.031405   15590 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-21 18:42:14.033917: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:46:44.284366Z",
     "start_time": "2025-05-21T16:46:44.270943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = pd.read_json('../train_dataset.json', lines=False)\n",
    "test_df = pd.read_json('../test_dataset.json', lines=False)\n",
    "train_df"
   ],
   "id": "4d6cb2e52215c782",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     turn                                          utterance     emotion  \\\n",
       "0       3                               I'll take one, too.    happiness   \n",
       "1       8  You know, we are superior to other clothes com...  no_emotion   \n",
       "2       5                          Her new boyfriend, right?  no_emotion   \n",
       "3       9  How about recommending him to use the storage ...  no_emotion   \n",
       "4       1   Oh, a bouquet of flowers. It's very kind of you.    surprise   \n",
       "..    ...                                                ...         ...   \n",
       "808     0                    I prefer potatoes to eggplants.  no_emotion   \n",
       "809     0  Mr. Smith, I would like to get right to the po...  no_emotion   \n",
       "810     4                                              Yeah?  no_emotion   \n",
       "811     0                             I am so bored all day.  no_emotion   \n",
       "812     2                           Do you play much tennis?  no_emotion   \n",
       "\n",
       "            act  hat  \n",
       "0        inform    0  \n",
       "1        inform    3  \n",
       "2    commissive    1  \n",
       "3     directive    4  \n",
       "4    commissive    1  \n",
       "..          ...  ...  \n",
       "808      inform    0  \n",
       "809    question    1  \n",
       "810    question    1  \n",
       "811      inform    0  \n",
       "812    question    1  \n",
       "\n",
       "[813 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turn</th>\n",
       "      <th>utterance</th>\n",
       "      <th>emotion</th>\n",
       "      <th>act</th>\n",
       "      <th>hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>I'll take one, too.</td>\n",
       "      <td>happiness</td>\n",
       "      <td>inform</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>You know, we are superior to other clothes com...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Her new boyfriend, right?</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>commissive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>How about recommending him to use the storage ...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>directive</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Oh, a bouquet of flowers. It's very kind of you.</td>\n",
       "      <td>surprise</td>\n",
       "      <td>commissive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>0</td>\n",
       "      <td>I prefer potatoes to eggplants.</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>0</td>\n",
       "      <td>Mr. Smith, I would like to get right to the po...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>4</td>\n",
       "      <td>Yeah?</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>0</td>\n",
       "      <td>I am so bored all day.</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>2</td>\n",
       "      <td>Do you play much tennis?</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>813 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:46:57.693106Z",
     "start_time": "2025-05-21T16:46:57.051231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "token_ids, attention_masks = bert_tokenize_data(tokenizer, train_df['utterance'].values)\n",
    "train_dataloader, val_dataloader = tensor_train_test_split(torch.tensor(train_df['hat'].values), token_ids, attention_masks, test_size=0.1)"
   ],
   "id": "e70d7ca38d0438c6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:49:00.120561Z",
     "start_time": "2025-05-21T16:47:08.514257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "epochs = 10\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "num_training_steps = epochs * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)\n",
    "\n",
    "model = train_bert_model(model, optimizer, scheduler, train_dataloader, val_dataloader, epochs)"
   ],
   "id": "8eac212dbbcdf59a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Epoch 1 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:47:09.612088\n",
      "Average Loss:     1.2928701347630958\n",
      "Time Taken:       0:00:10.870746\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:47:20.483301\n",
      "Average Loss:     1.348430503498424\n",
      "Average Accuracy: 0.48863636363636365\n",
      "Time Taken:       0:00:00.375991\n",
      "\n",
      "-------------------- Epoch 2 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:47:20.859794\n",
      "Average Loss:     1.068808484012666\n",
      "Time Taken:       0:00:10.467976\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:47:31.328147\n",
      "Average Loss:     1.4532366665926846\n",
      "Average Accuracy: 0.5\n",
      "Time Taken:       0:00:00.382272\n",
      "\n",
      "-------------------- Epoch 3 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:47:31.710835\n",
      "Average Loss:     0.8163206653426522\n",
      "Time Taken:       0:00:10.477185\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:47:42.188428\n",
      "Average Loss:     1.3265056691386483\n",
      "Average Accuracy: 0.48863636363636365\n",
      "Time Taken:       0:00:00.386425\n",
      "\n",
      "-------------------- Epoch 4 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:47:42.575291\n",
      "Average Loss:     0.5348018706654725\n",
      "Time Taken:       0:00:10.509215\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:47:53.084913\n",
      "Average Loss:     1.2252714037895203\n",
      "Average Accuracy: 0.5795454545454546\n",
      "Time Taken:       0:00:00.382919\n",
      "\n",
      "-------------------- Epoch 5 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:47:53.468222\n",
      "Average Loss:     0.3137663394394938\n",
      "Time Taken:       0:00:10.545931\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:48:04.014615\n",
      "Average Loss:     1.550137628208507\n",
      "Average Accuracy: 0.5340909090909091\n",
      "Time Taken:       0:00:00.386300\n",
      "\n",
      "-------------------- Epoch 6 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:48:04.401329\n",
      "Average Loss:     0.18288733282774364\n",
      "Time Taken:       0:00:10.543337\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:48:14.945209\n",
      "Average Loss:     1.9965556155551563\n",
      "Average Accuracy: 0.4772727272727273\n",
      "Time Taken:       0:00:00.388726\n",
      "\n",
      "-------------------- Epoch 7 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:48:15.334526\n",
      "Average Loss:     0.11181423331752581\n",
      "Time Taken:       0:00:11.257842\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:48:26.592858\n",
      "Average Loss:     1.9777574945579877\n",
      "Average Accuracy: 0.5454545454545454\n",
      "Time Taken:       0:00:00.386504\n",
      "\n",
      "-------------------- Epoch 8 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:48:26.979854\n",
      "Average Loss:     0.07782443788955393\n",
      "Time Taken:       0:00:10.633556\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:48:37.613814\n",
      "Average Loss:     1.9499297683889216\n",
      "Average Accuracy: 0.5454545454545454\n",
      "Time Taken:       0:00:00.388324\n",
      "\n",
      "-------------------- Epoch 9 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:48:38.002552\n",
      "Average Loss:     0.054717395019353084\n",
      "Time Taken:       0:00:10.629486\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:48:48.632444\n",
      "Average Loss:     2.088304481723092\n",
      "Average Accuracy: 0.5568181818181818\n",
      "Time Taken:       0:00:00.399701\n",
      "\n",
      "-------------------- Epoch 10 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:48:49.032545\n",
      "Average Loss:     0.04353815239712434\n",
      "Time Taken:       0:00:10.693543\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:48:59.726880\n",
      "Average Loss:     2.1481332670558584\n",
      "Average Accuracy: 0.5568181818181818\n",
      "Time Taken:       0:00:00.392326\n",
      "\n",
      "Total training time: 0:01:50.507955\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:49:00.658980Z",
     "start_time": "2025-05-21T16:49:00.134727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_texts = test_df['utterance'].values\n",
    "serie = pd.Series(test_texts)\n",
    "tids, amids = bert_tokenize_data(tokenizer, serie, max_length=64)\n",
    "dl = get_data_loader(tids, amids, batch_size=5, shuffle=False)\n",
    "preds, confidences = model_predict(model, dl)\n",
    "labels_flat = test_df['hat'].values.flatten()\n",
    "accuracy = np.sum(preds == labels_flat) / len(labels_flat)\n",
    "accuracy"
   ],
   "id": "d4f71d611d87e5b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5735294117647058)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:49:02.956924Z",
     "start_time": "2025-05-21T16:49:02.941775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hat_map = {\n",
    "    0: \"red\",\n",
    "    1: \"white\",\n",
    "    2: \"black\",\n",
    "    3: \"yellow\",\n",
    "    4: \"green\",\n",
    "}\n",
    "preds_array = np.array(preds)\n",
    "print(classification_report(labels_flat, preds_array, target_names=list(hat_map.values())))"
   ],
   "id": "9b2a2e4b9deb1a83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.46      0.40      0.43        40\n",
      "       white       0.69      0.75      0.72       110\n",
      "       black       0.38      0.39      0.38        23\n",
      "      yellow       0.64      0.37      0.47        19\n",
      "       green       0.19      0.25      0.21        12\n",
      "\n",
      "    accuracy                           0.57       204\n",
      "   macro avg       0.47      0.43      0.44       204\n",
      "weighted avg       0.58      0.57      0.57       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:49:32.980818Z",
     "start_time": "2025-05-21T16:49:32.958916Z"
    }
   },
   "cell_type": "code",
   "source": "augmented_train_df = pd.read_json('../eda_train_dataset.json', lines=False)",
   "id": "12094416d5c6987e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:58:22.024666Z",
     "start_time": "2025-05-21T16:49:44.049212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "token_ids, attention_masks = bert_tokenize_data(tokenizer, augmented_train_df['utterance'].values)\n",
    "train_dataloader, val_dataloader = tensor_train_test_split(torch.tensor(augmented_train_df['hat'].values), token_ids, attention_masks, test_size=0.1)\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "epochs = 10\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "num_training_steps = epochs * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)\n",
    "\n",
    "model = train_bert_model(model, optimizer, scheduler, train_dataloader, val_dataloader, epochs)\n",
    "test_texts = test_df['utterance'].values\n",
    "serie = pd.Series(test_texts)\n",
    "tids, amids = bert_tokenize_data(tokenizer, serie, max_length=64)\n",
    "dl = get_data_loader(tids, amids, batch_size=5, shuffle=False)\n",
    "preds, confidences = model_predict(model, dl)\n",
    "labels_flat = test_df['hat'].values.flatten()\n",
    "\n",
    "preds_array = np.array(preds)\n",
    "print(classification_report(labels_flat, preds_array, target_names=list(hat_map.values())))"
   ],
   "id": "7a72c4f6b580b1d9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Epoch 1 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:49:45.599930\n",
      "Average Loss:     0.8352723453822158\n",
      "Time Taken:       0:00:49.121369\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:50:34.721732\n",
      "Average Loss:     1.6427994484597064\n",
      "Average Accuracy: 0.4376899696048632\n",
      "Time Taken:       0:00:01.687571\n",
      "\n",
      "-------------------- Epoch 2 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:50:36.409734\n",
      "Average Loss:     0.19497602208099393\n",
      "Time Taken:       0:00:49.416321\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:51:25.826465\n",
      "Average Loss:     1.0135578555054963\n",
      "Average Accuracy: 0.7815349544072948\n",
      "Time Taken:       0:00:01.698944\n",
      "\n",
      "-------------------- Epoch 3 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:51:27.525819\n",
      "Average Loss:     0.07621283072581728\n",
      "Time Taken:       0:00:49.632151\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:52:17.158519\n",
      "Average Loss:     1.231666887971632\n",
      "Average Accuracy: 0.7541793313069909\n",
      "Time Taken:       0:00:01.710382\n",
      "\n",
      "-------------------- Epoch 4 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:52:18.869434\n",
      "Average Loss:     0.02680864908466737\n",
      "Time Taken:       0:00:50.003383\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:53:08.873264\n",
      "Average Loss:     0.6529110647339196\n",
      "Average Accuracy: 0.8765197568389057\n",
      "Time Taken:       0:00:01.723106\n",
      "\n",
      "-------------------- Epoch 5 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:53:10.596770\n",
      "Average Loss:     0.015489446931209464\n",
      "Time Taken:       0:00:50.291185\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:54:00.888419\n",
      "Average Loss:     0.46924261628164654\n",
      "Average Accuracy: 0.9088145896656535\n",
      "Time Taken:       0:00:01.729271\n",
      "\n",
      "-------------------- Epoch 6 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:54:02.618108\n",
      "Average Loss:     0.01538447400683182\n",
      "Time Taken:       0:00:50.013795\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:54:52.632321\n",
      "Average Loss:     0.5539516341275575\n",
      "Average Accuracy: 0.9012158054711246\n",
      "Time Taken:       0:00:01.734657\n",
      "\n",
      "-------------------- Epoch 7 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:54:54.367345\n",
      "Average Loss:     0.00910563577684649\n",
      "Time Taken:       0:00:50.156465\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:55:44.524198\n",
      "Average Loss:     0.8235992171524211\n",
      "Average Accuracy: 0.875\n",
      "Time Taken:       0:00:01.747269\n",
      "\n",
      "-------------------- Epoch 8 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:55:46.271840\n",
      "Average Loss:     0.008155516401214487\n",
      "Time Taken:       0:00:50.259353\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:56:36.532004\n",
      "Average Loss:     0.6683721565242059\n",
      "Average Accuracy: 0.8852583586626139\n",
      "Time Taken:       0:00:01.721954\n",
      "\n",
      "-------------------- Epoch 9 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:56:38.254347\n",
      "Average Loss:     0.004453104325992414\n",
      "Time Taken:       0:00:49.945758\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:57:28.200643\n",
      "Average Loss:     0.7898830939539426\n",
      "Average Accuracy: 0.8723404255319149\n",
      "Time Taken:       0:00:01.742859\n",
      "\n",
      "-------------------- Epoch 10 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:57:29.944091\n",
      "Average Loss:     0.0038203647106290284\n",
      "Time Taken:       0:00:49.904555\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 18:58:19.849185\n",
      "Average Loss:     0.6627375366803422\n",
      "Average Accuracy: 0.8852583586626139\n",
      "Time Taken:       0:00:01.735604\n",
      "\n",
      "Total training time: 0:08:35.985286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.56      0.50      0.53        40\n",
      "       white       0.68      0.88      0.77       110\n",
      "       black       0.30      0.13      0.18        23\n",
      "      yellow       0.40      0.21      0.28        19\n",
      "       green       0.50      0.25      0.33        12\n",
      "\n",
      "    accuracy                           0.62       204\n",
      "   macro avg       0.49      0.39      0.42       204\n",
      "weighted avg       0.58      0.62      0.58       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a591cf113b708480"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
