{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T18:11:51.991975Z",
     "start_time": "2025-05-25T18:11:47.996141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from util import get_dataframe_from_json\n",
    "from sklearn.metrics import classification_report\n",
    "from bert_util import bert_tokenize_data, tensor_train_test_split, train_bert_model, model_predict, get_data_loader, \\\n",
    "    train_pipeline, randomized_cv_search\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from bert_util import scoring_fn\n",
    "os.environ[\"USE_TF\"] = \"0\""
   ],
   "id": "e79ad018e19671f2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emiliano/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-25 20:11:50.287062: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-25 20:11:50.294392: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748196710.303578   13688 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748196710.306315   13688 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748196710.313539   13688 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748196710.313550   13688 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748196710.313551   13688 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748196710.313552   13688 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-25 20:11:50.316030: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-25 20:11:51,893\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-05-25 20:11:51,947\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Normal Hand-Labeled Dataset",
   "id": "51c5e17db8716ba2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T18:11:52.080327Z",
     "start_time": "2025-05-25T18:11:52.068511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "normal_train_df = pd.read_json('../normal_train_dataset.json', lines=False)\n",
    "normal_test_df = pd.read_json('../normal_test_dataset.json', lines=False)\n",
    "normal_train_df"
   ],
   "id": "4d6cb2e52215c782",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     turn                                          utterance     emotion  \\\n",
       "0       3                               I'll take one, too.    happiness   \n",
       "1       8  You know, we are superior to other clothes com...  no_emotion   \n",
       "2       5                          Her new boyfriend, right?  no_emotion   \n",
       "3       9  How about recommending him to use the storage ...  no_emotion   \n",
       "4       1   Oh, a bouquet of flowers. It's very kind of you.    surprise   \n",
       "..    ...                                                ...         ...   \n",
       "808     0                    I prefer potatoes to eggplants.  no_emotion   \n",
       "809     0  Mr. Smith, I would like to get right to the po...  no_emotion   \n",
       "810     4                                              Yeah?  no_emotion   \n",
       "811     0                             I am so bored all day.  no_emotion   \n",
       "812     2                           Do you play much tennis?  no_emotion   \n",
       "\n",
       "            act  hat  \n",
       "0        inform    0  \n",
       "1        inform    3  \n",
       "2    commissive    1  \n",
       "3     directive    4  \n",
       "4    commissive    1  \n",
       "..          ...  ...  \n",
       "808      inform    0  \n",
       "809    question    1  \n",
       "810    question    1  \n",
       "811      inform    0  \n",
       "812    question    1  \n",
       "\n",
       "[813 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turn</th>\n",
       "      <th>utterance</th>\n",
       "      <th>emotion</th>\n",
       "      <th>act</th>\n",
       "      <th>hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>I'll take one, too.</td>\n",
       "      <td>happiness</td>\n",
       "      <td>inform</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>You know, we are superior to other clothes com...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Her new boyfriend, right?</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>commissive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>How about recommending him to use the storage ...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>directive</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Oh, a bouquet of flowers. It's very kind of you.</td>\n",
       "      <td>surprise</td>\n",
       "      <td>commissive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>0</td>\n",
       "      <td>I prefer potatoes to eggplants.</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>0</td>\n",
       "      <td>Mr. Smith, I would like to get right to the po...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>4</td>\n",
       "      <td>Yeah?</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>0</td>\n",
       "      <td>I am so bored all day.</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>2</td>\n",
       "      <td>Do you play much tennis?</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>813 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Baseline BERT over the Normal Hand-Labeled Dataset",
   "id": "108d69a16f91ad58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "token_ids, attention_masks = bert_tokenize_data(tokenizer, normal_train_df['utterance'].values)\n",
    "train_dataloader, val_dataloader = tensor_train_test_split(torch.tensor(normal_train_df['hat'].values), token_ids, attention_masks, test_size=0.1)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "configs = {\n",
    "        \"epochs\": 10,\n",
    "        \"clip_grad_norm\": 1.0,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"early_stopping_delta\": 0.1,\n",
    "    }\n",
    "\n",
    "num_training_steps = 10 * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)\n",
    "\n",
    "model = train_bert_model(model, optimizer, scheduler, train_dataloader, val_dataloader, configs)"
   ],
   "id": "e70d7ca38d0438c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T18:12:42.762171Z",
     "start_time": "2025-05-25T18:12:42.435958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_texts = normal_test_df['utterance'].values\n",
    "serie = pd.Series(test_texts)\n",
    "tids, amids = bert_tokenize_data(tokenizer, serie, max_length=64)\n",
    "dl = get_data_loader(tids, amids, batch_size=5, shuffle=False)\n",
    "preds, confidences = model_predict(model, dl)\n",
    "labels_flat = normal_test_df['hat'].values.flatten()\n",
    "accuracy = np.sum(preds == labels_flat) / len(labels_flat)"
   ],
   "id": "d4f71d611d87e5b5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T18:12:42.778183Z",
     "start_time": "2025-05-25T18:12:42.773559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hat_map = {\n",
    "    0: \"red\",\n",
    "    1: \"white\",\n",
    "    2: \"black\",\n",
    "    3: \"yellow\",\n",
    "    4: \"green\",\n",
    "}\n",
    "preds_array = np.array(preds)\n",
    "print(classification_report(labels_flat, preds_array, target_names=list(hat_map.values())))"
   ],
   "id": "9b2a2e4b9deb1a83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.53      0.50      0.51        40\n",
      "       white       0.75      0.75      0.75       110\n",
      "       black       0.33      0.26      0.29        23\n",
      "      yellow       0.48      0.63      0.55        19\n",
      "       green       0.17      0.17      0.17        12\n",
      "\n",
      "    accuracy                           0.60       204\n",
      "   macro avg       0.45      0.46      0.45       204\n",
      "weighted avg       0.60      0.60      0.60       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# HyperParameter Tuning over the Normal Hand-Labeled Dataset",
   "id": "b7d3e1df4200b529"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_model():\n",
    "    return BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "best_model, best_config, best_score = randomized_cv_search(build_model, tokenizer, normal_train_df['utterance'], normal_train_df['hat'], num_folds=5, num_samples=20, use_lora=True)\n"
   ],
   "id": "f80560c9fd63836e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T17:31:01.507377Z",
     "start_time": "2025-05-25T17:31:00.963932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tids, amids = bert_tokenize_data(tokenizer, pd.Series(normal_test_df['utterance'].values), max_length=best_config[\"tokenizer_max_length\"])\n",
    "val_dataloader = get_data_loader(tids, amids, batch_size=best_config[\"dataloader_batch_size\"], shuffle=False)\n",
    "print(f\"Best config: {best_config}\")\n",
    "score = scoring_fn(best_model, val_dataloader, normal_test_df['hat'].values)\n",
    "#Best config: {'epochs': 15, 'model_dropout': 0.3, 'optimizer_lr': 0.0001, 'scheduler_warmup_steps': 200, 'lora_r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1, 'tokenizer_max_length': 128, 'dataloader_batch_size': 16, 'clip_grad_norm': 2.0, 'early_stopping_patience': 10, 'early_stopping_delta': 0.01, 'scheduler_type': 'constant', 'optimizer_type': 'AdamW', 'weight_decay': 0.0001}"
   ],
   "id": "23a232e9aebc0697",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best config: {'epochs': 15, 'model_dropout': 0.3, 'optimizer_lr': 0.0001, 'scheduler_warmup_steps': 200, 'lora_r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1, 'tokenizer_max_length': 128, 'dataloader_batch_size': 16, 'clip_grad_norm': 2.0, 'early_stopping_patience': 10, 'early_stopping_delta': 0.01, 'scheduler_type': 'constant', 'optimizer_type': 'AdamW', 'weight_decay': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.40      0.40      0.40        40\n",
      "       white       0.69      0.80      0.74       110\n",
      "       black       0.33      0.17      0.23        23\n",
      "      yellow       0.29      0.26      0.28        19\n",
      "       green       0.25      0.17      0.20        12\n",
      "\n",
      "    accuracy                           0.56       204\n",
      "   macro avg       0.39      0.36      0.37       204\n",
      "weighted avg       0.53      0.56      0.54       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# EDA Augmented Hand-Labeled Dataset",
   "id": "3b0d37bc4ec9baa7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T17:31:25.151492Z",
     "start_time": "2025-05-25T17:31:25.139051Z"
    }
   },
   "cell_type": "code",
   "source": "augmented_train_df = pd.read_json('../eda_train_dataset.json', lines=False)",
   "id": "12094416d5c6987e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T17:37:26.919697Z",
     "start_time": "2025-05-25T17:31:29.649679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "token_ids, attention_masks = bert_tokenize_data(tokenizer, augmented_train_df['utterance'].values)\n",
    "train_dataloader, val_dataloader = tensor_train_test_split(torch.tensor(augmented_train_df['hat'].values), token_ids, attention_masks, test_size=0.1)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "configs = {\n",
    "        \"epochs\": 10,\n",
    "        \"clip_grad_norm\": 1.0,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"early_stopping_delta\": 0.1,\n",
    "    }\n",
    "\n",
    "num_training_steps = 10 * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)\n",
    "\n",
    "model = train_bert_model(model, optimizer, scheduler, train_dataloader, val_dataloader, configs)"
   ],
   "id": "7a72c4f6b580b1d9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Epoch 1 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-25 19:31:31.164460\n",
      "Average Training Loss: 0.8062743402999824\n",
      "Time Taken:            0:00:34.284827\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-25 19:32:05.449674\n",
      "Average Validation Loss:     1.163320651079746\n",
      "Average Validation Accuracy: 0.5946048632218845\n",
      "Time Taken:                  0:00:01.075927\n",
      "\n",
      "-------------------- Epoch 2 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-25 19:32:06.527131\n",
      "Average Training Loss: 0.19978863342870867\n",
      "Time Taken:            0:00:34.782225\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-25 19:32:41.309958\n",
      "Average Validation Loss:     0.9578507408658241\n",
      "Average Validation Accuracy: 0.7572188449848024\n",
      "Time Taken:                  0:00:01.073582\n",
      "\n",
      "-------------------- Epoch 3 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-25 19:32:42.384912\n",
      "Average Training Loss: 0.0871732396174846\n",
      "Time Taken:            0:00:34.839396\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-25 19:33:17.224765\n",
      "Average Validation Loss:     1.495265507218527\n",
      "Average Validation Accuracy: 0.7226443768996961\n",
      "Time Taken:                  0:00:01.010429\n",
      "No improvement for 1 epoch(s).\n",
      "\n",
      "-------------------- Epoch 4 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-25 19:33:18.235658\n",
      "Average Training Loss: 0.03407926647965365\n",
      "Time Taken:            0:00:34.374887\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-25 19:33:52.610981\n",
      "Average Validation Loss:     0.6779200806879399\n",
      "Average Validation Accuracy: 0.8537234042553191\n",
      "Time Taken:                  0:00:01.050225\n",
      "\n",
      "-------------------- Epoch 5 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-25 19:33:53.662542\n",
      "Average Training Loss: 0.022467639998434943\n",
      "Time Taken:            0:00:34.239799\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-25 19:34:27.902843\n",
      "Average Validation Loss:     0.8408631975428161\n",
      "Average Validation Accuracy: 0.848404255319149\n",
      "Time Taken:                  0:00:01.050775\n",
      "No improvement for 1 epoch(s).\n",
      "\n",
      "-------------------- Epoch 6 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-25 19:34:28.954143\n",
      "Average Training Loss: 0.0105437625181859\n",
      "Time Taken:            0:00:34.264237\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-25 19:35:03.218858\n",
      "Average Validation Loss:     0.6414028212999023\n",
      "Average Validation Accuracy: 0.868161094224924\n",
      "Time Taken:                  0:00:01.048673\n",
      "No improvement for 2 epoch(s).\n",
      "\n",
      "-------------------- Epoch 7 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-25 19:35:04.268023\n",
      "Average Training Loss: 0.007156913024695499\n",
      "Time Taken:            0:00:34.864145\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-25 19:35:39.132671\n",
      "Average Validation Loss:     0.4135956579721483\n",
      "Average Validation Accuracy: 0.8985562310030395\n",
      "Time Taken:                  0:00:01.066726\n",
      "\n",
      "-------------------- Epoch 8 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-25 19:35:40.200978\n",
      "Average Training Loss: 0.008421260257477754\n",
      "Time Taken:            0:00:34.879516\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-25 19:36:15.080945\n",
      "Average Validation Loss:     0.3471482001409737\n",
      "Average Validation Accuracy: 0.9122340425531915\n",
      "Time Taken:                  0:00:01.054686\n",
      "No improvement for 1 epoch(s).\n",
      "\n",
      "-------------------- Epoch 9 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-25 19:36:16.136149\n",
      "Average Training Loss: 0.005201591223951444\n",
      "Time Taken:            0:00:34.366472\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-25 19:36:50.503079\n",
      "Average Validation Loss:     0.5293521928471547\n",
      "Average Validation Accuracy: 0.8746200607902735\n",
      "Time Taken:                  0:00:01.050580\n",
      "No improvement for 2 epoch(s).\n",
      "\n",
      "-------------------- Epoch 10 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-25 19:36:51.554145\n",
      "Average Training Loss: 0.004164888820122328\n",
      "Time Taken:            0:00:34.308510\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-25 19:37:25.863091\n",
      "Average Validation Loss:     0.6033394047259749\n",
      "Average Validation Accuracy: 0.8696808510638298\n",
      "Time Taken:                  0:00:01.053045\n",
      "No improvement for 3 epoch(s).\n",
      "\n",
      "Early stopping triggered after 10 epochs.\n",
      "\n",
      "Total training time: 0:05:55.754319\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T17:37:27.269054Z",
     "start_time": "2025-05-25T17:37:26.932479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_texts = normal_test_df['utterance'].values\n",
    "serie = pd.Series(test_texts)\n",
    "tids, amids = bert_tokenize_data(tokenizer, serie, max_length=64)\n",
    "dl = get_data_loader(tids, amids, batch_size=5, shuffle=False)\n",
    "preds, confidences = model_predict(model, dl)\n",
    "labels_flat = normal_test_df['hat'].values.flatten()\n",
    "accuracy = np.sum(preds == labels_flat) / len(labels_flat)\n",
    "hat_map = {\n",
    "    0: \"red\",\n",
    "    1: \"white\",\n",
    "    2: \"black\",\n",
    "    3: \"yellow\",\n",
    "    4: \"green\",\n",
    "}\n",
    "preds_array = np.array(preds)\n",
    "print(classification_report(labels_flat, preds_array, target_names=list(hat_map.values())))"
   ],
   "id": "c77ace27fdba0741",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.63      0.47      0.54        40\n",
      "       white       0.66      0.89      0.76       110\n",
      "       black       0.29      0.09      0.13        23\n",
      "      yellow       0.78      0.37      0.50        19\n",
      "       green       0.56      0.42      0.48        12\n",
      "\n",
      "    accuracy                           0.64       204\n",
      "   macro avg       0.58      0.45      0.48       204\n",
      "weighted avg       0.62      0.64      0.60       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### HyperParameter Tuning over the EDA Augmented Hand-Labeled Dataset",
   "id": "d1bf8d998a77b114"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_model():\n",
    "    return BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "best_model, best_config, best_score = randomized_cv_search(build_model, tokenizer, augmented_train_df['utterance'], augmented_train_df['hat'], num_folds=2, num_samples=10, use_lora=True)"
   ],
   "id": "f31099ab3a42785",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T17:55:47.099883Z",
     "start_time": "2025-05-25T17:55:46.550309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tids, amids = bert_tokenize_data(tokenizer, pd.Series(normal_test_df['utterance'].values), max_length=best_config[\"tokenizer_max_length\"])\n",
    "val_dataloader = get_data_loader(tids, amids, batch_size=best_config[\"dataloader_batch_size\"], shuffle=False)\n",
    "print(f\"Best config: {best_config}\")\n",
    "score = scoring_fn(best_model, val_dataloader, normal_test_df['hat'].values)"
   ],
   "id": "ec4222ce2d7ff39c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best config: {'epochs': 15, 'model_dropout': 0.3, 'optimizer_lr': 0.0001, 'scheduler_warmup_steps': 200, 'lora_r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1, 'tokenizer_max_length': 128, 'dataloader_batch_size': 16, 'clip_grad_norm': 2.0, 'early_stopping_patience': 10, 'early_stopping_delta': 0.01, 'scheduler_type': 'constant', 'optimizer_type': 'AdamW', 'weight_decay': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.55      0.30      0.39        40\n",
      "       white       0.64      0.88      0.74       110\n",
      "       black       0.40      0.26      0.32        23\n",
      "      yellow       0.55      0.32      0.40        19\n",
      "       green       0.50      0.17      0.25        12\n",
      "\n",
      "    accuracy                           0.60       204\n",
      "   macro avg       0.53      0.39      0.42       204\n",
      "weighted avg       0.58      0.60      0.56       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Randomized Search for Hyperparameter Tuning usage example",
   "id": "2c6e51b7f17b3c20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# BERT over the Automated Labeled Dataset",
   "id": "df88265d66c1334b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T17:57:17.043368Z",
     "start_time": "2025-05-25T17:57:17.031567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ald_train_df = pd.read_json('../ald_train_dataset.json', lines=False)\n",
    "ald_test_df = pd.read_json('../ald_test_dataset.json', lines=False)"
   ],
   "id": "cae3f54e46c49f2e",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "token_ids, attention_masks = bert_tokenize_data(tokenizer, ald_train_df['utterance'].values)\n",
    "train_dataloader, val_dataloader = tensor_train_test_split(torch.tensor(ald_train_df['hat'].values), token_ids, attention_masks, test_size=0.1)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "configs = {\n",
    "        \"epochs\": 10,\n",
    "        \"clip_grad_norm\": 1.0,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"early_stopping_delta\": 0.1,\n",
    "    }\n",
    "\n",
    "num_training_steps = 10 * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)\n",
    "\n",
    "model = train_bert_model(model, optimizer, scheduler, train_dataloader, val_dataloader, configs)"
   ],
   "id": "f37fb0cff2d5afa6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T17:58:29.692512Z",
     "start_time": "2025-05-25T17:58:29.154240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bert_util import scoring_fn\n",
    "tids, amids = bert_tokenize_data(tokenizer, pd.Series(ald_test_df['utterance'].values), max_length=best_config[\"tokenizer_max_length\"])\n",
    "val_dataloader = get_data_loader(tids, amids, batch_size=best_config[\"dataloader_batch_size\"], shuffle=False)\n",
    "\n",
    "score = scoring_fn(best_model, val_dataloader, ald_test_df['hat'].values)"
   ],
   "id": "e26e2c10b085d56b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.55      0.30      0.39        40\n",
      "       white       0.64      0.88      0.74       110\n",
      "       black       0.40      0.26      0.32        23\n",
      "      yellow       0.55      0.32      0.40        19\n",
      "       green       0.50      0.17      0.25        12\n",
      "\n",
      "    accuracy                           0.60       204\n",
      "   macro avg       0.53      0.39      0.42       204\n",
      "weighted avg       0.58      0.60      0.56       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_model():\n",
    "    return BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "best_model, best_config, best_score = randomized_cv_search(build_model, tokenizer, ald_train_df['utterance'], ald_train_df['hat'], num_folds=2, num_samples=10, use_lora=True)\n"
   ],
   "id": "6512d1475d97e995",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T18:07:47.845028Z",
     "start_time": "2025-05-25T18:07:47.314049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bert_util import scoring_fn\n",
    "tids, amids = bert_tokenize_data(tokenizer, pd.Series(ald_test_df['utterance'].values), max_length=best_config[\"tokenizer_max_length\"])\n",
    "val_dataloader = get_data_loader(tids, amids, batch_size=best_config[\"dataloader_batch_size\"], shuffle=False)\n",
    "print(f\"Best config: {best_config}\")\n",
    "score = scoring_fn(best_model, val_dataloader, ald_test_df['hat'].values)"
   ],
   "id": "ae6861a3c6aacf9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best config: {'epochs': 15, 'model_dropout': 0.3, 'optimizer_lr': 0.0001, 'scheduler_warmup_steps': 200, 'lora_r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1, 'tokenizer_max_length': 128, 'dataloader_batch_size': 16, 'clip_grad_norm': 2.0, 'early_stopping_patience': 10, 'early_stopping_delta': 0.01, 'scheduler_type': 'constant', 'optimizer_type': 'AdamW', 'weight_decay': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.40      0.40      0.40        40\n",
      "       white       0.69      0.80      0.74       110\n",
      "       black       0.33      0.17      0.23        23\n",
      "      yellow       0.29      0.26      0.28        19\n",
      "       green       0.25      0.17      0.20        12\n",
      "\n",
      "    accuracy                           0.56       204\n",
      "   macro avg       0.39      0.36      0.37       204\n",
      "weighted avg       0.53      0.56      0.54       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b908f74d231c34d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
