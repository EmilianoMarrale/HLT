{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    RobertaTokenizerFast,\n",
    "    RobertaForSequenceClassification,\n",
    ")\n",
    "from bert_util import bert_tokenize_data, tensor_train_test_split, train_bert_model, model_predict, get_data_loader\n",
    "import pandas as pd\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Normal Hand-Labeled Dataset",
   "id": "d8d23dbebff65125"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T14:26:11.847208Z",
     "start_time": "2025-05-26T14:26:11.837628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "normal_train_df = pd.read_json('../normal_train_dataset.json', lines=False)\n",
    "normal_test_df = pd.read_json('../normal_test_dataset.json', lines=False)\n",
    "normal_train_df"
   ],
   "id": "d16978a9f79dc67d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     turn                                          utterance     emotion  \\\n",
       "0       3                               I'll take one, too.    happiness   \n",
       "1       8  You know, we are superior to other clothes com...  no_emotion   \n",
       "2       5                          Her new boyfriend, right?  no_emotion   \n",
       "3       9  How about recommending him to use the storage ...  no_emotion   \n",
       "4       1   Oh, a bouquet of flowers. It's very kind of you.    surprise   \n",
       "..    ...                                                ...         ...   \n",
       "808     0                    I prefer potatoes to eggplants.  no_emotion   \n",
       "809     0  Mr. Smith, I would like to get right to the po...  no_emotion   \n",
       "810     4                                              Yeah?  no_emotion   \n",
       "811     0                             I am so bored all day.  no_emotion   \n",
       "812     2                           Do you play much tennis?  no_emotion   \n",
       "\n",
       "            act  hat  \n",
       "0        inform    0  \n",
       "1        inform    3  \n",
       "2    commissive    1  \n",
       "3     directive    4  \n",
       "4    commissive    1  \n",
       "..          ...  ...  \n",
       "808      inform    0  \n",
       "809    question    1  \n",
       "810    question    1  \n",
       "811      inform    0  \n",
       "812    question    1  \n",
       "\n",
       "[813 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turn</th>\n",
       "      <th>utterance</th>\n",
       "      <th>emotion</th>\n",
       "      <th>act</th>\n",
       "      <th>hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>I'll take one, too.</td>\n",
       "      <td>happiness</td>\n",
       "      <td>inform</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>You know, we are superior to other clothes com...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Her new boyfriend, right?</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>commissive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>How about recommending him to use the storage ...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>directive</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Oh, a bouquet of flowers. It's very kind of you.</td>\n",
       "      <td>surprise</td>\n",
       "      <td>commissive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>0</td>\n",
       "      <td>I prefer potatoes to eggplants.</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>0</td>\n",
       "      <td>Mr. Smith, I would like to get right to the po...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>4</td>\n",
       "      <td>Yeah?</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>0</td>\n",
       "      <td>I am so bored all day.</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>2</td>\n",
       "      <td>Do you play much tennis?</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>813 rows Ã— 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Baseline roBERTa over the Normal Hand-Labeled Dataset",
   "id": "c1145f0980441204"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "token_ids, attention_masks = bert_tokenize_data(tokenizer, normal_train_df['utterance'].values)\n",
    "train_dataloader, val_dataloader = tensor_train_test_split(torch.tensor(normal_train_df['hat'].values), token_ids, attention_masks, test_size=0.1)\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "configs = {\n",
    "        \"epochs\": 10,\n",
    "        \"clip_grad_norm\": 1.0,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"early_stopping_delta\": 0.1,\n",
    "    }\n",
    "\n",
    "num_training_steps = 10 * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)\n",
    "\n",
    "model = train_bert_model(model, optimizer, scheduler, train_dataloader, val_dataloader, configs)"
   ],
   "id": "4e3c929710b56921",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T14:27:44.373134Z",
     "start_time": "2025-05-26T14:27:44.067551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_texts = normal_test_df['utterance'].values\n",
    "serie = pd.Series(test_texts)\n",
    "tids, amids = bert_tokenize_data(tokenizer, serie, max_length=64)\n",
    "dl = get_data_loader(tids, amids, batch_size=5, shuffle=False)\n",
    "preds, confidences = model_predict(model, dl)\n",
    "labels_flat = normal_test_df['hat'].values.flatten()\n",
    "accuracy = np.sum(preds == labels_flat) / len(labels_flat)"
   ],
   "id": "769ebae4e5fd3ac1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T14:27:46.165570Z",
     "start_time": "2025-05-26T14:27:46.154745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hat_map = {\n",
    "    0: \"red\",\n",
    "    1: \"white\",\n",
    "    2: \"black\",\n",
    "    3: \"yellow\",\n",
    "    4: \"green\",\n",
    "}\n",
    "preds_array = np.array(preds)\n",
    "print(classification_report(labels_flat, preds_array, target_names=list(hat_map.values())))"
   ],
   "id": "df72e41f94901ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.48      0.38      0.42        40\n",
      "       white       0.66      0.84      0.74       110\n",
      "       black       1.00      0.09      0.16        23\n",
      "      yellow       0.45      0.47      0.46        19\n",
      "       green       0.18      0.17      0.17        12\n",
      "\n",
      "    accuracy                           0.59       204\n",
      "   macro avg       0.55      0.39      0.39       204\n",
      "weighted avg       0.61      0.59      0.55       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Baseline roBERTa over the EDA Augmented Hand-Labeled Dataset",
   "id": "f4cb107f40bd9db5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T14:30:04.311326Z",
     "start_time": "2025-05-26T14:30:04.297773Z"
    }
   },
   "cell_type": "code",
   "source": "augmented_train_df = pd.read_json('../eda_train_dataset.json', lines=False)",
   "id": "b065b7e82ca15e34",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "token_ids, attention_masks = bert_tokenize_data(tokenizer, augmented_train_df['utterance'].values)\n",
    "train_dataloader, val_dataloader = tensor_train_test_split(torch.tensor(augmented_train_df['hat'].values), token_ids, attention_masks, test_size=0.1)\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "configs = {\n",
    "        \"epochs\": 10,\n",
    "        \"clip_grad_norm\": 1.0,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"early_stopping_delta\": 0.1,\n",
    "    }\n",
    "\n",
    "num_training_steps = 10 * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)\n",
    "\n",
    "model = train_bert_model(model, optimizer, scheduler, train_dataloader, val_dataloader, configs)"
   ],
   "id": "e708b30622088ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T14:32:44.362765Z",
     "start_time": "2025-05-26T14:32:44.057832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_texts = normal_test_df['utterance'].values\n",
    "serie = pd.Series(test_texts)\n",
    "tids, amids = bert_tokenize_data(tokenizer, serie, max_length=64)\n",
    "dl = get_data_loader(tids, amids, batch_size=5, shuffle=False)\n",
    "preds, confidences = model_predict(model, dl)\n",
    "labels_flat = normal_test_df['hat'].values.flatten()\n",
    "accuracy = np.sum(preds == labels_flat) / len(labels_flat)\n",
    "hat_map = {\n",
    "    0: \"red\",\n",
    "    1: \"white\",\n",
    "    2: \"black\",\n",
    "    3: \"yellow\",\n",
    "    4: \"green\",\n",
    "}\n",
    "preds_array = np.array(preds)\n",
    "print(classification_report(labels_flat, preds_array, target_names=list(hat_map.values())))"
   ],
   "id": "3098f2ffae6adfd5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.49      0.47      0.48        40\n",
      "       white       0.70      0.78      0.74       110\n",
      "       black       0.50      0.26      0.34        23\n",
      "      yellow       0.53      0.53      0.53        19\n",
      "       green       0.25      0.25      0.25        12\n",
      "\n",
      "    accuracy                           0.61       204\n",
      "   macro avg       0.49      0.46      0.47       204\n",
      "weighted avg       0.60      0.61      0.60       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Baseline roBERTa over the Automated Labeled Dataset",
   "id": "27034850567db170"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T14:38:52.021450Z",
     "start_time": "2025-05-26T14:38:52.013978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ald_train_df = pd.read_json('../ald_train_dataset.json', lines=False)\n",
    "ald_test_df = pd.read_json('../ald_test_dataset.json', lines=False)"
   ],
   "id": "48eafed8b663aa11",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "token_ids, attention_masks = bert_tokenize_data(tokenizer, ald_train_df['utterance'].values)\n",
    "train_dataloader, val_dataloader = tensor_train_test_split(torch.tensor(ald_train_df['hat'].values), token_ids, attention_masks, test_size=0.1)\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "configs = {\n",
    "        \"epochs\": 10,\n",
    "        \"clip_grad_norm\": 1.0,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"early_stopping_delta\": 0.1,\n",
    "    }\n",
    "\n",
    "num_training_steps = 10 * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)\n",
    "\n",
    "model = train_bert_model(model, optimizer, scheduler, train_dataloader, val_dataloader, configs)"
   ],
   "id": "635a604796a6ece7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T14:42:34.318307Z",
     "start_time": "2025-05-26T14:42:32.814251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_texts = ald_test_df['utterance'].values\n",
    "serie = pd.Series(test_texts)\n",
    "tids, amids = bert_tokenize_data(tokenizer, serie, max_length=64)\n",
    "dl = get_data_loader(tids, amids, batch_size=5, shuffle=False)\n",
    "preds, confidences = model_predict(model, dl)\n",
    "labels_flat = ald_test_df['hat'].values.flatten()\n",
    "accuracy = np.sum(preds == labels_flat) / len(labels_flat)\n",
    "hat_map = {\n",
    "    0: \"red\",\n",
    "    1: \"white\",\n",
    "    2: \"black\",\n",
    "    3: \"yellow\",\n",
    "    4: \"green\",\n",
    "}\n",
    "preds_array = np.array(preds)\n",
    "print(classification_report(labels_flat, preds_array, target_names=list(hat_map.values())))"
   ],
   "id": "1d2ff51805edd9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.75      0.77      0.76       200\n",
      "       white       0.42      0.39      0.41       200\n",
      "       black       0.51      0.59      0.55       200\n",
      "      yellow       0.50      0.67      0.57       200\n",
      "       green       0.65      0.37      0.47       200\n",
      "\n",
      "    accuracy                           0.56      1000\n",
      "   macro avg       0.57      0.56      0.55      1000\n",
      "weighted avg       0.57      0.56      0.55      1000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Testing the model on the normal test dataset",
   "id": "4c60b6b41f4713cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T14:42:34.591169Z",
     "start_time": "2025-05-26T14:42:34.332790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_texts = normal_test_df['utterance'].values\n",
    "serie = pd.Series(test_texts)\n",
    "tids, amids = bert_tokenize_data(tokenizer, serie, max_length=64)\n",
    "dl = get_data_loader(tids, amids, batch_size=5, shuffle=False)\n",
    "preds, confidences = model_predict(model, dl)\n",
    "labels_flat = normal_test_df['hat'].values.flatten()\n",
    "accuracy = np.sum(preds == labels_flat) / len(labels_flat)\n",
    "hat_map = {\n",
    "    0: \"red\",\n",
    "    1: \"white\",\n",
    "    2: \"black\",\n",
    "    3: \"yellow\",\n",
    "    4: \"green\",\n",
    "}\n",
    "preds_array = np.array(preds)\n",
    "print(classification_report(labels_flat, preds_array, target_names=list(hat_map.values())))"
   ],
   "id": "a37ba4eaba7cfa52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.32      0.30      0.31        40\n",
      "       white       0.69      0.47      0.56       110\n",
      "       black       0.19      0.22      0.20        23\n",
      "      yellow       0.21      0.58      0.31        19\n",
      "       green       0.25      0.25      0.25        12\n",
      "\n",
      "    accuracy                           0.41       204\n",
      "   macro avg       0.33      0.36      0.33       204\n",
      "weighted avg       0.49      0.41      0.43       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "38147ec1087bc1e2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
