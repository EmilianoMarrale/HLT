{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-28T14:49:30.699036Z",
     "start_time": "2025-05-28T14:49:23.308661Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    RobertaTokenizerFast,\n",
    "    RobertaForSequenceClassification,\n",
    ")\n",
    "from bert_util import bert_tokenize_data, tensor_train_test_split, train_bert_model, model_predict, get_data_loader\n",
    "import pandas as pd\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emiliano/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-28 16:49:27.947049: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-28 16:49:28.208330: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748443768.309007   27370 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748443768.332787   27370 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748443768.546260   27370 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748443768.546284   27370 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748443768.546288   27370 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748443768.546289   27370 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-28 16:49:28.567723: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:49:30,524\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-05-28 16:49:30,623\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Normal Hand-Labeled Dataset",
   "id": "d8d23dbebff65125"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T14:49:30.731872Z",
     "start_time": "2025-05-28T14:49:30.710983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "normal_train_df = pd.read_json('../normal_train_dataset.json', lines=False)\n",
    "normal_test_df = pd.read_json('../normal_test_dataset.json', lines=False)\n",
    "normal_train_df"
   ],
   "id": "d16978a9f79dc67d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     turn                                          utterance     emotion  \\\n",
       "0       3                               I'll take one, too.    happiness   \n",
       "1       8  You know, we are superior to other clothes com...  no_emotion   \n",
       "2       5                          Her new boyfriend, right?  no_emotion   \n",
       "3       9  How about recommending him to use the storage ...  no_emotion   \n",
       "4       1   Oh, a bouquet of flowers. It's very kind of you.    surprise   \n",
       "..    ...                                                ...         ...   \n",
       "808     0                    I prefer potatoes to eggplants.  no_emotion   \n",
       "809     0  Mr. Smith, I would like to get right to the po...  no_emotion   \n",
       "810     4                                              Yeah?  no_emotion   \n",
       "811     0                             I am so bored all day.  no_emotion   \n",
       "812     2                           Do you play much tennis?  no_emotion   \n",
       "\n",
       "            act  hat  \n",
       "0        inform    0  \n",
       "1        inform    3  \n",
       "2    commissive    1  \n",
       "3     directive    4  \n",
       "4    commissive    1  \n",
       "..          ...  ...  \n",
       "808      inform    0  \n",
       "809    question    1  \n",
       "810    question    1  \n",
       "811      inform    0  \n",
       "812    question    1  \n",
       "\n",
       "[813 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turn</th>\n",
       "      <th>utterance</th>\n",
       "      <th>emotion</th>\n",
       "      <th>act</th>\n",
       "      <th>hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>I'll take one, too.</td>\n",
       "      <td>happiness</td>\n",
       "      <td>inform</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>You know, we are superior to other clothes com...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Her new boyfriend, right?</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>commissive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>How about recommending him to use the storage ...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>directive</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Oh, a bouquet of flowers. It's very kind of you.</td>\n",
       "      <td>surprise</td>\n",
       "      <td>commissive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>0</td>\n",
       "      <td>I prefer potatoes to eggplants.</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>0</td>\n",
       "      <td>Mr. Smith, I would like to get right to the po...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>4</td>\n",
       "      <td>Yeah?</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>0</td>\n",
       "      <td>I am so bored all day.</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>2</td>\n",
       "      <td>Do you play much tennis?</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>813 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Baseline roBERTa over the Normal Hand-Labeled Dataset",
   "id": "c1145f0980441204"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T14:50:15.094450Z",
     "start_time": "2025-05-28T14:49:41.757235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "token_ids, attention_masks = bert_tokenize_data(tokenizer, normal_train_df['utterance'].values)\n",
    "train_dataloader, val_dataloader = tensor_train_test_split(torch.tensor(normal_train_df['hat'].values), token_ids, attention_masks, test_size=0.1)\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "configs = {\n",
    "        \"epochs\": 10,\n",
    "        \"clip_grad_norm\": 1.0,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"early_stopping_delta\": 0.1,\n",
    "    }\n",
    "\n",
    "num_training_steps = 10 * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)\n",
    "\n",
    "model = train_bert_model(model, optimizer, scheduler, train_dataloader, val_dataloader, configs)"
   ],
   "id": "4e3c929710b56921",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Epoch 1 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-28 16:49:43.278552\n",
      "Average Training Loss: 1.3443113597838774\n",
      "Time Taken:            0:00:07.915859\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-28 16:49:51.194784\n",
      "Average Validation Loss:     1.3561261729760603\n",
      "Average Validation Accuracy: 0.45454545454545453\n",
      "Time Taken:                  0:00:00.235475\n",
      "\n",
      "-------------------- Epoch 2 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-28 16:49:51.431286\n",
      "Average Training Loss: 1.1360726962270944\n",
      "Time Taken:            0:00:07.568970\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-28 16:49:59.000608\n",
      "Average Validation Loss:     1.4277930476448752\n",
      "Average Validation Accuracy: 0.45454545454545453\n",
      "Time Taken:                  0:00:00.231653\n",
      "No improvement for 1 epoch(s).\n",
      "\n",
      "-------------------- Epoch 3 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-28 16:49:59.232652\n",
      "Average Training Loss: 0.9433356567245462\n",
      "Time Taken:            0:00:07.652947\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-28 16:50:06.886031\n",
      "Average Validation Loss:     1.3631521355022083\n",
      "Average Validation Accuracy: 0.5568181818181818\n",
      "Time Taken:                  0:00:00.241552\n",
      "No improvement for 2 epoch(s).\n",
      "\n",
      "-------------------- Epoch 4 --------------------\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-28 16:50:07.128035\n",
      "Average Training Loss: 0.7246189520734808\n",
      "Time Taken:            0:00:07.720284\n",
      "\n",
      "Validation:\n",
      "-----------\n",
      "Start Time:       2025-05-28 16:50:14.848718\n",
      "Average Validation Loss:     1.5262794928117231\n",
      "Average Validation Accuracy: 0.5795454545454546\n",
      "Time Taken:                  0:00:00.242592\n",
      "No improvement for 3 epoch(s).\n",
      "\n",
      "Early stopping triggered after 4 epochs.\n",
      "\n",
      "Total training time: 0:00:31.815079\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T14:51:09.935601Z",
     "start_time": "2025-05-28T14:51:09.932785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Implement cohen's kappa to evaluate the model\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "def cohen_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n"
   ],
   "id": "e5cfb5fcec1a4ee4",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T14:51:05.496347Z",
     "start_time": "2025-05-28T14:51:05.188323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_texts = normal_test_df['utterance'].values\n",
    "serie = pd.Series(test_texts)\n",
    "tids, amids = bert_tokenize_data(tokenizer, serie, max_length=64)\n",
    "dl = get_data_loader(tids, amids, batch_size=5, shuffle=False)\n",
    "preds, confidences = model_predict(model, dl)\n",
    "labels_flat = normal_test_df['hat'].values.flatten()\n",
    "accuracy = np.sum(preds == labels_flat) / len(labels_flat)"
   ],
   "id": "769ebae4e5fd3ac1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T14:51:07.307790Z",
     "start_time": "2025-05-28T14:51:07.301010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hat_map = {\n",
    "    0: \"red\",\n",
    "    1: \"white\",\n",
    "    2: \"black\",\n",
    "    3: \"yellow\",\n",
    "    4: \"green\",\n",
    "}\n",
    "preds_array = np.array(preds)\n",
    "print(classification_report(labels_flat, preds_array, target_names=list(hat_map.values())))"
   ],
   "id": "df72e41f94901ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.46      0.57      0.51        40\n",
      "       white       0.81      0.65      0.72       110\n",
      "       black       0.33      0.57      0.42        23\n",
      "      yellow       0.57      0.42      0.48        19\n",
      "       green       0.23      0.25      0.24        12\n",
      "\n",
      "    accuracy                           0.58       204\n",
      "   macro avg       0.48      0.49      0.47       204\n",
      "weighted avg       0.63      0.58      0.59       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T14:51:44.906345Z",
     "start_time": "2025-05-28T14:51:44.900949Z"
    }
   },
   "cell_type": "code",
   "source": "cohen_kappa(normal_test_df['hat'].values, preds)",
   "id": "1375c9669dfa1d7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.3573451545376186)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Baseline roBERTa over the EDA Augmented Hand-Labeled Dataset",
   "id": "f4cb107f40bd9db5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T14:30:04.311326Z",
     "start_time": "2025-05-26T14:30:04.297773Z"
    }
   },
   "cell_type": "code",
   "source": "augmented_train_df = pd.read_json('../eda_train_dataset.json', lines=False)",
   "id": "b065b7e82ca15e34",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "token_ids, attention_masks = bert_tokenize_data(tokenizer, augmented_train_df['utterance'].values)\n",
    "train_dataloader, val_dataloader = tensor_train_test_split(torch.tensor(augmented_train_df['hat'].values), token_ids, attention_masks, test_size=0.1)\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "configs = {\n",
    "        \"epochs\": 10,\n",
    "        \"clip_grad_norm\": 1.0,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"early_stopping_delta\": 0.1,\n",
    "    }\n",
    "\n",
    "num_training_steps = 10 * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)\n",
    "\n",
    "model = train_bert_model(model, optimizer, scheduler, train_dataloader, val_dataloader, configs)"
   ],
   "id": "e708b30622088ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T14:32:44.362765Z",
     "start_time": "2025-05-26T14:32:44.057832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_texts = normal_test_df['utterance'].values\n",
    "serie = pd.Series(test_texts)\n",
    "tids, amids = bert_tokenize_data(tokenizer, serie, max_length=64)\n",
    "dl = get_data_loader(tids, amids, batch_size=5, shuffle=False)\n",
    "preds, confidences = model_predict(model, dl)\n",
    "labels_flat = normal_test_df['hat'].values.flatten()\n",
    "accuracy = np.sum(preds == labels_flat) / len(labels_flat)\n",
    "hat_map = {\n",
    "    0: \"red\",\n",
    "    1: \"white\",\n",
    "    2: \"black\",\n",
    "    3: \"yellow\",\n",
    "    4: \"green\",\n",
    "}\n",
    "preds_array = np.array(preds)\n",
    "print(classification_report(labels_flat, preds_array, target_names=list(hat_map.values())))"
   ],
   "id": "3098f2ffae6adfd5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.49      0.47      0.48        40\n",
      "       white       0.70      0.78      0.74       110\n",
      "       black       0.50      0.26      0.34        23\n",
      "      yellow       0.53      0.53      0.53        19\n",
      "       green       0.25      0.25      0.25        12\n",
      "\n",
      "    accuracy                           0.61       204\n",
      "   macro avg       0.49      0.46      0.47       204\n",
      "weighted avg       0.60      0.61      0.60       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Baseline roBERTa over the Automated Labeled Dataset",
   "id": "27034850567db170"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T14:38:52.021450Z",
     "start_time": "2025-05-26T14:38:52.013978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ald_train_df = pd.read_json('../ald_train_dataset.json', lines=False)\n",
    "ald_test_df = pd.read_json('../ald_test_dataset.json', lines=False)"
   ],
   "id": "48eafed8b663aa11",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "token_ids, attention_masks = bert_tokenize_data(tokenizer, ald_train_df['utterance'].values)\n",
    "train_dataloader, val_dataloader = tensor_train_test_split(torch.tensor(ald_train_df['hat'].values), token_ids, attention_masks, test_size=0.1)\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "configs = {\n",
    "        \"epochs\": 10,\n",
    "        \"clip_grad_norm\": 1.0,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"early_stopping_delta\": 0.1,\n",
    "    }\n",
    "\n",
    "num_training_steps = 10 * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)\n",
    "\n",
    "model = train_bert_model(model, optimizer, scheduler, train_dataloader, val_dataloader, configs)"
   ],
   "id": "635a604796a6ece7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T14:42:34.318307Z",
     "start_time": "2025-05-26T14:42:32.814251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_texts = ald_test_df['utterance'].values\n",
    "serie = pd.Series(test_texts)\n",
    "tids, amids = bert_tokenize_data(tokenizer, serie, max_length=64)\n",
    "dl = get_data_loader(tids, amids, batch_size=5, shuffle=False)\n",
    "preds, confidences = model_predict(model, dl)\n",
    "labels_flat = ald_test_df['hat'].values.flatten()\n",
    "accuracy = np.sum(preds == labels_flat) / len(labels_flat)\n",
    "hat_map = {\n",
    "    0: \"red\",\n",
    "    1: \"white\",\n",
    "    2: \"black\",\n",
    "    3: \"yellow\",\n",
    "    4: \"green\",\n",
    "}\n",
    "preds_array = np.array(preds)\n",
    "print(classification_report(labels_flat, preds_array, target_names=list(hat_map.values())))"
   ],
   "id": "1d2ff51805edd9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.75      0.77      0.76       200\n",
      "       white       0.42      0.39      0.41       200\n",
      "       black       0.51      0.59      0.55       200\n",
      "      yellow       0.50      0.67      0.57       200\n",
      "       green       0.65      0.37      0.47       200\n",
      "\n",
      "    accuracy                           0.56      1000\n",
      "   macro avg       0.57      0.56      0.55      1000\n",
      "weighted avg       0.57      0.56      0.55      1000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Testing the model on the normal test dataset",
   "id": "4c60b6b41f4713cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T14:42:34.591169Z",
     "start_time": "2025-05-26T14:42:34.332790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_texts = normal_test_df['utterance'].values\n",
    "serie = pd.Series(test_texts)\n",
    "tids, amids = bert_tokenize_data(tokenizer, serie, max_length=64)\n",
    "dl = get_data_loader(tids, amids, batch_size=5, shuffle=False)\n",
    "preds, confidences = model_predict(model, dl)\n",
    "labels_flat = normal_test_df['hat'].values.flatten()\n",
    "accuracy = np.sum(preds == labels_flat) / len(labels_flat)\n",
    "hat_map = {\n",
    "    0: \"red\",\n",
    "    1: \"white\",\n",
    "    2: \"black\",\n",
    "    3: \"yellow\",\n",
    "    4: \"green\",\n",
    "}\n",
    "preds_array = np.array(preds)\n",
    "print(classification_report(labels_flat, preds_array, target_names=list(hat_map.values())))"
   ],
   "id": "a37ba4eaba7cfa52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.32      0.30      0.31        40\n",
      "       white       0.69      0.47      0.56       110\n",
      "       black       0.19      0.22      0.20        23\n",
      "      yellow       0.21      0.58      0.31        19\n",
      "       green       0.25      0.25      0.25        12\n",
      "\n",
      "    accuracy                           0.41       204\n",
      "   macro avg       0.33      0.36      0.33       204\n",
      "weighted avg       0.49      0.41      0.43       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "38147ec1087bc1e2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
