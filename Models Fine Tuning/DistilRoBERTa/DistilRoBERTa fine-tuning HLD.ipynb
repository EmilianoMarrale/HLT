{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e79ad018e19671f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:25:30.325035Z",
     "start_time": "2025-05-20T20:25:26.254470Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"../..\"))  # vai alla cartella superiore\n",
    "\n",
    "from bert_util import bert_tokenize_data, tensor_train_test_split, train_bert_model, model_predict, get_data_loader, \\\n",
    "    calculate_accuracy\n",
    "from util import get_dataframe_from_json\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "os.environ[\"USE_TF\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d6cb2e52215c782",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:25:30.420544Z",
     "start_time": "2025-05-20T20:25:30.380551Z"
    }
   },
   "outputs": [],
   "source": "dataset = get_dataframe_from_json('../../dailydialog/hand_labeled/hand_labelled_dataset.json')"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6a25b15",
   "metadata": {},
   "outputs": [],
   "source": "dataset_balanced = pd.read_csv('../../dailydialog/hand_labeled/balanced_dataset_100_each.csv')"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bff9a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "hld = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c1197a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hld['utterance'] = hld['utterance'].str.replace('__eou__', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53d408cc997ae229",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:25:32.633747Z",
     "start_time": "2025-05-20T20:25:32.627671Z"
    }
   },
   "outputs": [],
   "source": [
    "hat_map = {\n",
    "    0: \"red\",\n",
    "    1: \"white\",\n",
    "    2: \"black\",\n",
    "    3: \"yellow\",\n",
    "    4: \"green\",\n",
    "}\n",
    "\n",
    "reverse_hat_map = {v: k for k, v in hat_map.items()}\n",
    "hld['hat'] = hld['hat'].apply(lambda x: reverse_hat_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58c289453a62fbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:09:41.108217Z",
     "start_time": "2025-05-20T20:09:41.100487Z"
    }
   },
   "outputs": [],
   "source": [
    "# import train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# split the data into train and test\n",
    "train_df, test_df = train_test_split(hld, test_size=0.2, random_state=42, stratify=hld['hat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ee76618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e70d7ca38d0438c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:09:43.604073Z",
     "start_time": "2025-05-20T20:09:43.176413Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
    "token_ids, attention_masks = bert_tokenize_data(tokenizer, train_df['utterance'].values)\n",
    "train_dataloader, val_dataloader = tensor_train_test_split(torch.tensor(train_df['hat'].values), token_ids, attention_masks, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8eac212dbbcdf59a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:48:21.930760Z",
     "start_time": "2025-05-20T19:45:49.222902Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Epoch 1 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:30:59.531596\n",
      "Average Loss:     1.6016576726262162\n",
      "Time Taken:       0:00:03.571303\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:03.103550\n",
      "Average Loss:     1.5944644451141357\n",
      "Average Accuracy: 0.18\n",
      "Time Taken:       0:00:00.133373\n",
      "\n",
      "-------------------- Epoch 2 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:03.237491\n",
      "Average Loss:     1.4554031156912082\n",
      "Time Taken:       0:00:03.190001\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:06.428301\n",
      "Average Loss:     1.334792685508728\n",
      "Average Accuracy: 0.32999999999999996\n",
      "Time Taken:       0:00:00.137863\n",
      "\n",
      "-------------------- Epoch 3 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:06.566776\n",
      "Average Loss:     1.1189992442363645\n",
      "Time Taken:       0:00:03.179498\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:09.746901\n",
      "Average Loss:     1.2864388465881347\n",
      "Average Accuracy: 0.40499999999999997\n",
      "Time Taken:       0:00:00.126192\n",
      "\n",
      "-------------------- Epoch 4 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:09.873671\n",
      "Average Loss:     0.83249337789489\n",
      "Time Taken:       0:00:03.199911\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:13.074264\n",
      "Average Loss:     1.3242483139038086\n",
      "Average Accuracy: 0.45499999999999996\n",
      "Time Taken:       0:00:00.128758\n",
      "\n",
      "-------------------- Epoch 5 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:13.203642\n",
      "Average Loss:     0.5584588617813296\n",
      "Time Taken:       0:00:03.326860\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:16.531143\n",
      "Average Loss:     1.3971195220947266\n",
      "Average Accuracy: 0.44000000000000006\n",
      "Time Taken:       0:00:00.129552\n",
      "\n",
      "-------------------- Epoch 6 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:16.661249\n",
      "Average Loss:     0.35025678229768104\n",
      "Time Taken:       0:00:03.168238\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:19.830215\n",
      "Average Loss:     1.5363569617271424\n",
      "Average Accuracy: 0.52\n",
      "Time Taken:       0:00:00.132258\n",
      "\n",
      "-------------------- Epoch 7 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:19.963048\n",
      "Average Loss:     0.20050383431882393\n",
      "Time Taken:       0:00:03.359549\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:23.323243\n",
      "Average Loss:     1.8084505319595336\n",
      "Average Accuracy: 0.445\n",
      "Time Taken:       0:00:00.126620\n",
      "\n",
      "-------------------- Epoch 8 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:23.450430\n",
      "Average Loss:     0.1254613743867816\n",
      "Time Taken:       0:00:03.159247\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:26.610317\n",
      "Average Loss:     2.2212401628494263\n",
      "Average Accuracy: 0.44000000000000006\n",
      "Time Taken:       0:00:00.130359\n",
      "\n",
      "-------------------- Epoch 9 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:26.741431\n",
      "Average Loss:     0.08386000988596096\n",
      "Time Taken:       0:00:03.299129\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:30.041221\n",
      "Average Loss:     2.294021487236023\n",
      "Average Accuracy: 0.45499999999999996\n",
      "Time Taken:       0:00:00.140461\n",
      "\n",
      "-------------------- Epoch 10 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:30.182282\n",
      "Average Loss:     0.0548670166399239\n",
      "Time Taken:       0:00:05.713141\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:35.896034\n",
      "Average Loss:     2.6262701988220214\n",
      "Average Accuracy: 0.465\n",
      "Time Taken:       0:00:00.128578\n",
      "\n",
      "-------------------- Epoch 11 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:36.025202\n",
      "Average Loss:     0.028753113240094448\n",
      "Time Taken:       0:00:03.169989\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:39.195797\n",
      "Average Loss:     2.5909096956253053\n",
      "Average Accuracy: 0.445\n",
      "Time Taken:       0:00:00.129513\n",
      "\n",
      "-------------------- Epoch 12 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:39.325890\n",
      "Average Loss:     0.023195143476719172\n",
      "Time Taken:       0:00:03.275607\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:42.602119\n",
      "Average Loss:     2.6800853490829466\n",
      "Average Accuracy: 0.51\n",
      "Time Taken:       0:00:00.130356\n",
      "\n",
      "-------------------- Epoch 13 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:42.733066\n",
      "Average Loss:     0.01869914646078719\n",
      "Time Taken:       0:00:03.169546\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:45.903266\n",
      "Average Loss:     3.0802077770233156\n",
      "Average Accuracy: 0.44000000000000006\n",
      "Time Taken:       0:00:00.128975\n",
      "\n",
      "-------------------- Epoch 14 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:46.032901\n",
      "Average Loss:     0.015382900805689576\n",
      "Time Taken:       0:00:03.310023\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:49.343612\n",
      "Average Loss:     2.8947665691375732\n",
      "Average Accuracy: 0.48\n",
      "Time Taken:       0:00:00.141564\n",
      "\n",
      "-------------------- Epoch 15 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:49.485775\n",
      "Average Loss:     0.00957331137459089\n",
      "Time Taken:       0:00:03.193325\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:52.679731\n",
      "Average Loss:     2.9853350162506103\n",
      "Average Accuracy: 0.44000000000000006\n",
      "Time Taken:       0:00:00.132960\n",
      "\n",
      "-------------------- Epoch 16 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:52.813415\n",
      "Average Loss:     0.009290964444872082\n",
      "Time Taken:       0:00:03.236941\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:56.051115\n",
      "Average Loss:     2.9358258485794066\n",
      "Average Accuracy: 0.43\n",
      "Time Taken:       0:00:00.140783\n",
      "\n",
      "-------------------- Epoch 17 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:56.192545\n",
      "Average Loss:     0.007092913754693255\n",
      "Time Taken:       0:00:03.297366\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:59.490520\n",
      "Average Loss:     3.0364636898040773\n",
      "Average Accuracy: 0.41500000000000004\n",
      "Time Taken:       0:00:00.132272\n",
      "\n",
      "-------------------- Epoch 18 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:31:59.623355\n",
      "Average Loss:     0.005867093023503336\n",
      "Time Taken:       0:00:03.231156\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:32:02.855178\n",
      "Average Loss:     2.9596835374832153\n",
      "Average Accuracy: 0.43\n",
      "Time Taken:       0:00:00.142924\n",
      "\n",
      "-------------------- Epoch 19 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:32:02.998758\n",
      "Average Loss:     0.005720722043273471\n",
      "Time Taken:       0:00:05.909881\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:32:08.909304\n",
      "Average Loss:     2.8714584350585937\n",
      "Average Accuracy: 0.445\n",
      "Time Taken:       0:00:00.132113\n",
      "\n",
      "-------------------- Epoch 20 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:32:09.042022\n",
      "Average Loss:     0.00742409155397426\n",
      "Time Taken:       0:00:03.353428\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 12:32:12.396150\n",
      "Average Loss:     2.8522581338882445\n",
      "Average Accuracy: 0.45999999999999996\n",
      "Time Taken:       0:00:00.128605\n",
      "\n",
      "Total training time: 0:01:12.993787\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilroberta-base', num_labels=5)\n",
    "epochs = 20\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "num_training_steps = epochs * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)\n",
    "\n",
    "model = train_bert_model(model, optimizer, scheduler, train_dataloader, val_dataloader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4f71d611d87e5b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:38:21.394048Z",
     "start_time": "2025-05-20T16:38:20.991861Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5441176470588235)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts = test_df['utterance'].values\n",
    "serie = pd.Series(test_texts)\n",
    "tids, amids = bert_tokenize_data(tokenizer, serie, max_length=64)\n",
    "dl = get_data_loader(tids, amids, batch_size=5, shuffle=False)\n",
    "preds, confidences = model_predict(model, dl)\n",
    "labels_flat = test_df['hat'].values.flatten()\n",
    "accuracy = np.sum(preds == labels_flat) / len(labels_flat)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b2a2e4b9deb1a83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:47:43.717838Z",
     "start_time": "2025-05-20T16:47:43.704401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.45      0.33      0.38        40\n",
      "       white       0.65      0.74      0.69       110\n",
      "       black       0.31      0.22      0.26        23\n",
      "      yellow       0.50      0.47      0.49        19\n",
      "       green       0.19      0.25      0.21        12\n",
      "\n",
      "    accuracy                           0.54       204\n",
      "   macro avg       0.42      0.40      0.40       204\n",
      "weighted avg       0.53      0.54      0.53       204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "preds_array = np.array(preds)\n",
    "print(classification_report(labels_flat, preds_array, target_names=list(hat_map.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cae8a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/atlas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/atlas/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Scarica WordNet e i dati multilingue (se serve)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12094416d5c6987e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:36:20.922167Z",
     "start_time": "2025-05-20T20:36:20.656319Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "turn",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "utterance",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "emotion",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "act",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "hat",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "d5419c24-68bd-4f62-bfa0-016bd3d24f0e",
       "rows": [
        [
         "0",
         "3",
         "I'll take one , too . ",
         "happiness",
         "inform",
         "0"
        ],
        [
         "1",
         "8",
         "You know , we are superior to other clothes companies .",
         "no_emotion",
         "inform",
         "3"
        ],
        [
         "2",
         "5",
         "Her new boyfriend , right ?",
         "no_emotion",
         "commissive",
         "1"
        ],
        [
         "3",
         "9",
         "How about recommending him to use the storage room down the hall ?",
         "no_emotion",
         "directive",
         "4"
        ],
        [
         "4",
         "1",
         "Oh , a bouquet of flowers . It's very kind of you .",
         "surprise",
         "commissive",
         "1"
        ],
        [
         "5",
         "4",
         "That's all right . ",
         "happiness",
         "inform",
         "1"
        ],
        [
         "6",
         "0",
         "Should I ask Sara to the party ?",
         "no_emotion",
         "question",
         "1"
        ],
        [
         "7",
         "1",
         "She does have a heart of gold .",
         "no_emotion",
         "inform",
         "1"
        ],
        [
         "8",
         "1",
         "Okay . ",
         "no_emotion",
         "commissive",
         "1"
        ],
        [
         "9",
         "3",
         "I know , I'm sorry . ",
         "sadness",
         "inform",
         "0"
        ],
        [
         "10",
         "1",
         "My condolences . ",
         "sadness",
         "inform",
         "1"
        ],
        [
         "11",
         "0",
         "What's happening in your life ?",
         "no_emotion",
         "question",
         "1"
        ],
        [
         "12",
         "0",
         "My uncle passed away last night .",
         "sadness",
         "inform",
         "1"
        ],
        [
         "13",
         "0",
         "I like the design of the coat .",
         "happiness",
         "inform",
         "0"
        ],
        [
         "14",
         "0",
         "I got the job you recommended me for last week .",
         "no_emotion",
         "inform",
         "1"
        ],
        [
         "15",
         "11",
         "A seminar of technical specialists including the surveyors from both sides will be held to clarify which result is correct . ",
         "no_emotion",
         "inform",
         "1"
        ],
        [
         "16",
         "2",
         "Nice meeting you , too .",
         "happiness",
         "inform",
         "3"
        ],
        [
         "17",
         "10",
         "So cool , if we do business through other ways , it will cost a lot of time .",
         "no_emotion",
         "inform",
         "2"
        ],
        [
         "18",
         "1",
         "You bet ! ",
         "happiness",
         "inform",
         "1"
        ],
        [
         "19",
         "6",
         "I hate computers . Sometimes I think they cause more trouble than they're worth .",
         "disgust",
         "inform",
         "1"
        ],
        [
         "20",
         "2",
         "I'm wondering if you often trade with others on the internet .",
         "no_emotion",
         "question",
         "1"
        ],
        [
         "21",
         "1",
         "I told you so . ",
         "no_emotion",
         "inform",
         "1"
        ],
        [
         "22",
         "5",
         "How much is it ?",
         "no_emotion",
         "question",
         "1"
        ],
        [
         "23",
         "7",
         "I've never experienced such a kind of feeling .",
         "no_emotion",
         "inform",
         "0"
        ],
        [
         "24",
         "2",
         "Why not ?",
         "no_emotion",
         "question",
         "1"
        ],
        [
         "25",
         "2",
         "Anyway , it's my ideal coat . I'll take it .",
         "happiness",
         "inform",
         "0"
        ],
        [
         "26",
         "5",
         "Deeper , softer , none of your twittering larks . I would banish nightingales from her garden before they interrupted her song .",
         "happiness",
         "inform",
         "4"
        ],
        [
         "27",
         "3",
         "E-commerce , or Electronic Commerce is the practice of purchasing and selling products or services over the Internet .",
         "no_emotion",
         "inform",
         "1"
        ],
        [
         "28",
         "0",
         "To be frank , I find philosophy rather boring .",
         "no_emotion",
         "inform",
         "0"
        ],
        [
         "29",
         "3",
         "If you say so ! ",
         "no_emotion",
         "inform",
         "0"
        ],
        [
         "30",
         "12",
         "Yes , he was . I'm not imagining it.Finally , just when I got home , I turned around and looked at him.He was just standing there . He didn't smile . He just stood there . It was so obvious.What should I do ? I'm so scared . Can I call the police ?",
         "fear",
         "question",
         "0"
        ],
        [
         "31",
         "0",
         "I'm so angry . I feel like killing someone .",
         "anger",
         "inform",
         "0"
        ],
        [
         "32",
         "0",
         "Mrs . Lee , I ’ Ve stayed here for almost a week . And I really must leave tomorrow .",
         "no_emotion",
         "inform",
         "2"
        ],
        [
         "33",
         "0",
         "Quite a few times , I had my back to the wall .",
         "no_emotion",
         "inform",
         "1"
        ],
        [
         "34",
         "4",
         "Yeah !",
         "no_emotion",
         "inform",
         "1"
        ],
        [
         "35",
         "5",
         "Mr.Cook is on Line Six . I'll put you through .",
         "no_emotion",
         "directive",
         "1"
        ],
        [
         "36",
         "10",
         "OK . We will manage it later . ",
         "no_emotion",
         "commissive",
         "1"
        ],
        [
         "37",
         "2",
         "OK , Let me see . I suppose we must strengthen our promotion , because our brand is still new to some consumers . Maybe we should start our advertising program with our local and overseas distributors simultaneously , because they stand on a better position for selecting the best ways to advertise in market places . Besides , the advertisement fund can encourage them to spend more attention on advertising our products .",
         "no_emotion",
         "directive",
         "4"
        ],
        [
         "38",
         "2",
         "I feel he's like a talking machine .",
         "no_emotion",
         "inform",
         "0"
        ],
        [
         "39",
         "2",
         "The operation was very successful .",
         "happiness",
         "inform",
         "0"
        ],
        [
         "40",
         "3",
         "I laughed up my sleeves at him . He ’ s seen his girlfriend off at the airport and had lipstick on his face and did not know it . ",
         "no_emotion",
         "inform",
         "2"
        ],
        [
         "41",
         "0",
         "Bob , What are you thinking about ?",
         "no_emotion",
         "question",
         "1"
        ],
        [
         "42",
         "0",
         "My granny is such a kind-hearted person . She is willing to help .",
         "happiness",
         "inform",
         "3"
        ],
        [
         "43",
         "2",
         "Our quality control department is in charge of the inspection , which is directly under general manager . It is essential to attach the importance to the quality of goods . It is the basement for the future cooperation .",
         "no_emotion",
         "inform",
         "1"
        ],
        [
         "44",
         "4",
         "I see . ",
         "no_emotion",
         "inform",
         "1"
        ],
        [
         "45",
         "6",
         "Consult your bank and see if they will reduce the required deposit to a minimum . ",
         "no_emotion",
         "directive",
         "4"
        ],
        [
         "46",
         "9",
         "People who are honest , dedicated to their work and have integrity .",
         "no_emotion",
         "inform",
         "3"
        ],
        [
         "47",
         "0",
         "How have you been ?",
         "happiness",
         "question",
         "1"
        ],
        [
         "48",
         "1",
         "Sarah and I could see eye to eye with each other .",
         "happiness",
         "inform",
         "3"
        ],
        [
         "49",
         "1",
         "I know I'm late . I'm sorry . I tried to call you to tell you , but your phone seems to be disconnected .",
         "no_emotion",
         "inform",
         "2"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 2943
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turn</th>\n",
       "      <th>utterance</th>\n",
       "      <th>emotion</th>\n",
       "      <th>act</th>\n",
       "      <th>hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>I'll take one , too .</td>\n",
       "      <td>happiness</td>\n",
       "      <td>inform</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>You know , we are superior to other clothes co...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Her new boyfriend , right ?</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>commissive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>How about recommending him to use the storage ...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>directive</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Oh , a bouquet of flowers . It's very kind of ...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>commissive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938</th>\n",
       "      <td>2</td>\n",
       "      <td>problem seems to be the what</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2939</th>\n",
       "      <td>2</td>\n",
       "      <td>what seems to be the problem</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2940</th>\n",
       "      <td>2</td>\n",
       "      <td>be seems to what the problem</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2941</th>\n",
       "      <td>2</td>\n",
       "      <td>look what seems to be the problem</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2942</th>\n",
       "      <td>2</td>\n",
       "      <td>what seems to be look the problem</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2943 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      turn                                          utterance     emotion  \\\n",
       "0        3                             I'll take one , too .    happiness   \n",
       "1        8  You know , we are superior to other clothes co...  no_emotion   \n",
       "2        5                        Her new boyfriend , right ?  no_emotion   \n",
       "3        9  How about recommending him to use the storage ...  no_emotion   \n",
       "4        1  Oh , a bouquet of flowers . It's very kind of ...    surprise   \n",
       "...    ...                                                ...         ...   \n",
       "2938     2                       problem seems to be the what  no_emotion   \n",
       "2939     2                       what seems to be the problem  no_emotion   \n",
       "2940     2                       be seems to what the problem  no_emotion   \n",
       "2941     2                  look what seems to be the problem  no_emotion   \n",
       "2942     2                  what seems to be look the problem  no_emotion   \n",
       "\n",
       "             act  hat  \n",
       "0         inform    0  \n",
       "1         inform    3  \n",
       "2     commissive    1  \n",
       "3      directive    4  \n",
       "4     commissive    1  \n",
       "...          ...  ...  \n",
       "2938    question    2  \n",
       "2939    question    2  \n",
       "2940    question    2  \n",
       "2941    question    2  \n",
       "2942    question    2  \n",
       "\n",
       "[2943 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from util import eda_augment_dataset\n",
    "\n",
    "train_df, test_df = train_test_split(hld, test_size=0.2, random_state=42, stratify=hld['hat'])\n",
    "\n",
    "green_augmented = eda_augment_dataset(train_df[train_df['hat'] == 4], num_aug=10, alpha_sr=0.05, alpha_ri=0.05, alpha_rs=0.05, p_rd=0.1)\n",
    "yellow_augmented = eda_augment_dataset(train_df[train_df['hat'] == 3], num_aug=10, alpha_sr=0.05, alpha_ri=0.05, alpha_rs=0.05, p_rd=0.1)\n",
    "black_augmented = eda_augment_dataset(train_df[train_df['hat'] == 2], num_aug=10, alpha_sr=0.05, alpha_ri=0.05, alpha_rs=0.05, p_rd=0.1)\n",
    "\n",
    "augmented_train_df = pd.concat([train_df, green_augmented, yellow_augmented, black_augmented], ignore_index=True)\n",
    "augmented_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07e16e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
    "token_ids, attention_masks = bert_tokenize_data(tokenizer, augmented_train_df['utterance'].values)\n",
    "train_dataloader, val_dataloader = tensor_train_test_split(torch.tensor(augmented_train_df['hat'].values), token_ids, attention_masks, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a72c4f6b580b1d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:41:03.087837Z",
     "start_time": "2025-05-20T20:36:29.315721Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Epoch 1 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 11:39:38.248794\n",
      "Average Loss:     0.7542178766559978\n",
      "Time Taken:       0:00:26.368342\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 11:40:04.617810\n",
      "Average Loss:     0.8680468499660492\n",
      "Average Accuracy: 0.7601351351351351\n",
      "Time Taken:       0:00:00.764201\n",
      "\n",
      "-------------------- Epoch 2 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 11:40:05.382658\n",
      "Average Loss:     0.2911073781797533\n",
      "Time Taken:       0:00:29.856861\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 11:40:35.240188\n",
      "Average Loss:     0.7945205372430988\n",
      "Average Accuracy: 0.8161196911196912\n",
      "Time Taken:       0:00:00.739565\n",
      "\n",
      "-------------------- Epoch 3 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 11:40:35.980411\n",
      "Average Loss:     0.19424756761750042\n",
      "Time Taken:       0:00:29.326564\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 11:41:05.307753\n",
      "Average Loss:     0.6573282419672437\n",
      "Average Accuracy: 0.8648648648648649\n",
      "Time Taken:       0:00:00.746163\n",
      "\n",
      "-------------------- Epoch 4 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 11:41:06.054547\n",
      "Average Loss:     0.13958761728828734\n",
      "Time Taken:       0:00:29.390875\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 11:41:35.446048\n",
      "Average Loss:     0.8898960346689548\n",
      "Average Accuracy: 0.8175675675675675\n",
      "Time Taken:       0:00:00.750472\n",
      "\n",
      "-------------------- Epoch 5 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 11:41:36.197148\n",
      "Average Loss:     0.08242903339002687\n",
      "Time Taken:       0:00:29.477047\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 11:42:05.675112\n",
      "Average Loss:     0.52770194097743\n",
      "Average Accuracy: 0.9015444015444014\n",
      "Time Taken:       0:00:00.768348\n",
      "\n",
      "-------------------- Epoch 6 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 11:42:06.444112\n",
      "Average Loss:     0.06147847650326462\n",
      "Time Taken:       0:00:29.361813\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 11:42:35.806638\n",
      "Average Loss:     0.2873864377466521\n",
      "Average Accuracy: 0.9459459459459459\n",
      "Time Taken:       0:00:00.760581\n",
      "\n",
      "-------------------- Epoch 7 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 11:42:36.567864\n",
      "Average Loss:     0.03468626706020026\n",
      "Time Taken:       0:00:26.578983\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 11:43:03.147616\n",
      "Average Loss:     0.25180812343770975\n",
      "Average Accuracy: 0.9459459459459459\n",
      "Time Taken:       0:00:00.760147\n",
      "\n",
      "-------------------- Epoch 8 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 11:43:03.908422\n",
      "Average Loss:     0.019562457816570143\n",
      "Time Taken:       0:00:29.373747\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 11:43:33.282815\n",
      "Average Loss:     0.24119014433990987\n",
      "Average Accuracy: 0.9517374517374517\n",
      "Time Taken:       0:00:00.776257\n",
      "\n",
      "-------------------- Epoch 9 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 11:43:34.059788\n",
      "Average Loss:     0.012736160769431537\n",
      "Time Taken:       0:00:29.609358\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 11:44:03.669782\n",
      "Average Loss:     0.215030339724271\n",
      "Average Accuracy: 0.956081081081081\n",
      "Time Taken:       0:00:00.737860\n",
      "\n",
      "-------------------- Epoch 10 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-21 11:44:04.408317\n",
      "Average Loss:     0.012473973641539586\n",
      "Time Taken:       0:00:29.571079\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-21 11:44:33.980019\n",
      "Average Loss:     0.18690094611775823\n",
      "Average Accuracy: 0.9594594594594594\n",
      "Time Taken:       0:00:00.726886\n",
      "\n",
      "Total training time: 0:04:56.458799\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.45      0.53      0.48        40\n",
      "       white       0.70      0.80      0.75       110\n",
      "       black       0.31      0.17      0.22        23\n",
      "      yellow       0.58      0.37      0.45        19\n",
      "       green       0.29      0.17      0.21        12\n",
      "\n",
      "    accuracy                           0.60       204\n",
      "   macro avg       0.47      0.41      0.42       204\n",
      "weighted avg       0.57      0.60      0.58       204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilroberta-base', num_labels=5)\n",
    "epochs = 10\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "num_training_steps = epochs * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)\n",
    "\n",
    "model = train_bert_model(model, optimizer, scheduler, train_dataloader, val_dataloader, epochs)\n",
    "test_texts = test_df['utterance'].values\n",
    "serie = pd.Series(test_texts)\n",
    "tids, amids = bert_tokenize_data(tokenizer, serie, max_length=64)\n",
    "dl = get_data_loader(tids, amids, batch_size=5, shuffle=False)\n",
    "preds, confidences = model_predict(model, dl)\n",
    "labels_flat = test_df['hat'].values.flatten()\n",
    "\n",
    "preds_array = np.array(preds)\n",
    "print(classification_report(labels_flat, preds_array, target_names=list(hat_map.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a591cf113b708480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5980392156862745)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts = test_df['utterance'].values\n",
    "serie = pd.Series(test_texts)\n",
    "tids, amids = bert_tokenize_data(tokenizer, serie, max_length=64)\n",
    "dl = get_data_loader(tids, amids, batch_size=5, shuffle=False)\n",
    "preds, confidences = model_predict(model, dl)\n",
    "labels_flat = test_df['hat'].values.flatten()\n",
    "accuracy = np.sum(preds == labels_flat) / len(labels_flat)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc2b08d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.45      0.53      0.48        40\n",
      "       white       0.70      0.80      0.75       110\n",
      "       black       0.31      0.17      0.22        23\n",
      "      yellow       0.58      0.37      0.45        19\n",
      "       green       0.29      0.17      0.21        12\n",
      "\n",
      "    accuracy                           0.60       204\n",
      "   macro avg       0.47      0.41      0.42       204\n",
      "weighted avg       0.57      0.60      0.58       204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "preds_array = np.array(preds)\n",
    "print(classification_report(labels_flat, preds_array, target_names=list(hat_map.values())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_hlt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
