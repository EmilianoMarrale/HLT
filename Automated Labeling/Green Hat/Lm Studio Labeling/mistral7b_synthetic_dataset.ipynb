{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da87bdd0",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "- protobuf sentencepiece bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c5dc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "# load the data in one single dataframe\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8b6b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refactor(df):# Extract all utterances from the dialogues\n",
    "    all_utterances = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Each row has an 'utterances' field which is a list of utterance dictionaries\n",
    "        utterances = row['utterances']\n",
    "\n",
    "        # Add dialogue ID and topic to each utterance for reference\n",
    "        for utterance in utterances:\n",
    "            utterance['dialogue_id'] = row['id']\n",
    "            utterance['topic'] = row['topic']\n",
    "            all_utterances.append(utterance)\n",
    "\n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df = pd.DataFrame(all_utterances)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70ce68",
   "metadata": {},
   "outputs": [],
   "source": "df = load_data('../../../dailydialog/dialogues.json')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357c6804",
   "metadata": {},
   "outputs": [],
   "source": [
    "refactor_df = refactor(df)\n",
    "refactor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e917c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dffc16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Scegli il checkpoint quantizzato 4‑bit (gguf/q4_0) su HF\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d8d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read content of file huggingface_token.txt\n",
    "with open('huggingface_token.txt', 'r') as file:\n",
    "    token = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72c88d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HUGGINGFACE_HUB_TOKEN=token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c591197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Carica tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2aa604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Configurazione 4‑bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c9eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_mem = {\n",
    "    0: 5_300 * 1024**2,      # GPU 0\n",
    "    \"cpu\": 60 * 1024**3,     # tutto ciò che non sta in GPU\n",
    "}\n",
    "\n",
    "# 4) Carica modello su GPU (device_map=\"auto\" sposta layer su GPU fino a saturazione)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    max_memory=max_mem,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a566606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Definisci la “definizione” da ripassare al modello:\n",
    "prompt_prefix = \"\"\"\n",
    "You are helping create a synthetic dataset for a classification task.\n",
    "The goal is to generate single-sentence utterances that reflect one specific thinking style based on Edward De Bono's Six Thinking Hats. Here is the definition for the [CAP] hat:\n",
    "[DEFINITION]\n",
    "Please generate 10 distinct, realistic, mixed length utterances that clearly follow this definition. Each utterance must reflect the corresponding thinking style.\n",
    "Only output a JSON list of objects, each in the format:\n",
    "{\"utterance\": \"your sentence here\", \"hat\": \"[CAP]\"}\n",
    "Respond only with the json text. No explanations, no notes, no markdown. Only valid JSON.\n",
    "Now generate 10 utterances for the [CAP] hat.\n",
    "Your answer must start with the json list and nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966c5c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "white_hat_definition = \"Exchanging or providing plain informations. Things generally true, things that happened to someone. Not trying to convince anyone.\"\n",
    "black_hat_definition = \"Analysis of a situation. Answers the why of something that is not being done. Negative analysis, explaining weak points of a thing, logically.\"\n",
    "red_hat_definition = \"Emotions involved in the answer. The utterance is clearly stated due to emotion involved. Intuitions, feelings, gut reactions. No need for logical justification.\"\n",
    "yellow_hat_definition = \"Statements that highlight the positive sides (negatives can exists). Changes that offer benefits. Open up remote but highly desirable possibilities. Reflect ideas to believe in. Provide encouragement to take action. Express positive judgments.\"\n",
    "green_hat_definition = \"Proposing new point of views. New Ideas. Solutions to problems. Imagining new scenarios, going beyond what is known.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c4424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "white_hat_prompt = prompt_prefix.replace(\"[CAP]\", \"White\")\n",
    "white_hat_prompt = white_hat_prompt.replace(\"[DEFINITION]\", white_hat_definition)\n",
    "black_hat_prompt = prompt_prefix.replace(\"[CAP]\", \"Black\")\n",
    "black_hat_prompt = black_hat_prompt.replace(\"[DEFINITION]\", black_hat_definition)\n",
    "red_hat_prompt = prompt_prefix.replace(\"[CAP]\", \"Red\")\n",
    "red_hat_prompt = red_hat_prompt.replace(\"[DEFINITION]\", red_hat_definition)\n",
    "yellow_hat_prompt = prompt_prefix.replace(\"[CAP]\", \"Yellow\")\n",
    "yellow_hat_prompt = yellow_hat_prompt.replace(\"[DEFINITION]\", yellow_hat_definition)\n",
    "green_hat_prompt = prompt_prefix.replace(\"[CAP]\", \"Green\")\n",
    "green_hat_prompt = green_hat_prompt.replace(\"[DEFINITION]\", green_hat_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ecbf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def remove_spaces_outside_quotes(text):\n",
    "    result = []\n",
    "    inside_string = False\n",
    "\n",
    "    for char in text:\n",
    "        if char == '\"':\n",
    "            inside_string = not inside_string\n",
    "            result.append(char)\n",
    "        elif not inside_string and char == ' ':\n",
    "            continue  # ignora spazi fuori dalle virgolette\n",
    "        else:\n",
    "            result.append(char)\n",
    "\n",
    "    return ''.join(result)\n",
    "\n",
    "def generate_synthetic_examples(prompt, model, tokenizer, max_new_tokens=512):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(output[0][ inputs[\"input_ids\"].shape[-1]: ], skip_special_tokens=True).strip()\n",
    "    # remove newlines from decoded\n",
    "    decoded = decoded.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    # remove spaces except the rha spaces inside double quotes\n",
    "    decoded = ' '.join(decoded.split())\n",
    "    decoded = remove_spaces_outside_quotes(decoded)\n",
    "    # print(\"decoded:\", decoded)\n",
    "    \n",
    "    try:\n",
    "        json_start = decoded.find(\"[{\")\n",
    "        json_end = decoded.rfind(\"}]\") + 2\n",
    "        json_text = decoded[json_start:json_end]\n",
    "        return json.loads(json_text)\n",
    "    except Exception as e:\n",
    "        print(\"Errore nel parsing:\", e)\n",
    "        print(\"Output ricevuto:\\n\", decoded)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d3d952",
   "metadata": {},
   "outputs": [],
   "source": [
    "white_examples = []\n",
    "for _ in range(100):\n",
    "    examples = generate_synthetic_examples(white_hat_prompt, model, tokenizer)\n",
    "    white_examples.extend(examples)\n",
    "    \n",
    "white_examples = pd.DataFrame(white_examples)\n",
    "white_examples['utterance'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e75358",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_examples = []\n",
    "for _ in range(100):\n",
    "    examples = generate_synthetic_examples(green_hat_prompt, model, tokenizer)\n",
    "    green_examples.extend(examples)\n",
    "\n",
    "green_examples = pd.DataFrame(green_examples)\n",
    "green_examples['utterance'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c14ba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_examples = []\n",
    "for _ in range(100):\n",
    "    examples = generate_synthetic_examples(red_hat_prompt, model, tokenizer)\n",
    "    red_examples.extend(examples)\n",
    "red_examples = pd.DataFrame(red_examples)\n",
    "red_examples['utterance'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14cb2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "black_examples = []\n",
    "for _ in range(100):\n",
    "    examples = generate_synthetic_examples(black_hat_prompt, model, tokenizer)\n",
    "    black_examples.extend(examples)\n",
    "black_examples = pd.DataFrame(black_examples)\n",
    "black_examples['utterance'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c9c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_examples = []\n",
    "for _ in range(100):\n",
    "    examples = generate_synthetic_examples(yellow_hat_prompt, model, tokenizer)\n",
    "    yellow_examples.extend(examples)\n",
    "yellow_examples = pd.DataFrame(yellow_examples)\n",
    "yellow_examples['utterance'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8103cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates from white_examples\n",
    "white_examples = white_examples.drop_duplicates(subset='utterance')\n",
    "# remove duplicates from yellow_examples\n",
    "yellow_examples = yellow_examples.drop_duplicates(subset='utterance')\n",
    "# remove duplicates from red_examples\n",
    "red_examples = red_examples.drop_duplicates(subset='utterance')\n",
    "# remove duplicates from black_examples\n",
    "black_examples = black_examples.drop_duplicates(subset='utterance')\n",
    "# remove duplicates from green_examples\n",
    "green_examples = green_examples.drop_duplicates(subset='utterance')\n",
    "\n",
    "# all_examples = white_examples + yellow_examples + red_examples + green_examples + black_examples\n",
    "\n",
    "#remove duplicates from all_examples\n",
    "all_examples = pd.concat([white_examples, yellow_examples, red_examples, green_examples, black_examples]).drop_duplicates(subset='utterance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d645e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index in all_examples\n",
    "all_examples.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf6ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write all_examples to a json file\n",
    "all_examples.to_json(\"synthetic_hat_dataset.json\", orient=\"records\", force_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59060ef",
   "metadata": {},
   "source": [
    "# Read and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6779ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load the json file to pandas dataframe\n",
    "synthetic_df = pd.read_json(\"synthetic_hat_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d7f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hat_map = {\n",
    "    \"red\": 0,\n",
    "    \"white\": 1,\n",
    "    \"black\": 2,\n",
    "    \"yellow\": 3,\n",
    "    \"green\": 4\n",
    "}\n",
    "# lowercase hat column\n",
    "synthetic_df['hat'] = synthetic_df['hat'].str.lower()\n",
    "#print unique values in hat column\n",
    "print(synthetic_df['hat'].unique())\n",
    "#print distribution of hat column\n",
    "print(synthetic_df['hat'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754e92bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map hat column to hat_map\n",
    "synthetic_df['hat'] = synthetic_df['hat'].map(hat_map)\n",
    "print(synthetic_df['hat'].unique())\n",
    "# print distribution of hat column which is a pandas Series\n",
    "print(synthetic_df['hat'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9855bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(synthetic_df, test_size=0.2, random_state=42, stratify=synthetic_df['hat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e9190d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap: 0 / 888\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "train_texts = set(d['utterance'] for d in json.load(open(\"/home/atlas/hlt/HLT/Models Fine Tuning/synthetic_train_dataset.json\")))\n",
    "test_texts = set(d['utterance'] for d in json.load(open(\"/home/atlas/hlt/HLT/Models Fine Tuning/synthetic_test_dataset.json\")))\n",
    "\n",
    "print(f\"Overlap: {len(train_texts & test_texts)} / {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cceb001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_hlt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
