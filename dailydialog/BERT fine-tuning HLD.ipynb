{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:25:30.325035Z",
     "start_time": "2025-05-20T20:25:26.254470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from bert_util import bert_tokenize_data, tensor_train_test_split, train_bert_model, model_predict, get_data_loader, \\\n",
    "    calculate_accuracy\n",
    "from util import get_dataframe_from_json\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "os.environ[\"USE_TF\"] = \"0\""
   ],
   "id": "e79ad018e19671f2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emiliano/Desktop/Università/Human Language Technologies/Project/HLT Project/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-20 22:25:28.949224: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-20 22:25:28.956161: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747772728.965427   54178 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747772728.968258   54178 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747772728.975980   54178 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747772728.975990   54178 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747772728.975991   54178 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747772728.975992   54178 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-20 22:25:28.978704: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/emiliano/Desktop/Università/Human Language Technologies/Project/HLT Project/eda.py:177: SyntaxWarning: \"is not\" with 'str' literal. Did you mean \"!=\"?\n",
      "  words = [word for word in words if word is not '']\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:25:30.420544Z",
     "start_time": "2025-05-20T20:25:30.380551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hld = get_dataframe_from_json('./hand_labeled/hand_labelled_dataset.json')\n",
    "hld"
   ],
   "id": "4d6cb2e52215c782",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      turn                                          utterance     emotion  \\\n",
       "0        0       I'm so angry . I feel like killing someone .       anger   \n",
       "1        1                                Calm down . __eou__  no_emotion   \n",
       "2        0  I was just about to go to bed when the telepho...  no_emotion   \n",
       "3        1                                       Who was it ?  no_emotion   \n",
       "4        2  Kate . She said she was too excited to go to s...  no_emotion   \n",
       "...    ...                                                ...         ...   \n",
       "1012     6  I want to live abroad and learn to speak a dif...  no_emotion   \n",
       "1013     7              I'm really sorry . But I understand .  no_emotion   \n",
       "1014     8                      Thank you , manager . __eou__  no_emotion   \n",
       "1015     0                            I fired Mr . Li today .  no_emotion   \n",
       "1016     1                          That's terrible . __eou__     sadness   \n",
       "\n",
       "             act    hat  \n",
       "0         inform    red  \n",
       "1         inform    red  \n",
       "2         inform  white  \n",
       "3       question  white  \n",
       "4         inform  white  \n",
       "...          ...    ...  \n",
       "1012      inform  white  \n",
       "1013  commissive  white  \n",
       "1014      inform  white  \n",
       "1015      inform  white  \n",
       "1016      inform  black  \n",
       "\n",
       "[1017 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turn</th>\n",
       "      <th>utterance</th>\n",
       "      <th>emotion</th>\n",
       "      <th>act</th>\n",
       "      <th>hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm so angry . I feel like killing someone .</td>\n",
       "      <td>anger</td>\n",
       "      <td>inform</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Calm down . __eou__</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I was just about to go to bed when the telepho...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Who was it ?</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Kate . She said she was too excited to go to s...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>6</td>\n",
       "      <td>I want to live abroad and learn to speak a dif...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>7</td>\n",
       "      <td>I'm really sorry . But I understand .</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>commissive</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>8</td>\n",
       "      <td>Thank you , manager . __eou__</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>0</td>\n",
       "      <td>I fired Mr . Li today .</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>1</td>\n",
       "      <td>That's terrible . __eou__</td>\n",
       "      <td>sadness</td>\n",
       "      <td>inform</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1017 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:25:32.457786Z",
     "start_time": "2025-05-20T20:25:32.454914Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "da62261522bfed02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:25:32.633747Z",
     "start_time": "2025-05-20T20:25:32.627671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hat_map = {\n",
    "    0: \"red\",\n",
    "    1: \"white\",\n",
    "    2: \"black\",\n",
    "    3: \"yellow\",\n",
    "    4: \"green\",\n",
    "}\n",
    "\n",
    "reverse_hat_map = {v: k for k, v in hat_map.items()}\n",
    "hld['hat'] = hld['hat'].apply(lambda x: reverse_hat_map[x])"
   ],
   "id": "53d408cc997ae229",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:09:41.108217Z",
     "start_time": "2025-05-20T20:09:41.100487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# split the data into train and test\n",
    "train_df, test_df = train_test_split(hld, test_size=0.2, random_state=42, stratify=hld['hat'])\n"
   ],
   "id": "58c289453a62fbc",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:09:43.604073Z",
     "start_time": "2025-05-20T20:09:43.176413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "token_ids, attention_masks = bert_tokenize_data(tokenizer, train_df['utterance'].values)\n",
    "train_dataloader, val_dataloader = tensor_train_test_split(torch.tensor(train_df['hat'].values), token_ids, attention_masks, test_size=0.1)"
   ],
   "id": "e70d7ca38d0438c6",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:48:21.930760Z",
     "start_time": "2025-05-20T19:45:49.222902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "epochs = 20\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "num_training_steps = epochs * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)\n",
    "\n",
    "model = train_bert_model(model, optimizer, scheduler, train_dataloader, val_dataloader, epochs)"
   ],
   "id": "8eac212dbbcdf59a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Epoch 1 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:45:49.853438\n",
      "Average Loss:     1.289921478084896\n",
      "Time Taken:       0:00:07.659268\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:45:57.513115\n",
      "Average Loss:     1.3286631865934893\n",
      "Average Accuracy: 0.45454545454545453\n",
      "Time Taken:       0:00:00.231185\n",
      "\n",
      "-------------------- Epoch 2 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:45:57.744682\n",
      "Average Loss:     1.0797201783760735\n",
      "Time Taken:       0:00:07.201838\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:46:04.946889\n",
      "Average Loss:     1.2672974846579812\n",
      "Average Accuracy: 0.5\n",
      "Time Taken:       0:00:00.237452\n",
      "\n",
      "-------------------- Epoch 3 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:46:05.184793\n",
      "Average Loss:     0.8025512677495894\n",
      "Time Taken:       0:00:07.271905\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:46:12.457073\n",
      "Average Loss:     1.3192306729880245\n",
      "Average Accuracy: 0.5340909090909091\n",
      "Time Taken:       0:00:00.243593\n",
      "\n",
      "-------------------- Epoch 4 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:46:12.701056\n",
      "Average Loss:     0.47431340084775636\n",
      "Time Taken:       0:00:07.230434\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:46:19.931861\n",
      "Average Loss:     1.343697266145186\n",
      "Average Accuracy: 0.625\n",
      "Time Taken:       0:00:00.235101\n",
      "\n",
      "-------------------- Epoch 5 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:46:20.167331\n",
      "Average Loss:     0.26403724835456716\n",
      "Time Taken:       0:00:07.267905\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:46:27.435662\n",
      "Average Loss:     1.7529007304798474\n",
      "Average Accuracy: 0.5340909090909091\n",
      "Time Taken:       0:00:00.251706\n",
      "\n",
      "-------------------- Epoch 6 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:46:27.687764\n",
      "Average Loss:     0.1101150847768978\n",
      "Time Taken:       0:00:07.338912\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:46:35.027057\n",
      "Average Loss:     2.3387119986794214\n",
      "Average Accuracy: 0.5\n",
      "Time Taken:       0:00:00.250899\n",
      "\n",
      "-------------------- Epoch 7 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:46:35.279496\n",
      "Average Loss:     0.07011479575632383\n",
      "Time Taken:       0:00:07.294906\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:46:42.574747\n",
      "Average Loss:     2.0442453289658506\n",
      "Average Accuracy: 0.6022727272727273\n",
      "Time Taken:       0:00:00.249830\n",
      "\n",
      "-------------------- Epoch 8 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:46:42.825002\n",
      "Average Loss:     0.04962056854223509\n",
      "Time Taken:       0:00:07.300625\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:46:50.125993\n",
      "Average Loss:     2.5956009084528144\n",
      "Average Accuracy: 0.5227272727272727\n",
      "Time Taken:       0:00:00.237495\n",
      "\n",
      "-------------------- Epoch 9 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:46:50.363898\n",
      "Average Loss:     0.03803915793896127\n",
      "Time Taken:       0:00:07.294858\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:46:57.659160\n",
      "Average Loss:     2.6174545288085938\n",
      "Average Accuracy: 0.5454545454545454\n",
      "Time Taken:       0:00:00.249755\n",
      "\n",
      "-------------------- Epoch 10 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:46:57.909299\n",
      "Average Loss:     0.021794093488057588\n",
      "Time Taken:       0:00:07.405630\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:47:05.315910\n",
      "Average Loss:     2.333297190044753\n",
      "Average Accuracy: 0.6022727272727273\n",
      "Time Taken:       0:00:00.229053\n",
      "\n",
      "-------------------- Epoch 11 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:47:05.545721\n",
      "Average Loss:     0.022909278580260907\n",
      "Time Taken:       0:00:07.331534\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:47:12.878153\n",
      "Average Loss:     2.4208136302503673\n",
      "Average Accuracy: 0.5795454545454546\n",
      "Time Taken:       0:00:00.235797\n",
      "\n",
      "-------------------- Epoch 12 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:47:13.114323\n",
      "Average Loss:     0.023114910811114976\n",
      "Time Taken:       0:00:07.437874\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:47:20.553176\n",
      "Average Loss:     2.742688187144019\n",
      "Average Accuracy: 0.5568181818181818\n",
      "Time Taken:       0:00:00.246285\n",
      "\n",
      "-------------------- Epoch 13 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:47:20.799950\n",
      "Average Loss:     0.01769405378992973\n",
      "Time Taken:       0:00:07.332341\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:47:28.132663\n",
      "Average Loss:     2.772698835893111\n",
      "Average Accuracy: 0.5227272727272727\n",
      "Time Taken:       0:00:00.244228\n",
      "\n",
      "-------------------- Epoch 14 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:47:28.377306\n",
      "Average Loss:     0.014733520217503057\n",
      "Time Taken:       0:00:07.348147\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:47:35.725974\n",
      "Average Loss:     2.490678906440735\n",
      "Average Accuracy: 0.5568181818181818\n",
      "Time Taken:       0:00:00.251827\n",
      "\n",
      "-------------------- Epoch 15 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:47:35.978232\n",
      "Average Loss:     0.013929416040216734\n",
      "Time Taken:       0:00:07.423239\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:47:43.401871\n",
      "Average Loss:     2.6186644326556814\n",
      "Average Accuracy: 0.5681818181818182\n",
      "Time Taken:       0:00:00.249034\n",
      "\n",
      "-------------------- Epoch 16 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:47:43.651296\n",
      "Average Loss:     0.010129262370553435\n",
      "Time Taken:       0:00:07.384060\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:47:51.037027\n",
      "Average Loss:     2.2934572283728896\n",
      "Average Accuracy: 0.6022727272727273\n",
      "Time Taken:       0:00:00.223821\n",
      "\n",
      "-------------------- Epoch 17 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:47:51.261269\n",
      "Average Loss:     0.011724546534621724\n",
      "Time Taken:       0:00:07.463509\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:47:58.726020\n",
      "Average Loss:     2.2959063875340773\n",
      "Average Accuracy: 0.6136363636363636\n",
      "Time Taken:       0:00:00.237279\n",
      "\n",
      "-------------------- Epoch 18 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:47:58.963734\n",
      "Average Loss:     0.0076514515556324195\n",
      "Time Taken:       0:00:07.530524\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:48:06.495060\n",
      "Average Loss:     2.6235710924321953\n",
      "Average Accuracy: 0.6022727272727273\n",
      "Time Taken:       0:00:00.239073\n",
      "\n",
      "-------------------- Epoch 19 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:48:06.734804\n",
      "Average Loss:     0.005557532594336763\n",
      "Time Taken:       0:00:07.374786\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:48:14.109988\n",
      "Average Loss:     2.5890587026422676\n",
      "Average Accuracy: 0.5909090909090909\n",
      "Time Taken:       0:00:00.245271\n",
      "\n",
      "-------------------- Epoch 20 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:48:14.355694\n",
      "Average Loss:     0.008079994070019735\n",
      "Time Taken:       0:00:07.334078\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 21:48:21.690155\n",
      "Average Loss:     2.642385558648543\n",
      "Average Accuracy: 0.5795454545454546\n",
      "Time Taken:       0:00:00.239435\n",
      "\n",
      "Total training time: 0:02:32.076548\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:38:21.394048Z",
     "start_time": "2025-05-20T16:38:20.991861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_texts = test_df['utterance'].values\n",
    "serie = pd.Series(test_texts)\n",
    "tids, amids = bert_tokenize_data(tokenizer, serie, max_length=64)\n",
    "dl = get_data_loader(tids, amids, batch_size=5, shuffle=False)\n",
    "preds, confidences = model_predict(model, dl)\n",
    "labels_flat = test_df['hat'].values.flatten()\n",
    "accuracy = np.sum(preds == labels_flat) / len(labels_flat)\n",
    "accuracy"
   ],
   "id": "d4f71d611d87e5b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.553921568627451)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:47:43.717838Z",
     "start_time": "2025-05-20T16:47:43.704401Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.53      0.40      0.46        40\n",
      "       white       0.71      0.75      0.73       110\n",
      "       black       0.23      0.26      0.24        23\n",
      "      yellow       0.47      0.42      0.44        19\n",
      "       green       0.07      0.08      0.07        12\n",
      "\n",
      "    accuracy                           0.55       204\n",
      "   macro avg       0.40      0.38      0.39       204\n",
      "weighted avg       0.56      0.55      0.55       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17,
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels_flat, preds_array, target_names=list(hat_map.values())))"
   ],
   "id": "9b2a2e4b9deb1a83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:36:20.922167Z",
     "start_time": "2025-05-20T20:36:20.656319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from util import eda_augment_dataset\n",
    "\n",
    "train_df, test_df = train_test_split(hld, test_size=0.2, random_state=42, stratify=hld['hat'])\n",
    "\n",
    "green_augmented = eda_augment_dataset(train_df[train_df['hat'] == 4], num_aug=10, alpha_sr=0.05, alpha_ri=0.05, alpha_rs=0.05, p_rd=0.1)\n",
    "yellow_augmented = eda_augment_dataset(train_df[train_df['hat'] == 3], num_aug=10, alpha_sr=0.05, alpha_ri=0.05, alpha_rs=0.05, p_rd=0.1)\n",
    "black_augmented = eda_augment_dataset(train_df[train_df['hat'] == 2], num_aug=10, alpha_sr=0.05, alpha_ri=0.05, alpha_rs=0.05, p_rd=0.1)\n",
    "\n",
    "augmented_train_df = pd.concat([train_df, green_augmented, yellow_augmented, black_augmented], ignore_index=True)\n",
    "augmented_train_df"
   ],
   "id": "12094416d5c6987e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      turn                                          utterance     emotion  \\\n",
       "0        3                      I'll take one , too . __eou__   happiness   \n",
       "1        8  You know , we are superior to other clothes co...  no_emotion   \n",
       "2        5                        Her new boyfriend , right ?  no_emotion   \n",
       "3        9  How about recommending him to use the storage ...  no_emotion   \n",
       "4        1  Oh , a bouquet of flowers . It's very kind of ...    surprise   \n",
       "...    ...                                                ...         ...   \n",
       "2938     2                       what seems to be the problem  no_emotion   \n",
       "2939     2                       what seems to be the trouble  no_emotion   \n",
       "2940     2                  what seems to be seem the problem  no_emotion   \n",
       "2941     2                       seems what to be the problem  no_emotion   \n",
       "2942     2                       what seems to be the trouble  no_emotion   \n",
       "\n",
       "             act  hat  \n",
       "0         inform    0  \n",
       "1         inform    3  \n",
       "2     commissive    1  \n",
       "3      directive    4  \n",
       "4     commissive    1  \n",
       "...          ...  ...  \n",
       "2938    question    2  \n",
       "2939    question    2  \n",
       "2940    question    2  \n",
       "2941    question    2  \n",
       "2942    question    2  \n",
       "\n",
       "[2943 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turn</th>\n",
       "      <th>utterance</th>\n",
       "      <th>emotion</th>\n",
       "      <th>act</th>\n",
       "      <th>hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>I'll take one , too . __eou__</td>\n",
       "      <td>happiness</td>\n",
       "      <td>inform</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>You know , we are superior to other clothes co...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Her new boyfriend , right ?</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>commissive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>How about recommending him to use the storage ...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>directive</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Oh , a bouquet of flowers . It's very kind of ...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>commissive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938</th>\n",
       "      <td>2</td>\n",
       "      <td>what seems to be the problem</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2939</th>\n",
       "      <td>2</td>\n",
       "      <td>what seems to be the trouble</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2940</th>\n",
       "      <td>2</td>\n",
       "      <td>what seems to be seem the problem</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2941</th>\n",
       "      <td>2</td>\n",
       "      <td>seems what to be the problem</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2942</th>\n",
       "      <td>2</td>\n",
       "      <td>what seems to be the trouble</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2943 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:41:03.087837Z",
     "start_time": "2025-05-20T20:36:29.315721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "token_ids, attention_masks = bert_tokenize_data(tokenizer, augmented_train_df['utterance'].values)\n",
    "train_dataloader, val_dataloader = tensor_train_test_split(torch.tensor(augmented_train_df['hat'].values), token_ids, attention_masks, test_size=0.1)\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "epochs = 10\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "num_training_steps = epochs * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)\n",
    "\n",
    "model = train_bert_model(model, optimizer, scheduler, train_dataloader, val_dataloader, epochs)\n",
    "test_texts = test_df['utterance'].values\n",
    "serie = pd.Series(test_texts)\n",
    "tids, amids = bert_tokenize_data(tokenizer, serie, max_length=64)\n",
    "dl = get_data_loader(tids, amids, batch_size=5, shuffle=False)\n",
    "preds, confidences = model_predict(model, dl)\n",
    "labels_flat = test_df['hat'].values.flatten()\n",
    "\n",
    "preds_array = np.array(preds)\n",
    "print(classification_report(labels_flat, preds_array, target_names=list(hat_map.values())))"
   ],
   "id": "7a72c4f6b580b1d9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Epoch 1 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 22:36:30.704541\n",
      "Average Loss:     0.8054862971842469\n",
      "Time Taken:       0:00:26.048986\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 22:36:56.754114\n",
      "Average Loss:     0.307436011228207\n",
      "Average Accuracy: 0.9324324324324325\n",
      "Time Taken:       0:00:00.822701\n",
      "\n",
      "-------------------- Epoch 2 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 22:36:57.577185\n",
      "Average Loss:     0.25033063978783987\n",
      "Time Taken:       0:00:26.196718\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 22:37:23.774278\n",
      "Average Loss:     0.9572705428141195\n",
      "Average Accuracy: 0.7693050193050194\n",
      "Time Taken:       0:00:00.822414\n",
      "\n",
      "-------------------- Epoch 3 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 22:37:24.597094\n",
      "Average Loss:     0.13844849159652678\n",
      "Time Taken:       0:00:26.161896\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 22:37:50.759325\n",
      "Average Loss:     0.31897426713761445\n",
      "Average Accuracy: 0.9256756756756757\n",
      "Time Taken:       0:00:00.823214\n",
      "\n",
      "-------------------- Epoch 4 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 22:37:51.582907\n",
      "Average Loss:     0.07006206330871406\n",
      "Time Taken:       0:00:26.256215\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 22:38:17.839721\n",
      "Average Loss:     0.3942863450706559\n",
      "Average Accuracy: 0.9222972972972973\n",
      "Time Taken:       0:00:00.830384\n",
      "\n",
      "-------------------- Epoch 5 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 22:38:18.670508\n",
      "Average Loss:     0.030524573509378802\n",
      "Time Taken:       0:00:26.387888\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 22:38:45.058750\n",
      "Average Loss:     0.719497947303918\n",
      "Average Accuracy: 0.871138996138996\n",
      "Time Taken:       0:00:00.827308\n",
      "\n",
      "-------------------- Epoch 6 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 22:38:45.886673\n",
      "Average Loss:     0.01603411105296116\n",
      "Time Taken:       0:00:26.610005\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 22:39:12.497044\n",
      "Average Loss:     0.2593177786738788\n",
      "Average Accuracy: 0.9425675675675675\n",
      "Time Taken:       0:00:00.831232\n",
      "\n",
      "-------------------- Epoch 7 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 22:39:13.328714\n",
      "Average Loss:     0.010204319424330739\n",
      "Time Taken:       0:00:26.646837\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 22:39:39.976351\n",
      "Average Loss:     0.24495473756404543\n",
      "Average Accuracy: 0.9459459459459459\n",
      "Time Taken:       0:00:00.826618\n",
      "\n",
      "-------------------- Epoch 8 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 22:39:40.803981\n",
      "Average Loss:     0.006764541439458115\n",
      "Time Taken:       0:00:26.561815\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 22:40:07.366219\n",
      "Average Loss:     0.14983775183429462\n",
      "Average Accuracy: 0.9724903474903475\n",
      "Time Taken:       0:00:00.836941\n",
      "\n",
      "-------------------- Epoch 9 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 22:40:08.203558\n",
      "Average Loss:     0.006488780281260283\n",
      "Time Taken:       0:00:26.427761\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 22:40:34.631676\n",
      "Average Loss:     0.14037372496268535\n",
      "Average Accuracy: 0.9695945945945946\n",
      "Time Taken:       0:00:00.829694\n",
      "\n",
      "-------------------- Epoch 10 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-05-20 22:40:35.461800\n",
      "Average Loss:     0.003936538196241292\n",
      "Time Taken:       0:00:26.465575\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-05-20 22:41:01.927791\n",
      "Average Loss:     0.14851000932252942\n",
      "Average Accuracy: 0.9695945945945946\n",
      "Time Taken:       0:00:00.833224\n",
      "\n",
      "Total training time: 0:04:32.056887\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.50      0.55      0.52        40\n",
      "       white       0.70      0.80      0.75       110\n",
      "       black       0.31      0.22      0.26        23\n",
      "      yellow       0.86      0.32      0.46        19\n",
      "       green       0.27      0.25      0.26        12\n",
      "\n",
      "    accuracy                           0.61       204\n",
      "   macro avg       0.53      0.43      0.45       204\n",
      "weighted avg       0.61      0.61      0.59       204\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a591cf113b708480"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
