{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da87bdd0",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "- protobuf sentencepiece bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9c5dc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "# load the data in one single dataframe\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f8b6b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refactor(df):# Extract all utterances from the dialogues\n",
    "    all_utterances = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Each row has an 'utterances' field which is a list of utterance dictionaries\n",
    "        utterances = row['utterances']\n",
    "\n",
    "        # Add dialogue ID and topic to each utterance for reference\n",
    "        for utterance in utterances:\n",
    "            utterance['dialogue_id'] = row['id']\n",
    "            utterance['topic'] = row['topic']\n",
    "            all_utterances.append(utterance)\n",
    "\n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df = pd.DataFrame(all_utterances)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e70ce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('../../dailydialog/dialogues.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "357c6804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "turn",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "utterance",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "emotion",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "act",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "hat",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dialogue_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "topic",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "b3d45e7f-fc6e-497d-b3cf-8ab7133a0910",
       "rows": [
        [
         "0",
         "0",
         "The kitchen stinks .",
         "disgust",
         "directive",
         "",
         "0",
         "Ordinary_Life"
        ],
        [
         "1",
         "1",
         "I'll throw out the garbage . __eou__",
         "no_emotion",
         "commissive",
         "",
         "0",
         "Ordinary_Life"
        ],
        [
         "2",
         "0",
         "So Dick , how about getting some coffee for tonight ?",
         "happiness",
         "directive",
         "",
         "1",
         "Ordinary_Life"
        ],
        [
         "3",
         "1",
         "Coffee ? I don ’ t honestly like that kind of stuff .",
         "disgust",
         "commissive",
         "",
         "1",
         "Ordinary_Life"
        ],
        [
         "4",
         "2",
         "Come on , you can at least try a little , besides your cigarette .",
         "no_emotion",
         "directive",
         "",
         "1",
         "Ordinary_Life"
        ],
        [
         "5",
         "3",
         "What ’ s wrong with that ? Cigarette is the thing I go crazy for .",
         "anger",
         "inform",
         "",
         "1",
         "Ordinary_Life"
        ],
        [
         "6",
         "4",
         "Not for me , Dick . __eou__",
         "no_emotion",
         "inform",
         "",
         "1",
         "Ordinary_Life"
        ],
        [
         "7",
         "0",
         "Are things still going badly with your houseguest ?",
         "no_emotion",
         "question",
         "",
         "2",
         "Ordinary_Life"
        ],
        [
         "8",
         "1",
         "Getting worse . Now he ’ s eating me out of house and home . I ’ Ve tried talking to him but it all goes in one ear and out the other . He makes himself at home , which is fine . But what really gets me is that yesterday he walked into the living room in the raw and I had company over ! That was the last straw .",
         "anger",
         "inform",
         "",
         "2",
         "Ordinary_Life"
        ],
        [
         "9",
         "2",
         "Leo , I really think you ’ re beating around the bush with this guy . I know he used to be your best friend in college , but I really think it ’ s time to lay down the law .",
         "no_emotion",
         "directive",
         "",
         "2",
         "Ordinary_Life"
        ],
        [
         "10",
         "3",
         "You ’ re right . Everything is probably going to come to a head tonight . I ’ ll keep you informed . __eou__",
         "no_emotion",
         "commissive",
         "",
         "2",
         "Ordinary_Life"
        ],
        [
         "11",
         "0",
         "Would you mind waiting a while ?",
         "no_emotion",
         "directive",
         "",
         "3",
         "Ordinary_Life"
        ],
        [
         "12",
         "1",
         "Well , how long will it be ?",
         "no_emotion",
         "question",
         "",
         "3",
         "Ordinary_Life"
        ],
        [
         "13",
         "2",
         "I'm not sure . But I'll get a table ready as fast as I can .",
         "no_emotion",
         "inform",
         "",
         "3",
         "Ordinary_Life"
        ],
        [
         "14",
         "3",
         "OK . We'll wait . __eou__",
         "happiness",
         "inform",
         "",
         "3",
         "Ordinary_Life"
        ],
        [
         "15",
         "0",
         "Are you going to the annual party ? I can give you a ride if you need one .",
         "no_emotion",
         "directive",
         "",
         "4",
         "Ordinary_Life"
        ],
        [
         "16",
         "1",
         "Thanks a lot . That's the favor I was going to ask you for .",
         "happiness",
         "commissive",
         "",
         "4",
         "Ordinary_Life"
        ],
        [
         "17",
         "2",
         "The pleasure is mine . __eou__",
         "happiness",
         "inform",
         "",
         "4",
         "Ordinary_Life"
        ],
        [
         "18",
         "0",
         "Isn ’ t he the best instructor ? I think he ’ s so hot . Wow ! I really feel energized , don ’ t you ?",
         "happiness",
         "question",
         "",
         "5",
         "Ordinary_Life"
        ],
        [
         "19",
         "1",
         "I swear , I ’ m going to kill you for this .",
         "disgust",
         "inform",
         "",
         "5",
         "Ordinary_Life"
        ],
        [
         "20",
         "2",
         "What ’ s wrong ? Didn ’ t you think it was fun ? !",
         "surprise",
         "question",
         "",
         "5",
         "Ordinary_Life"
        ],
        [
         "21",
         "3",
         "Oh , yeah ! I had a blast ! I love sweating like a pig with a bunch of pot bellies who all smell bad . Sorry , I ’ m just not into this health kick .",
         "disgust",
         "inform",
         "",
         "5",
         "Ordinary_Life"
        ],
        [
         "22",
         "4",
         "Oh , no , get off it . It wasn ’ t such a killer class . You just have to get into it . Like they say , no pain , no gain .",
         "no_emotion",
         "directive",
         "",
         "5",
         "Ordinary_Life"
        ],
        [
         "23",
         "5",
         "I am wiped out . Thank you .",
         "no_emotion",
         "commissive",
         "",
         "5",
         "Ordinary_Life"
        ],
        [
         "24",
         "6",
         "Look , next time get yourself some comfy shoes . You ’ re gonna come back again with me , aren ’ t you ?",
         "no_emotion",
         "directive",
         "",
         "5",
         "Ordinary_Life"
        ],
        [
         "25",
         "7",
         "Never ! But thank you for inviting me .",
         "no_emotion",
         "commissive",
         "",
         "5",
         "Ordinary_Life"
        ],
        [
         "26",
         "8",
         "Come on . You ’ ll feel better after we hit the showers . __eou__",
         "no_emotion",
         "directive",
         "",
         "5",
         "Ordinary_Life"
        ],
        [
         "27",
         "0",
         "Can I take your order now or do you still want to look at the menu ?",
         "no_emotion",
         "question",
         "",
         "6",
         "Ordinary_Life"
        ],
        [
         "28",
         "1",
         "Well , I want a fillet steak , medium , but my little girl doesn't care for steak . Could she have something else instead ?",
         "no_emotion",
         "question",
         "",
         "6",
         "Ordinary_Life"
        ],
        [
         "29",
         "2",
         "Certainly . How about spaghetti with clams and shrimps .",
         "no_emotion",
         "directive",
         "",
         "6",
         "Ordinary_Life"
        ],
        [
         "30",
         "3",
         "Sounds delicious . OK . She'll try that . __eou__",
         "happiness",
         "commissive",
         "",
         "6",
         "Ordinary_Life"
        ],
        [
         "31",
         "0",
         "Can you manage chopsticks ?",
         "no_emotion",
         "question",
         "",
         "7",
         "Ordinary_Life"
        ],
        [
         "32",
         "1",
         "Why not ? See .",
         "no_emotion",
         "inform",
         "",
         "7",
         "Ordinary_Life"
        ],
        [
         "33",
         "2",
         "Good mastery . How do you like our Chinese food ?",
         "happiness",
         "question",
         "",
         "7",
         "Ordinary_Life"
        ],
        [
         "34",
         "3",
         "Oh , great ! It's delicious . You see , I am already putting on weight . There is one thing I don't like however , MSG .",
         "happiness",
         "inform",
         "",
         "7",
         "Ordinary_Life"
        ],
        [
         "35",
         "4",
         "What's wrong with MSG ? It helps to bring out the taste of the food .",
         "surprise",
         "question",
         "",
         "7",
         "Ordinary_Life"
        ],
        [
         "36",
         "5",
         "According to some studies it may cause cancer .",
         "no_emotion",
         "inform",
         "",
         "7",
         "Ordinary_Life"
        ],
        [
         "37",
         "6",
         "Oh , don't let that worry you . If that were true , China wouldn't have such a large population .",
         "no_emotion",
         "inform",
         "",
         "7",
         "Ordinary_Life"
        ],
        [
         "38",
         "7",
         "I just happen to have a question for you guys . Why do the Chinese cook the vegetables ? You see what I mean is that most vitamin are destroyed when heated .",
         "no_emotion",
         "question",
         "",
         "7",
         "Ordinary_Life"
        ],
        [
         "39",
         "8",
         "I don't know exactly . It's a tradition . Maybe it's for sanitary reasons . __eou__",
         "no_emotion",
         "inform",
         "",
         "7",
         "Ordinary_Life"
        ],
        [
         "40",
         "0",
         "I'm exhausted .",
         "no_emotion",
         "directive",
         "",
         "8",
         "Ordinary_Life"
        ],
        [
         "41",
         "1",
         "Okay , let's go home . __eou__",
         "no_emotion",
         "commissive",
         "",
         "8",
         "Ordinary_Life"
        ],
        [
         "42",
         "0",
         "Good evening . Welcome to Cherry's . Do you have a reservation ?",
         "no_emotion",
         "question",
         "",
         "9",
         "Ordinary_Life"
        ],
        [
         "43",
         "1",
         "No , we don't .",
         "no_emotion",
         "inform",
         "",
         "9",
         "Ordinary_Life"
        ],
        [
         "44",
         "2",
         "How many of you , please ?",
         "no_emotion",
         "question",
         "",
         "9",
         "Ordinary_Life"
        ],
        [
         "45",
         "3",
         "Six , including two kids .",
         "no_emotion",
         "inform",
         "",
         "9",
         "Ordinary_Life"
        ],
        [
         "46",
         "4",
         "I'm afraid all the big tables are taken . __eou__",
         "no_emotion",
         "inform",
         "",
         "9",
         "Ordinary_Life"
        ],
        [
         "47",
         "0",
         "What kind of food do you like ?",
         "no_emotion",
         "question",
         "",
         "10",
         "Ordinary_Life"
        ],
        [
         "48",
         "1",
         "I like Chinese food .",
         "happiness",
         "inform",
         "",
         "10",
         "Ordinary_Life"
        ],
        [
         "49",
         "2",
         "But your American ?",
         "surprise",
         "question",
         "",
         "10",
         "Ordinary_Life"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 102968
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turn</th>\n",
       "      <th>utterance</th>\n",
       "      <th>emotion</th>\n",
       "      <th>act</th>\n",
       "      <th>hat</th>\n",
       "      <th>dialogue_id</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The kitchen stinks .</td>\n",
       "      <td>disgust</td>\n",
       "      <td>directive</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Ordinary_Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I'll throw out the garbage . __eou__</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>commissive</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Ordinary_Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>So Dick , how about getting some coffee for to...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>directive</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Ordinary_Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Coffee ? I don ’ t honestly like that kind of ...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>commissive</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Ordinary_Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Come on , you can at least try a little , besi...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>directive</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Ordinary_Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102963</th>\n",
       "      <td>10</td>\n",
       "      <td>Well , thank you very much for all that inform...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>directive</td>\n",
       "      <td></td>\n",
       "      <td>13117</td>\n",
       "      <td>Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102964</th>\n",
       "      <td>11</td>\n",
       "      <td>Are you going to make an offer today ?</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>question</td>\n",
       "      <td></td>\n",
       "      <td>13117</td>\n",
       "      <td>Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102965</th>\n",
       "      <td>12</td>\n",
       "      <td>Yes . My customer is in urgent need of the ste...</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>inform</td>\n",
       "      <td></td>\n",
       "      <td>13117</td>\n",
       "      <td>Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102966</th>\n",
       "      <td>13</td>\n",
       "      <td>Ok , I'll get this rate right away .</td>\n",
       "      <td>no_emotion</td>\n",
       "      <td>commissive</td>\n",
       "      <td></td>\n",
       "      <td>13117</td>\n",
       "      <td>Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102967</th>\n",
       "      <td>14</td>\n",
       "      <td>Thank you . __eou__</td>\n",
       "      <td>happiness</td>\n",
       "      <td>inform</td>\n",
       "      <td></td>\n",
       "      <td>13117</td>\n",
       "      <td>Finance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102968 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        turn                                          utterance     emotion  \\\n",
       "0          0                               The kitchen stinks .     disgust   \n",
       "1          1               I'll throw out the garbage . __eou__  no_emotion   \n",
       "2          0  So Dick , how about getting some coffee for to...   happiness   \n",
       "3          1  Coffee ? I don ’ t honestly like that kind of ...     disgust   \n",
       "4          2  Come on , you can at least try a little , besi...  no_emotion   \n",
       "...      ...                                                ...         ...   \n",
       "102963    10  Well , thank you very much for all that inform...  no_emotion   \n",
       "102964    11             Are you going to make an offer today ?  no_emotion   \n",
       "102965    12  Yes . My customer is in urgent need of the ste...  no_emotion   \n",
       "102966    13               Ok , I'll get this rate right away .  no_emotion   \n",
       "102967    14                                Thank you . __eou__   happiness   \n",
       "\n",
       "               act hat  dialogue_id          topic  \n",
       "0        directive                0  Ordinary_Life  \n",
       "1       commissive                0  Ordinary_Life  \n",
       "2        directive                1  Ordinary_Life  \n",
       "3       commissive                1  Ordinary_Life  \n",
       "4        directive                1  Ordinary_Life  \n",
       "...            ...  ..          ...            ...  \n",
       "102963   directive            13117        Finance  \n",
       "102964    question            13117        Finance  \n",
       "102965      inform            13117        Finance  \n",
       "102966  commissive            13117        Finance  \n",
       "102967      inform            13117        Finance  \n",
       "\n",
       "[102968 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refactor_df = refactor(df)\n",
    "refactor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e917c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atlas/hlt/HLT/venv_hlt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dffc16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Scegli il checkpoint quantizzato 4‑bit (gguf/q4_0) su HF\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72c88d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HUGGINGFACE_HUB_TOKEN=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c591197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Carica tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e2aa604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Configurazione 4‑bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "921c9eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:31<00:00, 10.51s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32768, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_mem = {\n",
    "    0: 5_300 * 1024**2,      # GPU 0\n",
    "    \"cpu\": 60 * 1024**3,     # tutto ciò che non sta in GPU\n",
    "}\n",
    "\n",
    "# 4) Carica modello su GPU (device_map=\"auto\" sposta layer su GPU fino a saturazione)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    max_memory=max_mem,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a566606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Definisci la “definizione” da ripassare al modello:\n",
    "prompt_prefix = \"\"\"\n",
    "You are helping create a synthetic dataset for a classification task.\n",
    "The goal is to generate single-sentence utterances that reflect one specific thinking style based on Edward De Bono's Six Thinking Hats. Here is the definition for the [CAP] hat:\n",
    "[DEFINITION]\n",
    "Please generate 10 distinct, realistic, mixed length utterances that clearly follow this definition. Each utterance must reflect the corresponding thinking style.\n",
    "Only output a JSON list of objects, each in the format:\n",
    "{\"utterance\": \"your sentence here\", \"hat\": \"[CAP]\"}\n",
    "Respond only with the json text. No explanations, no notes, no markdown. Only valid JSON.\n",
    "Now generate 10 utterances for the [CAP] hat.\n",
    "Your answer must start with the json list and nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "966c5c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "white_hat_definition = \"Exchanging or providing plain informations. Things generally true, things that happened to someone. Not trying to convince anyone.\"\n",
    "black_hat_definition = \"Analysis of a situation. Answers the why of something that is not being done. Negative analysis, explaining weak points of a thing, logically.\"\n",
    "red_hat_definition = \"Emotions involved in the answer. The utterance is clearly stated due to emotion involved. Intuitions, feelings, gut reactions. No need for logical justification.\"\n",
    "yellow_hat_definition = \"Statements that highlight the positive sides (negatives can exists). Changes that offer benefits. Open up remote but highly desirable possibilities. Reflect ideas to believe in. Provide encouragement to take action. Express positive judgments.\"\n",
    "green_hat_definition = \"Proposing new point of views. New Ideas. Solutions to problems. Imagining new scenarios, going beyond what is known.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96c4424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "white_hat_prompt = prompt_prefix.replace(\"[CAP]\", \"White\")\n",
    "white_hat_prompt = white_hat_prompt.replace(\"[DEFINITION]\", white_hat_definition)\n",
    "black_hat_prompt = prompt_prefix.replace(\"[CAP]\", \"Black\")\n",
    "black_hat_prompt = black_hat_prompt.replace(\"[DEFINITION]\", black_hat_definition)\n",
    "red_hat_prompt = prompt_prefix.replace(\"[CAP]\", \"Red\")\n",
    "red_hat_prompt = red_hat_prompt.replace(\"[DEFINITION]\", red_hat_definition)\n",
    "yellow_hat_prompt = prompt_prefix.replace(\"[CAP]\", \"Yellow\")\n",
    "yellow_hat_prompt = yellow_hat_prompt.replace(\"[DEFINITION]\", yellow_hat_definition)\n",
    "green_hat_prompt = prompt_prefix.replace(\"[CAP]\", \"Green\")\n",
    "green_hat_prompt = green_hat_prompt.replace(\"[DEFINITION]\", green_hat_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6ecbf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def remove_spaces_outside_quotes(text):\n",
    "    result = []\n",
    "    inside_string = False\n",
    "\n",
    "    for char in text:\n",
    "        if char == '\"':\n",
    "            inside_string = not inside_string\n",
    "            result.append(char)\n",
    "        elif not inside_string and char == ' ':\n",
    "            continue  # ignora spazi fuori dalle virgolette\n",
    "        else:\n",
    "            result.append(char)\n",
    "\n",
    "    return ''.join(result)\n",
    "\n",
    "def generate_synthetic_examples(prompt, model, tokenizer, max_new_tokens=512):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(output[0][ inputs[\"input_ids\"].shape[-1]: ], skip_special_tokens=True).strip()\n",
    "    # remove newlines from decoded\n",
    "    decoded = decoded.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    # remove spaces except the rha spaces inside double quotes\n",
    "    decoded = ' '.join(decoded.split())\n",
    "    decoded = remove_spaces_outside_quotes(decoded)\n",
    "    # print(\"decoded:\", decoded)\n",
    "    \n",
    "    try:\n",
    "        json_start = decoded.find(\"[{\")\n",
    "        json_end = decoded.rfind(\"}]\") + 2\n",
    "        json_text = decoded[json_start:json_end]\n",
    "        return json.loads(json_text)\n",
    "    except Exception as e:\n",
    "        print(\"Errore nel parsing:\", e)\n",
    "        print(\"Output ricevuto:\\n\", decoded)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52d3d952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(216)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "white_examples = []\n",
    "for _ in range(100):\n",
    "    examples = generate_synthetic_examples(white_hat_prompt, model, tokenizer)\n",
    "    white_examples.extend(examples)\n",
    "    \n",
    "white_examples = pd.DataFrame(white_examples)\n",
    "white_examples['utterance'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21e75358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errore nel parsing: Expecting ',' delimiter: line 1 column 1293 (char 1292)\n",
      "Output ricevuto:\n",
      " [{\"utterance\":\"What if we integrated AI capabilities into our current customer service system to provide faster and more accurate responses?\",\"hat\":\"Green\"},{\"utterance\":\"Could we design a new product line that caters to both the environmentally conscious and the tech-savvy market?\",\"hat\":\"Green\"},{\"utterance\":\"What if we implemented a loyalty program that rewards customers for recycling our products?\",\"hat\":\"Green\"},{\"utterance\":\"How about a mobile app that allows users to customize their own eco-friendly home decor?\",\"hat\":\"Green\"},{\"utterance\":\"What if we created a system that uses AI to predict and prevent equipment failures in our manufacturing plants?\",\"hat\":\"Green\"},{\"utterance\":\"Could we develop a virtual reality training program for new hires in the tech industry?\",\"hat\":\"Green\"},{\"utterance\":\"What if we designed a smart city infrastructure that optimizes traffic flow and reduces carbon emissions?\",\"hat\":\"Green\"},{\"utterance\":\"How about a social media platform that encourages users to share and discuss innovative ideas?\",\"hat\":\"Green\"},{\"utterance\":\"What if we implemented a system that uses AI to analyze customer feedback and suggest improvements?\",\"hat\":\"Green\"},{\"utterance\":\"Could we develop a renewable energy solution that is both cost-effective and scalable?\".\"hat\":\"Green\"}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(24)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "green_examples = []\n",
    "for _ in range(100):\n",
    "    examples = generate_synthetic_examples(green_hat_prompt, model, tokenizer)\n",
    "    green_examples.extend(examples)\n",
    "\n",
    "green_examples = pd.DataFrame(green_examples)\n",
    "green_examples['utterance'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c14ba0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(151)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_examples = []\n",
    "for _ in range(100):\n",
    "    examples = generate_synthetic_examples(red_hat_prompt, model, tokenizer)\n",
    "    red_examples.extend(examples)\n",
    "red_examples = pd.DataFrame(red_examples)\n",
    "red_examples['utterance'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a14cb2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(126)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "black_examples = []\n",
    "for _ in range(100):\n",
    "    examples = generate_synthetic_examples(black_hat_prompt, model, tokenizer)\n",
    "    black_examples.extend(examples)\n",
    "black_examples = pd.DataFrame(black_examples)\n",
    "black_examples['utterance'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5c9c1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(34)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_examples = []\n",
    "for _ in range(100):\n",
    "    examples = generate_synthetic_examples(yellow_hat_prompt, model, tokenizer)\n",
    "    yellow_examples.extend(examples)\n",
    "yellow_examples = pd.DataFrame(yellow_examples)\n",
    "yellow_examples['utterance'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8103cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates from white_examples\n",
    "white_examples = white_examples.drop_duplicates(subset='utterance')\n",
    "# remove duplicates from yellow_examples\n",
    "yellow_examples = yellow_examples.drop_duplicates(subset='utterance')\n",
    "# remove duplicates from red_examples\n",
    "red_examples = red_examples.drop_duplicates(subset='utterance')\n",
    "# remove duplicates from black_examples\n",
    "black_examples = black_examples.drop_duplicates(subset='utterance')\n",
    "# remove duplicates from green_examples\n",
    "green_examples = green_examples.drop_duplicates(subset='utterance')\n",
    "\n",
    "# all_examples = white_examples + yellow_examples + red_examples + green_examples + black_examples\n",
    "\n",
    "#remove duplicates from all_examples\n",
    "all_examples = pd.concat([white_examples, yellow_examples, red_examples, green_examples, black_examples]).drop_duplicates(subset='utterance')\n",
    "\n",
    "with open(\"synthetic_hat_dataset.jsonl\", \"w\") as f:\n",
    "    for item in all_examples:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d645e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index in all_examples\n",
    "all_examples.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bf6ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write all_examples to a json file\n",
    "all_examples.to_json(\"synthetic_hat_dataset.json\", orient=\"records\", force_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc6779ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load the json file to pandas dataframe\n",
    "synthetic_df = pd.read_json(\"synthetic_hat_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9855bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(synthetic_df, test_size=0.2, random_state=42, stratify=synthetic_df['hat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57a722d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "618811ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write train and test to json files\n",
    "train_df.to_json(\"synthetic_train_dataset.json\", orient=\"records\", force_ascii=False, indent=4)\n",
    "test_df.to_json(\"synthetic_test_dataset.json\", orient=\"records\", force_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8958458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_hlt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
